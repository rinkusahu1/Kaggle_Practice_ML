{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8759299,"sourceType":"datasetVersion","datasetId":5262554},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684},{"sourceId":59742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":49982},{"sourceId":68174,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":56841}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-23T08:00:57.569919Z","iopub.execute_input":"2024-06-23T08:00:57.570567Z","iopub.status.idle":"2024-06-23T08:00:57.980524Z","shell.execute_reply.started":"2024-06-23T08:00:57.570525Z","shell.execute_reply":"2024-06-23T08:00:57.979634Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/processed-test-and-train/train_processed.csv\n/kaggle/input/processed-test-and-train/test_preprocessed.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/kmeans_model.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_3.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/character_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_1.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/word_tfidf_feature_names.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/count_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_2.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/word_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_0.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_4.txt\n/kaggle/input/automated-essay-scoring-2.0/other/essayscoring/1/model_checkpoint.weights.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install pyspellchecker\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:00:57.981980Z","iopub.execute_input":"2024-06-23T08:00:57.982334Z","iopub.status.idle":"2024-06-23T08:01:11.792458Z","shell.execute_reply.started":"2024-06-23T08:00:57.982309Z","shell.execute_reply":"2024-06-23T08:01:11.791000Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspellchecker\n  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\nDownloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport polars as pl\nimport spacy\nfrom spellchecker import SpellChecker\n\nfrom collections import OrderedDict, Counter, defaultdict\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:11.795462Z","iopub.execute_input":"2024-06-23T08:01:11.795788Z","iopub.status.idle":"2024-06-23T08:01:28.706707Z","shell.execute_reply.started":"2024-06-23T08:01:11.795757Z","shell.execute_reply":"2024-06-23T08:01:28.705884Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-23 08:01:13.417808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-23 08:01:13.417906: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-23 08:01:13.545692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nimport json, string\nfrom typing import List\n\nimport spacy\nfrom spellchecker import SpellChecker\n\nfrom collections import OrderedDict, Counter, defaultdict\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import log_evaluation, early_stopping\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:28.709345Z","iopub.execute_input":"2024-06-23T08:01:28.710277Z","iopub.status.idle":"2024-06-23T08:01:31.099630Z","shell.execute_reply.started":"2024-06-23T08:01:28.710240Z","shell.execute_reply":"2024-06-23T08:01:31.098686Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install textstat","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:31.100904Z","iopub.execute_input":"2024-06-23T08:01:31.101895Z","iopub.status.idle":"2024-06-23T08:01:43.686454Z","shell.execute_reply.started":"2024-06-23T08:01:31.101858Z","shell.execute_reply":"2024-06-23T08:01:43.685254Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting textstat\n  Downloading textstat-0.7.3-py3-none-any.whl.metadata (14 kB)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.15.0-py3-none-any.whl.metadata (3.3 kB)\nDownloading textstat-0.7.3-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, textstat\nSuccessfully installed pyphen-0.15.0 textstat-0.7.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import textstat\nfrom textblob import TextBlob\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:43.688090Z","iopub.execute_input":"2024-06-23T08:01:43.688473Z","iopub.status.idle":"2024-06-23T08:01:44.138125Z","shell.execute_reply.started":"2024-06-23T08:01:43.688440Z","shell.execute_reply":"2024-06-23T08:01:44.137317Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:44.139254Z","iopub.execute_input":"2024-06-23T08:01:44.139620Z","iopub.status.idle":"2024-06-23T08:01:45.421555Z","shell.execute_reply.started":"2024-06-23T08:01:44.139581Z","shell.execute_reply":"2024-06-23T08:01:45.420451Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.423359Z","iopub.execute_input":"2024-06-23T08:01:45.423792Z","iopub.status.idle":"2024-06-23T08:01:45.429192Z","shell.execute_reply.started":"2024-06-23T08:01:45.423753Z","shell.execute_reply":"2024-06-23T08:01:45.428115Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of score","metadata":{}},{"cell_type":"code","source":"# score_distribution = df['score'].value_counts().sort_index()\n\n# # Plot the distribution using a bar chart\n# score_distribution.plot(kind='bar', color='skyblue', edgecolor='black')\n\n# # Add titles and labels\n# plt.title('Distribution of Scores')\n# plt.xlabel('Score')\n# plt.ylabel('Frequency')\n\n# # Show the plot\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.430348Z","iopub.execute_input":"2024-06-23T08:01:45.430920Z","iopub.status.idle":"2024-06-23T08:01:45.439443Z","shell.execute_reply.started":"2024-06-23T08:01:45.430895Z","shell.execute_reply":"2024-06-23T08:01:45.438560Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# This is the list for stop words\nstopwords_list = [\n    \"a\", \"about\", \"above\", \"according\", \"across\", \"actually\", \"adj\", \"after\", \"afterwards\", \"again\",\n    \"all\", \"almost\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"an\",\n    \"am\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anywhere\", \"are\", \"aren\",\n    \"aren't\", \"around\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"been\",\n    \"beforehand\", \"begin\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"both\", \"but\", \"by\",\n    \"can\", \"cannot\", \"can't\", \"caption\", \"co\", \"come\", \"could\", \"couldn\", \"couldn't\", \"did\", \"didn\",\n    \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"early\",\n    \"eg\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"etc\", \"even\", \"ever\", \"every\",\n    \"everywhere\", \"except\", \"few\", \"for\", \"found\", \"from\", \"further\", \"had\", \"has\", \"hasn\", \"hasn't\",\n    \"have\", \"haven\", \"haven't\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\",\n    \"hereupon\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"ie\", \"i.e.\", \"if\", \"in\", \"inc\", \"inc.\",\n    \"indeed\", \"instead\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"its\", \"itself\", \"last\", \"late\",\n    \"later\", \"less\", \"let\", \"like\", \"likely\", \"ll\", \"ltd\", \"made\", \"make\", \"makes\", \"many\", \"may\",\n    \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"might\", \"miss\", \"more\", \"most\", \"mostly\", \"mr\", \"mrs\",\n    \"much\", \"must\", \"my\", \"myself\", \"namely\", \"near\", \"neither\", \"never\", \"nevertheless\", \"new\",\n    \"next\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"not\", \"now\", \"NULL\",\n    \"of\", \"off\", \"often\", \"on\", \"once\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"per\", \"perhaps\", \"rather\", \"re\", \"said\", \"same\",\n    \"say\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"several\", \"she\", \"should\", \"shouldn\", \"shouldn't\",\n    \"since\", \"so\", \"some\", \"still\", \"stop\", \"such\", \"taking\", \"ten\", \"than\", \"that\", \"the\", \"their\",\n    \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\",\n    \"therein\", \"thereupon\", \"these\", \"they\", \"this\", \"those\", \"though\", \"thousand\", \"through\",\n    \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\", \"under\", \"unless\",\n    \"unlike\", \"unlikely\", \"until\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"using\", \"ve\", \"very\", \"via\",\n    \"was\", \"wasn\", \"we\", \"well\", \"were\", \"weren\", \"weren't\", \"what\", \"whatever\", \"when\", \"whence\",\n    \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n    \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whose\",\n    \"why\", \"will\", \"with\", \"within\", \"without\", \"won\", \"would\", \"wouldn\", \"wouldn't\", \"yes\", \"yet\",\n    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n]\n\nfinal_stopwords_list = list(set(stopwords.words('english')) | set(stopwords_list))\nprint(len(final_stopwords_list))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.443320Z","iopub.execute_input":"2024-06-23T08:01:45.443593Z","iopub.status.idle":"2024-06-23T08:01:45.465340Z","shell.execute_reply.started":"2024-06-23T08:01:45.443571Z","shell.execute_reply":"2024-06-23T08:01:45.464395Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"337\n","output_type":"stream"}]},{"cell_type":"code","source":"contractions = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\n\ncontraction_pattern = re.compile('(%s)' % '|'.join(contractions .keys()))\n\ndef expand_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contraction_pattern.sub(replace, text)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.466626Z","iopub.execute_input":"2024-06-23T08:01:45.466907Z","iopub.status.idle":"2024-06-23T08:01:45.485006Z","shell.execute_reply.started":"2024-06-23T08:01:45.466884Z","shell.execute_reply":"2024-06-23T08:01:45.484176Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n\ndef remove_stopwords(text):\n    tokens = nltk.word_tokenize(text)\n    stop_words = set(final_stopwords_list)\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\ndef lemmatization(text):\n    words = nltk.word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if len(word) > 1]\n    return ' '.join(lemmatized_words)\n\ndef preprocess_with_contractions_and_punctuation_removal(text):\n    text = text.lower() # Convert words to lowercase\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML\n    text = expand_contractions(text)\n    text = remove_stopwords(text) # Remove stopwords\n    text = re.sub(\"@\\w+\", '',text) # Delete strings starting with @\n    text = text.replace(u'\\xa0',' ') # Remove \\xa0\n    text = re.sub(\"'\\d+\", '',text) # Delete Numbers\n    text = re.sub(\"\\d+\", '',text)\n    text = re.sub(r'_+', ' ', text)\n    text = re.sub(\"http\\w+\", '',text)     # Delete URL\n    text = remove_punctuation(text) # Remove punctuation\n    text = re.sub(r\"\\s+\", \" \", text) # Replace consecutive empty spaces with a single space character\n    text = lemmatization(text) # Lemmatizing\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.486073Z","iopub.execute_input":"2024-06-23T08:01:45.486370Z","iopub.status.idle":"2024-06-23T08:01:45.497021Z","shell.execute_reply.started":"2024-06-23T08:01:45.486331Z","shell.execute_reply":"2024-06-23T08:01:45.496165Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocess_data(text):\n    text = text.lower()\n    text = re.sub(r'<.*?>', '', text) # Remove HTML\n    text = re.sub(\"@\\w+\", '',text)     # Delete strings starting with @\n    text = re.sub(\"\\d+\", '',text)\n    text = re.sub(\"'\\d+\", '',text) # Delete Numbers\n    text = re.sub(\"http\\w+\", '',text) # Delete URL\n    text = text.replace(u'\\xa0',' ') # Remove \\xa0\n    text = re.sub(r'_+', ' ', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = expand_contractions(text)\n    # Replace consecutive commas and periods with one comma and period character\n    text = re.sub(r\"\\.+\", \".\", text)\n    text = re.sub(r\"\\,+\", \",\", text)\n    text = re.sub(r\"\\s+\", \" \", text) # Replace consecutive empty spaces with a single space character\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.498103Z","iopub.execute_input":"2024-06-23T08:01:45.498406Z","iopub.status.idle":"2024-06-23T08:01:45.508907Z","shell.execute_reply.started":"2024-06-23T08:01:45.498383Z","shell.execute_reply":"2024-06-23T08:01:45.508142Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### remove_duplicates:\nused to remove duplicating sentence in an essay. There is a case in score 2 where there is a very long essay but the content is repeated setences. To process that kind of behavior, I'm removing duplicating sentences in essays.\n\n### preprocessing_for_paragraphs: \njust normalize the text so that each \\n\\n is garantee to be the separator betweens paragraphs. It handles case where there are abundance of \\n\\n and if the text after the \\n\\n is undercase - which it will just treat as a another sentence in the paragrahph (not as a paragraph).\n\n### extract_paragraphs:\nExtract paragraph by using \\n\\n as delimitor.","metadata":{}},{"cell_type":"code","source":"def remove_duplicates(text):\n    sentences = text.split('. ')\n    \n    # Use an OrderedDict to remove duplicates while preserving order\n    unique_sentences = list(OrderedDict.fromkeys(sentences))\n    \n    # Join the unique sentences back into a single string\n    result = '. '.join(unique_sentences)\n    \n    # Ensure the final sentence ends with a period if it originally did\n    if text.endswith('.'):\n        result += '.'\n    \n    return result\n\ndef extract_sentences(text):\n    # Use a regular expression to split the text into sentences\n    # This will handle periods, exclamation marks, and question marks as sentence terminators\n    sentences = re.split(r'[.!?]+\\s*', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return sentences\n\ndef extract_words(text):\n    words = re.findall(r\"\\w+(?:[-']\\w+)*\", text)\n    return words\n\ndef preprocessing_for_paragraphs(text):\n    # If before /n/n is not a mark, this is not the end of a paragraph    \n    text = re.sub(r'(?<![\\.\\!\\?])\\n\\n', ' ', text)\n\n    #If after \\n\\n is an normal case, replace with space\n    text = re.sub(r'\\n\\n([a-z])', ' ', text)\n    \n    return text.strip()\n\ndef extract_paragraphs(text):\n    processed_text = preprocessing_for_paragraphs(text)\n    paragraphs = processed_text.split('\\n\\n')\n    \n    return paragraphs","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.510128Z","iopub.execute_input":"2024-06-23T08:01:45.510656Z","iopub.status.idle":"2024-06-23T08:01:45.521388Z","shell.execute_reply.started":"2024-06-23T08:01:45.510625Z","shell.execute_reply":"2024-06-23T08:01:45.520579Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# columns = [(pl.col(\"full_text\").apply(extract_paragraphs).alias(\"paragraph\"))]\n# df = pl.from_pandas(df).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\n\n# df = df.with_columns(columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.522733Z","iopub.execute_input":"2024-06-23T08:01:45.523052Z","iopub.status.idle":"2024-06-23T08:01:45.532281Z","shell.execute_reply.started":"2024-06-23T08:01:45.523024Z","shell.execute_reply":"2024-06-23T08:01:45.531460Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"* Paragraph_Preprocess: Calculate features paragraph_len, paragraph_sentence_cnt and paragraph_word_cnt.\n* Paragraph_Eng: Calculate more advanced features that based on classification of paragraph_len - by bucket (the size probably change a lot but there are 2 ways to say this:\n   - Accumalative: The paragraph is > 500, >600...\n   - Bucked: The paragrah is between 0-200, 200-400...\n   The code I'm using bucket classification.\n* Features that it calculate:\n  - Bucket (count accurance) of each paragraph to each bucket defined above.\n  - Calculate features: max, mean, sum, min for each features created in Paragraph_Preprocess","metadata":{}},{"cell_type":"code","source":"# paragraph features\ndef paragraph_preprocess(temp_df):\n    # Expand the paragraph list into several lines of data\n    temp_df = temp_df.explode('paragraph')\n    \n    # Paragraph preprocessing\n    temp_df = temp_df.with_columns(pl.col('paragraph').map_elements(preprocess_data))\n    \n    # Calculate the length of each paragraph\n    temp_df = temp_df.with_columns(\n        pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"),\n        pl.col('paragraph').map_elements(lambda x: len(extract_sentences(x))).alias('paragraph_sentence_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(extract_words(x))).alias('paragraph_word_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(set(extract_words(x)))).alias('paragraph_unique_word_cnt')\n    )\n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.533383Z","iopub.execute_input":"2024-06-23T08:01:45.533693Z","iopub.status.idle":"2024-06-23T08:01:45.541610Z","shell.execute_reply.started":"2024-06-23T08:01:45.533668Z","shell.execute_reply":"2024-06-23T08:01:45.540745Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"length_ranges = [(1, 100), (101, 200), (201, 300), (301, 400), (401, 500), (501, 600), (601, 800)]\nparagraph_fea = ['paragraph_len', 'paragraph_sentence_cnt', 'paragraph_word_cnt', 'paragraph_unique_word_cnt']\n\ndef paragraph_feature_engineering(train_df):\n    count_aggs = [\n        pl.col('paragraph').filter((pl.col('paragraph_len') >= start) & (pl.col('paragraph_len') <= end)).count().alias(f\"paragraph_len_between_{start}_{end}_cnt\")\n        for start, end in length_ranges\n    ]\n\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_len_geq_{i}_cnt\") for i in [100,150,200,300,350,400,500,600,700] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        ]\n\n    df = train_df.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.542680Z","iopub.execute_input":"2024-06-23T08:01:45.542973Z","iopub.status.idle":"2024-06-23T08:01:45.554888Z","shell.execute_reply.started":"2024-06-23T08:01:45.542951Z","shell.execute_reply":"2024-06-23T08:01:45.553992Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = paragraph_preprocess(df)\n# train_features = paragraph_feature_engineering(train_preprocessed)\n\n# # Obtain feature names\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n\n# print(f'Features Number: {len(feature_names)}')\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.555907Z","iopub.execute_input":"2024-06-23T08:01:45.556141Z","iopub.status.idle":"2024-06-23T08:01:45.564010Z","shell.execute_reply.started":"2024-06-23T08:01:45.556121Z","shell.execute_reply":"2024-06-23T08:01:45.563202Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Sentence based features¶\n**Now we will do some sentence processing. The steps is pretty similar to paragraph, which only a few differences:**\n\n* Sentence_Preprocess: Calculate features sentence_len, sentence_word_cnt\n* Sentence_Eng: Calculate bucketed ranges of sentence + max, min, mean of features created in Sentence_Preprocess.","metadata":{}},{"cell_type":"code","source":"def sentence_preprocess(temp_df):\n    # Preprocess full_text and use periods to segment sentences in the text\n    temp_df = temp_df.with_columns( pl.col('full_text').map_elements(preprocess_data).map_elements(extract_sentences).alias(\"sentences\"))\n    temp_df = temp_df.explode('sentences')\n    \n    temp_df = temp_df.with_columns(\n        pl.col('sentences').map_elements(lambda x: len(x)).alias(\"sentence_len\"),\n        pl.col('sentences').map_elements(lambda x: len(extract_words(x))).alias(\"sentence_word_cnt\"),\n        pl.col('sentences').map_elements(lambda x: len(set(extract_words(x)))).alias(\"sentence_unique_word_cnt\")\n    )\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.565025Z","iopub.execute_input":"2024-06-23T08:01:45.565294Z","iopub.status.idle":"2024-06-23T08:01:45.575432Z","shell.execute_reply.started":"2024-06-23T08:01:45.565272Z","shell.execute_reply":"2024-06-23T08:01:45.574641Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sentence_length_ranges = [(1, 50), (51, 100), (101, 150), (151, 300)]\nsentence_fea = ['sentence_len','sentence_word_cnt', 'sentence_unique_word_cnt']\n\ndef sentence_feature_engineering(train_tmp):\n    \n    count_aggs = [\n        pl.col('sentences').filter((pl.col('sentence_len') >= start) & (pl.col('sentence_len') <= end)).count().alias(f\"sentence_len_between_{start}_{end}_cnt\")\n        for start, end in sentence_length_ranges\n    ]\n    \n    aggs = [\n        *[pl.col('sentences').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_len_geq_{i}_cnt\") for i in [50,100,150,300] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n    ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.576341Z","iopub.execute_input":"2024-06-23T08:01:45.576670Z","iopub.status.idle":"2024-06-23T08:01:45.585087Z","shell.execute_reply.started":"2024-06-23T08:01:45.576647Z","shell.execute_reply":"2024-06-23T08:01:45.584268Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = sentence_preprocess(df)\n# train_features = train_features.merge(sentence_feature_engineering(train_preprocessed), on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.586197Z","iopub.execute_input":"2024-06-23T08:01:45.586535Z","iopub.status.idle":"2024-06-23T08:01:45.593685Z","shell.execute_reply.started":"2024-06-23T08:01:45.586506Z","shell.execute_reply":"2024-06-23T08:01:45.592805Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Now we will do some sentence processing. The steps is pretty similar to paragraph and sentence, which only a few differences:**\n\n* Word_Preprocess: Calculate features word_len (making sure word is not empty)\n* Word_Eng: Calulate bucket length for word_len + max, mean, std, sum for features created in Word_Preprocess.","metadata":{}},{"cell_type":"code","source":"def word_preprocess(temp_df):\n    # Preprocess full_text and use spaces to separate words from the text\n    temp_df = temp_df.with_columns(pl.col('full_text').map_elements(preprocess_data).map_elements(extract_words).alias('word'))\n    temp_df = temp_df.explode('word')\n    \n    # Calculate the length of each word\n    temp_df = temp_df.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    \n    # Delete data with a word length of 0\n    temp_df = temp_df.filter(pl.col('word_len') != 0)\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.594751Z","iopub.execute_input":"2024-06-23T08:01:45.595002Z","iopub.status.idle":"2024-06-23T08:01:45.605594Z","shell.execute_reply.started":"2024-06-23T08:01:45.594980Z","shell.execute_reply":"2024-06-23T08:01:45.604811Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"word_length_ranges = [(1, 5), (6, 10), (11, 15)]\n\ndef word_feature_engineering(train_tmp):\n    \n    count_aggs = [\n        pl.col('word').filter((pl.col('word_len') >= start) & (pl.col('word_len') <= end)).count().alias(f\"word_len_between_{start}_{end}_cnt\")\n        for start, end in word_length_ranges\n    ]\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_len_geq_{i+1}_cnt\") for i in range(15) ], \n\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    \n    df = df.to_pandas()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.606756Z","iopub.execute_input":"2024-06-23T08:01:45.607042Z","iopub.status.idle":"2024-06-23T08:01:45.616072Z","shell.execute_reply.started":"2024-06-23T08:01:45.607019Z","shell.execute_reply":"2024-06-23T08:01:45.615262Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = word_preprocess(df)\n\n# # Merge the newly generated feature data with the previously generated feature data\n# train_features = train_features.merge(word_feature_engineering(train_preprocessed), on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.617132Z","iopub.execute_input":"2024-06-23T08:01:45.617475Z","iopub.status.idle":"2024-06-23T08:01:45.624619Z","shell.execute_reply.started":"2024-06-23T08:01:45.617423Z","shell.execute_reply":"2024-06-23T08:01:45.623893Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#  train_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.625558Z","iopub.execute_input":"2024-06-23T08:01:45.625880Z","iopub.status.idle":"2024-06-23T08:01:45.635778Z","shell.execute_reply.started":"2024-06-23T08:01:45.625850Z","shell.execute_reply":"2024-06-23T08:01:45.635014Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Character TFIDF features¶","metadata":{}},{"cell_type":"code","source":"def tokenizer_function(x):\n    return x\n\ndef preprocessor_function(x):\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.636855Z","iopub.execute_input":"2024-06-23T08:01:45.637272Z","iopub.status.idle":"2024-06-23T08:01:45.644863Z","shell.execute_reply.started":"2024-06-23T08:01:45.637241Z","shell.execute_reply":"2024-06-23T08:01:45.644135Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# character_tfidf_vectorizer = TfidfVectorizer(\n#             tokenizer = tokenizer_function,\n#             preprocessor = preprocessor_function,\n#             token_pattern = None,\n#             strip_accents = 'unicode',\n#             analyzer = 'word',\n#             ngram_range = (1,3),\n#             min_df = 0.1,\n#             max_df = 0.95,\n#             sublinear_tf = True,\n# )\n# # Processed text\n# processed_text = df[\"full_text\"].apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n# character_train_tfidf = character_tfidf_vectorizer.fit_transform([i for i in processed_text])\n# joblib.dump(character_tfidf_vectorizer, 'character_tfidf_vectorizer.pkl')\n# character_tfidf_feature_names = character_tfidf_vectorizer.get_feature_names_out()\n# tfidf_features = pd.DataFrame(character_train_tfidf.toarray(), columns=[f\"tfidf_{name}\" for name in character_tfidf_feature_names ])\n# tfidf_features['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(tfidf_features, on='essay_id', how='left')\n\n# print('Character tf-idf features:')\n# print(character_tfidf_feature_names[0:100])\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.645902Z","iopub.execute_input":"2024-06-23T08:01:45.646789Z","iopub.status.idle":"2024-06-23T08:01:45.654279Z","shell.execute_reply.started":"2024-06-23T08:01:45.646738Z","shell.execute_reply":"2024-06-23T08:01:45.653371Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word TF-IDF","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # TfidfVectorizer parameter\n# word_tfidf_vectorizer = TfidfVectorizer(\n#     preprocessor = preprocessor_function,\n#     strip_accents = 'unicode',\n#     analyzer = 'word',\n#     ngram_range = (1, 3),\n#     min_df = 0.05,\n#     max_df = 0.85,\n#     sublinear_tf = True,\n#     stop_words = final_stopwords_list,\n# )\n\n# # Fit all datasets into TfidfVectorizer\n# train_tfidf = word_tfidf_vectorizer.fit_transform([i for i in processed_text])\n# word_tfidf_feature_names = word_tfidf_vectorizer.get_feature_names_out()\n# print('Word tf-idf features:')\n# print(word_tfidf_feature_names[0:100])\n\n# df_temp = pd.DataFrame(train_tfidf.toarray(), columns=[f\"tfidf_{name}\" for name in word_tfidf_feature_names])\n# df_temp['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(df_temp, on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.662732Z","iopub.execute_input":"2024-06-23T08:01:45.663037Z","iopub.status.idle":"2024-06-23T08:01:45.667395Z","shell.execute_reply.started":"2024-06-23T08:01:45.663016Z","shell.execute_reply":"2024-06-23T08:01:45.666481Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(word_tfidf_vectorizer, 'word_tfidf_vectorizer.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.668590Z","iopub.execute_input":"2024-06-23T08:01:45.668874Z","iopub.status.idle":"2024-06-23T08:01:45.678024Z","shell.execute_reply.started":"2024-06-23T08:01:45.668853Z","shell.execute_reply":"2024-06-23T08:01:45.677253Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(word_tfidf_feature_names, 'word_tfidf_feature_names.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.680975Z","iopub.execute_input":"2024-06-23T08:01:45.681263Z","iopub.status.idle":"2024-06-23T08:01:45.686833Z","shell.execute_reply.started":"2024-06-23T08:01:45.681241Z","shell.execute_reply":"2024-06-23T08:01:45.685949Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# processed_text = df[\"full_text\"].apply(lambda x: preprocess_data(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.687836Z","iopub.execute_input":"2024-06-23T08:01:45.688124Z","iopub.status.idle":"2024-06-23T08:01:45.695002Z","shell.execute_reply.started":"2024-06-23T08:01:45.688102Z","shell.execute_reply":"2024-06-23T08:01:45.694240Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# processed_text","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.695942Z","iopub.execute_input":"2024-06-23T08:01:45.696218Z","iopub.status.idle":"2024-06-23T08:01:45.704516Z","shell.execute_reply.started":"2024-06-23T08:01:45.696186Z","shell.execute_reply":"2024-06-23T08:01:45.703727Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Centroid Features\nThe logic for this feature is simple:\n\n* We use TFIDF for the traind data and it looks like it works in the test data -> The essays in the test data must have similar words -> Might have the same topics as the train data.\n* As we have encovered the essay topics in the dataset, we can use K-means to cluster them and calculate the distance between it and the centroid - effectively measure if the essays is close to its topic, and therefore - better.","metadata":{}},{"cell_type":"code","source":"# tfidf_w_columns = [ f'tfidf_{i}' for i in word_tfidf_feature_names]\n# test_tfidf = train_features[tfidf_w_columns]\n# test_tfidf[test_tfidf > 0].agg(['count', 'min', 'max', 'std', 'mean']).T.to_csv('tfidf_test.csv')\n# # Create test copy of dataframe\n# kmean_test = train_features[tfidf_w_columns]\n# # Initialize KMeans with the number of clusters you want\n# kmeans = KMeans(n_clusters=7, random_state=42)\n\n# # Fit the model to the data\n# kmeans.fit(kmean_test)\n\n\n# # Predict the clusters for the data points\n# labels = kmeans.labels_\n\n# # Get the centroids\n# centroids = kmeans.cluster_centers_\n\n# joblib.dump(kmeans, 'kmeans_model.pkl')\n\n# # Calculate the distance to the centroid\n# distances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\n\n# cosine_distances_to_centroid = [\n#     cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n#     for i, label in enumerate(labels)\n# ]\n\n# # Add the distances to the DataFrame\n# kmean_test['DistanceToCentroid'] = distances\n# kmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\n# train_features['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\n# train_features['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.705482Z","iopub.execute_input":"2024-06-23T08:01:45.705734Z","iopub.status.idle":"2024-06-23T08:01:45.713882Z","shell.execute_reply.started":"2024-06-23T08:01:45.705713Z","shell.execute_reply":"2024-06-23T08:01:45.713125Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## CountVectorizer Features","metadata":{}},{"cell_type":"code","source":"# count_vectorizer = CountVectorizer(\n#             strip_accents='unicode',\n#             analyzer = 'word',\n#             ngram_range=(2,3),\n#             min_df=0.05, \n#             max_df=0.85,\n# )\n\n# train_count = count_vectorizer.fit_transform([i for i in processed_text])\n\n# dense_matrix = train_count.toarray()\n# word_count_feature_names = count_vectorizer.get_feature_names_out()\n# print(word_count_feature_names[0:100])\n\n# df = pd.DataFrame(dense_matrix,  columns=[f\"count_{name}\" for name in word_count_feature_names])\n# df['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(df, on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.714917Z","iopub.execute_input":"2024-06-23T08:01:45.715241Z","iopub.status.idle":"2024-06-23T08:01:45.725020Z","shell.execute_reply.started":"2024-06-23T08:01:45.715211Z","shell.execute_reply":"2024-06-23T08:01:45.724223Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(count_vectorizer, 'count_tfidf_vectorizer.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.725927Z","iopub.execute_input":"2024-06-23T08:01:45.726183Z","iopub.status.idle":"2024-06-23T08:01:45.737231Z","shell.execute_reply.started":"2024-06-23T08:01:45.726155Z","shell.execute_reply":"2024-06-23T08:01:45.736387Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Extra features:¶\nFollowings are some extra features.\n\n* spelling: Calculate spelling mistakes in an essay\n* count_sym: Calculate synnonym\n* run: Calculate a bunch of features like unique_word_count, splling_err_num, full_stop_ratio, comma_ratio","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:45.738283Z","iopub.execute_input":"2024-06-23T08:01:45.738575Z","iopub.status.idle":"2024-06-23T08:01:46.987699Z","shell.execute_reply.started":"2024-06-23T08:01:45.738525Z","shell.execute_reply":"2024-06-23T08:01:46.986893Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self) -> None:\n        self.STOP_WORDS = set(final_stopwords_list)\n        self.spellchecker = SpellChecker()\n\n    def spelling(self, text):\n        text_2 = re.sub(r'[^\\w\\s]', ' ', text)\n        amount_miss = len(list(self.spellchecker.unknown(text_2.split())))\n        return amount_miss\n    \n    def find_wrong_punctuation(self, text):\n        punctuations = ['.', ',', ';', '?', '!', ':']\n        lowercase_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n        uppercase_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n\n        # find punctuation in the text\n        wrong_punctuations = 0\n        length = len(text)\n\n        for i in range(length):\n            if text[i] in punctuations and i < length - 1:\n                if text[i + 1] in lowercase_list or text[i + 1] in uppercase_list:\n                    wrong_punctuations += 1\n\n        return wrong_punctuations\n    \n    def noun_verb_adj_adv_adp_others(self, text):\n        doc = nlp(text)\n        pos_counts = doc.count_by(spacy.attrs.POS)\n        nouns = 0\n        verbs = 0\n        adj = 0\n        adv = 0\n        adp_conj = 0\n        others = 0\n\n        for pos_id, count in pos_counts.items():\n            pos_tag = doc.vocab.strings[pos_id]\n            if pos_tag in ['NOUN', 'PROPN', 'PRON']:\n                nouns += count\n            elif pos_tag in ['VERB', 'AUX']:\n                verbs += count\n            elif pos_tag == 'ADJ':\n                adj += count\n            elif pos_tag == 'ADV':\n                adv += count\n            elif pos_tag in ['ADP', 'CONJ']:\n                adp_conj += count\n            else:\n                others += count\n\n        return nouns, verbs, adj, adv, adp_conj, others\n    \n    def count_sym(self, text, sym):\n        sym_count = 0\n        for l in text:\n            if l == sym:\n                sym_count += 1\n        return sym_count\n    \n    def lexical_diversity(self,text):\n        # Tokenize the text into words\n        words = nltk.word_tokenize(text)\n\n        # Calculate the number of unique words (types) and total number of words (tokens)\n        num_types = len(set(words))\n        num_tokens = len(words)\n\n        # Calculate the Type-Token Ratio (TTR)\n        ttr = num_types / num_tokens\n\n        return ttr\n    \n    def calculate_collocation_diversity(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        return len(finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(tokens))\n\n    def calculate_collocation_strength(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        collocations = finder.nbest(BigramAssocMeasures.mi_like, 10)  # Get top 10 collocations\n        return sum(score for bigram, score in finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(collocations))    \n\n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        \n          # preprocessing the text\n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n\n            \n        data[['nouns', 'verbs', 'adj', 'adv', 'adp_conj', 'others']] = data['processed_text'].apply(lambda x: pd.Series(self.noun_verb_adj_adv_adp_others(x)))\n        \n         # distinct word count\n        data['distinct_word_count'] = data['processed_text'].apply(lambda x: len(set(word_tokenize(x))))\n        \n        # coleman\n        data['coleman_liau'] = data['processed_text'].apply(lambda x: textstat.coleman_liau_index(x))\n        \n        # Text tokenization\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        \n         # lexical diversity\n        data['lexical_diversity'] = data['full_text'].apply(lambda x: self.lexical_diversity(x))\n        \n        # collocation diversity\n        data['collocation_diversity'] = data['processed_text'].apply(lambda x: self.calculate_collocation_diversity(x))\n                \n        # collocation strength\n        data['collocation_strength'] = data['processed_text'].apply(lambda x: self.calculate_collocation_strength(x))\n        \n        # essay length\n        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"full_text_length\"] = data[\"full_text\"].apply(lambda x: len(x))\n        \n        # essay word count\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        \n        # essay unique word count\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        \n        # essay sentence count\n        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(extract_sentences(x)))\n        \n        # essay paragraph count\n        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(extract_paragraphs(x)))\n        \n        # count misspelling\n        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        data[\"splling_err_ratio\"] = data[\"splling_err_num\"] / data[\"text_length\"]\n        \n        # ratio fullstop / text_length \n        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n        \n        # ratio comma / text_length\n        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n        \n        return data","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:46.989160Z","iopub.execute_input":"2024-06-23T08:01:46.989775Z","iopub.status.idle":"2024-06-23T08:01:47.016362Z","shell.execute_reply.started":"2024-06-23T08:01:46.989741Z","shell.execute_reply":"2024-06-23T08:01:47.015479Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# df.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.017779Z","iopub.execute_input":"2024-06-23T08:01:47.018069Z","iopub.status.idle":"2024-06-23T08:01:47.047510Z","shell.execute_reply.started":"2024-06-23T08:01:47.018046Z","shell.execute_reply":"2024-06-23T08:01:47.046541Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# preprocessor = Preprocessor()\n# tmp = preprocessor.run(df, mode=\"train\")\n# train_features = train_features.merge(tmp, on='essay_id', how='left')\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.048667Z","iopub.execute_input":"2024-06-23T08:01:47.050715Z","iopub.status.idle":"2024-06-23T08:01:47.056220Z","shell.execute_reply.started":"2024-06-23T08:01:47.050689Z","shell.execute_reply":"2024-06-23T08:01:47.055390Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# train_features.to_csv('output.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.057473Z","iopub.execute_input":"2024-06-23T08:01:47.058237Z","iopub.status.idle":"2024-06-23T08:01:47.064675Z","shell.execute_reply.started":"2024-06-23T08:01:47.058202Z","shell.execute_reply":"2024-06-23T08:01:47.063871Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Test Data","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.065686Z","iopub.execute_input":"2024-06-23T08:01:47.065969Z","iopub.status.idle":"2024-06-23T08:01:47.073269Z","shell.execute_reply.started":"2024-06-23T08:01:47.065947Z","shell.execute_reply":"2024-06-23T08:01:47.072451Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(test_path)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.074228Z","iopub.execute_input":"2024-06-23T08:01:47.074552Z","iopub.status.idle":"2024-06-23T08:01:47.106350Z","shell.execute_reply.started":"2024-06-23T08:01:47.074530Z","shell.execute_reply":"2024-06-23T08:01:47.105472Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"  essay_id                                          full_text\n0  000d118  Many people have car where they live. The thin...\n1  000fe60  I am a scientist at NASA that is discussing th...\n2  001ab80  People always wish they had the same technolog...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>Many people have car where they live. The thin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>I am a scientist at NASA that is discussing th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>People always wish they had the same technolog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"columns = [(pl.col(\"full_text\").apply(extract_paragraphs).alias(\"paragraph\"))]\ntest = pl.from_pandas(test).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.107526Z","iopub.execute_input":"2024-06-23T08:01:47.107778Z","iopub.status.idle":"2024-06-23T08:01:47.198852Z","shell.execute_reply.started":"2024-06-23T08:01:47.107757Z","shell.execute_reply":"2024-06-23T08:01:47.197949Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"test = test.with_columns(columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.199916Z","iopub.execute_input":"2024-06-23T08:01:47.200182Z","iopub.status.idle":"2024-06-23T08:01:47.211775Z","shell.execute_reply.started":"2024-06-23T08:01:47.200160Z","shell.execute_reply":"2024-06-23T08:01:47.210919Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.212871Z","iopub.execute_input":"2024-06-23T08:01:47.213200Z","iopub.status.idle":"2024-06-23T08:01:47.231634Z","shell.execute_reply.started":"2024-06-23T08:01:47.213170Z","shell.execute_reply":"2024-06-23T08:01:47.230701Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"shape: (3, 3)\n┌──────────┬───────────────────────────────────┬───────────────────────────────────┐\n│ essay_id ┆ full_text                         ┆ paragraph                         │\n│ ---      ┆ ---                               ┆ ---                               │\n│ str      ┆ str                               ┆ list[str]                         │\n╞══════════╪═══════════════════════════════════╪═══════════════════════════════════╡\n│ 000d118  ┆ Many people have car where they … ┆ [\"Many people have car where the… │\n│ 000fe60  ┆ I am a scientist at NASA that is… ┆ [\"I am a scientist at NASA that … │\n│ 001ab80  ┆ People always wish they had the … ┆ [\"People always wish they had th… │\n└──────────┴───────────────────────────────────┴───────────────────────────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>paragraph</th></tr><tr><td>str</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people ha…</td><td>[&quot;Many people have car where they live. The thing they don&#x27;t know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban&#x27;s families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won&#x27;t see a car in Vauban&#x27;s streets because they are completely &quot;car free&quot; but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called &quot;smart planning&quot;. The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that &quot;All of our development since World war 2 has been centered on the cars,and that will have to change&quot; and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don&#x27;t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called &quot;car reduced&quot;communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.&quot;]</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scienti…</td><td>[&quot;I am a scientist at NASA that is discussing the &quot;face&quot; on mars. I will be explaining how the &quot;face&quot; is a land form. By sharing my information about this isue i will tell you just that.&quot;, &quot;First off, how could it be a martions drawing. There is no plant life on mars as of rite now that we know of, which means so far as we know it is not possible for any type of life. That explains how it could not be made by martians. Also why and how would a martion build a face so big. It just does not make any since that a martian did this.&quot;, … &quot;To sum all this up the &quot;face&quot; on mars is a landform but others would like to beleive it&#x27;s a martian sculpture. Which every one that works at NASA says it&#x27;s a landform and they are all the ones working on the planet and taking pictures..&quot;]</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always …</td><td>[&quot;People always wish they had the same technology that they have seen in movies, or the best new piece of technology that is all over social media. However, nobody seems to think of the risks that these kinds of new technologies may have. Cars have been around for many decades, and now manufacturers are starting to get on the bandwagon and come up with the new and improved technology that they hope will appeal to everyone. As of right now, it seems as though the negative characteristics of these cars consume the positive idea that these manufacturers have tried to convey.&quot;, &quot;Currently, this new technology in cars has a very long way to go before being completely &quot;driverless&quot;. Drivers still need to be on alert when they are driving, as well as control the car near any accidents or complicated traffic situations. This seems to totally defeat the purpose of the &quot;driverless&quot; car. Eventually the technology may improve, but nobody can be certain that the driverless car will eventually become completely &quot;driverless&quot;. This idea just seems like a lot of hard work and money for something that is not very neccessary. If someone does not want to drive their car they can just take a city bus or a subway. There are so many options of transportation that can already solve this problem. Even if masnufacturers are trying to make driving more &quot;fun&quot;, driving is not meant to be &quot;fun&quot; it is meant to get people where they need to go. Playing around in a car just to have &quot;fun&quot; is just a recipe for disaster.&quot;, … &quot;The technology car manufacturers are trying to develope may just be a diasaster in the making. There are many alternative options of transportations if you do not feel like driving yourself, and these options are way less expensive than buying a brand new car. Although this technology is relatively new, we can not be certain that this new idea will even pay off in the end, it may just be a waste of money and time. Sometimes the newest technology is not the most benefical.&quot;]</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"models_base_path = '/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4'","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.232582Z","iopub.execute_input":"2024-06-23T08:01:47.232835Z","iopub.status.idle":"2024-06-23T08:01:47.239038Z","shell.execute_reply.started":"2024-06-23T08:01:47.232808Z","shell.execute_reply":"2024-06-23T08:01:47.238147Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Paragraph\ntmp = paragraph_preprocess(test)\ntest_features = paragraph_feature_engineering(tmp)\n\n# Sentence\ntmp = sentence_preprocess(test)\ntest_features = test_features.merge(sentence_feature_engineering(tmp), on='essay_id', how='left')\n\n# Word\ntmp = word_preprocess(test)\ntest_features = test_features.merge(word_feature_engineering(tmp), on='essay_id', how='left')\n\n# Character Tfidf\nprocessed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n\ncharacter_tfidf_vectorizer = joblib.load(f\"{models_base_path}/character_tfidf_vectorizer.pkl\")\ntest_tfidf = character_tfidf_vectorizer.transform([i for i in processed_text])\nfeature_names = character_tfidf_vectorizer.get_feature_names_out()\ndf = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\ndf['essay_id'] = test_features['essay_id']\ntest_features = test_features.merge(df, on='essay_id', how='left')\n\nword_tfidf_vectorizer = joblib.load(f\"{models_base_path}/word_tfidf_vectorizer.pkl\")\n# Word Tfidf\ntest_tfidf = word_tfidf_vectorizer.transform([i for i in processed_text])\nfeature_names = word_tfidf_vectorizer.get_feature_names_out()\ndf = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\ndf['essay_id'] = test_features['essay_id']\ntest_features = test_features.merge(df, on='essay_id', how='left')\n\n\nword_tfidf_feature_names = joblib.load(f\"{models_base_path}/word_tfidf_feature_names.pkl\")\ntfidf_w_columns = [ f'tfidf_{i}' for i in word_tfidf_feature_names]\n\n# Fit the model to the data\nkmean_test = test_features[tfidf_w_columns]\n\nkmeans = joblib.load(f\"{models_base_path}/kmeans_model.pkl\")\n\nlabels = kmeans.predict(kmean_test)\ncentroids = kmeans.cluster_centers_\nprint(len(centroids))\ndistances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\ncosine_distances_to_centroid = [\n    cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n    for i, label in enumerate(labels)\n]\n# Add the distances to the DataFrame\nkmean_test['DistanceToCentroid'] = distances\nkmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\ntest_features['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\ntest_features['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']\n\ncount_vectorizer = joblib.load(f\"{models_base_path}/count_tfidf_vectorizer.pkl\")\n# Word vectorize\ntest_count = count_vectorizer.transform([i for i in processed_text])\nfeature_names = count_vectorizer.get_feature_names_out()\ntest_count_df = pd.DataFrame(test_count.toarray(), columns=[f'count_{name}' for name in feature_names])\ntest_count_df['essay_id'] = test_features['essay_id']\ntest_features = test_features.merge(test_count_df, on='essay_id', how='left')\n\n\n# Extra feature\npreprocessor = Preprocessor()\ntmp = preprocessor.run(test.to_pandas(), mode=\"train\")\ntest_features = test_features.merge(tmp, on='essay_id', how='left')\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_features.columns))\nprint('Features number: ',len(feature_names))\ntest_features.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:01:47.240289Z","iopub.execute_input":"2024-06-23T08:01:47.240553Z","iopub.status.idle":"2024-06-23T08:02:10.756097Z","shell.execute_reply.started":"2024-06-23T08:01:47.240531Z","shell.execute_reply":"2024-06-23T08:02:10.755255Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:02<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 1440.68it/s]","output_type":"stream"},{"name":"stdout","text":"Features number:  2339\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"  essay_id  paragraph_len_between_1_100_cnt  \\\n0  000d118                                0   \n1  000fe60                                0   \n2  001ab80                                0   \n\n   paragraph_len_between_101_200_cnt  paragraph_len_between_201_300_cnt  \\\n0                                  0                                  0   \n1                                  1                                  1   \n2                                  0                                  0   \n\n   paragraph_len_between_301_400_cnt  paragraph_len_between_401_500_cnt  \\\n0                                  0                                  0   \n1                                  2                                  1   \n2                                  0                                  1   \n\n   paragraph_len_between_501_600_cnt  paragraph_len_between_601_800_cnt  \\\n0                                  0                                  0   \n1                                  0                                  0   \n2                                  1                                  0   \n\n   paragraph_len_geq_100_cnt  paragraph_len_geq_150_cnt  ...  text_length  \\\n0                          1                          1  ...         1447   \n1                          5                          5  ...          709   \n2                          4                          4  ...         1491   \n\n   full_text_length  word_count  unique_word_count  sentence_count  \\\n0              2677         220                144              13   \n1              1670         108                 68              21   \n2              3077         208                125              24   \n\n   paragraph_count  splling_err_num  splling_err_ratio  fullstop_ratio  \\\n0                1               23           0.015895        0.004856   \n1                5                8           0.011283        0.011976   \n2                4                7           0.004695        0.007800   \n\n   comma_ratio  \n0     0.005230  \n1     0.003593  \n2     0.005200  \n\n[3 rows x 2340 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>paragraph_len_between_1_100_cnt</th>\n      <th>paragraph_len_between_101_200_cnt</th>\n      <th>paragraph_len_between_201_300_cnt</th>\n      <th>paragraph_len_between_301_400_cnt</th>\n      <th>paragraph_len_between_401_500_cnt</th>\n      <th>paragraph_len_between_501_600_cnt</th>\n      <th>paragraph_len_between_601_800_cnt</th>\n      <th>paragraph_len_geq_100_cnt</th>\n      <th>paragraph_len_geq_150_cnt</th>\n      <th>...</th>\n      <th>text_length</th>\n      <th>full_text_length</th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>sentence_count</th>\n      <th>paragraph_count</th>\n      <th>splling_err_num</th>\n      <th>splling_err_ratio</th>\n      <th>fullstop_ratio</th>\n      <th>comma_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1447</td>\n      <td>2677</td>\n      <td>220</td>\n      <td>144</td>\n      <td>13</td>\n      <td>1</td>\n      <td>23</td>\n      <td>0.015895</td>\n      <td>0.004856</td>\n      <td>0.005230</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>709</td>\n      <td>1670</td>\n      <td>108</td>\n      <td>68</td>\n      <td>21</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0.011283</td>\n      <td>0.011976</td>\n      <td>0.003593</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>...</td>\n      <td>1491</td>\n      <td>3077</td>\n      <td>208</td>\n      <td>125</td>\n      <td>24</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0.004695</td>\n      <td>0.007800</td>\n      <td>0.005200</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 2340 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# test_features.to_csv('test_preprocessed.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:10.757298Z","iopub.execute_input":"2024-06-23T08:02:10.757607Z","iopub.status.idle":"2024-06-23T08:02:10.761705Z","shell.execute_reply.started":"2024-06-23T08:02:10.757581Z","shell.execute_reply":"2024-06-23T08:02:10.760700Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Embedding","metadata":{}},{"cell_type":"code","source":"from transformers import DebertaTokenizer, TFDebertaModel","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:10.763132Z","iopub.execute_input":"2024-06-23T08:02:10.763549Z","iopub.status.idle":"2024-06-23T08:02:12.340569Z","shell.execute_reply.started":"2024-06-23T08:02:10.763517Z","shell.execute_reply":"2024-06-23T08:02:12.339738Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class DebertaEmbedder:\n    def __init__(self, model_name=\"microsoft/deberta-base\"):\n        print(\"Loading DeBERTa model...\")\n        self.tokenizer = DebertaTokenizer.from_pretrained(model_name)\n        self.model = TFDebertaModel.from_pretrained(model_name)\n        print(\"Model loaded.\")\n\n    def embed(self, texts, batch_size=64):\n        print(\"Embedding texts using DeBERTa...\")\n        embeddings = []\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n            batch_texts = texts[i:i + batch_size]\n            inputs = self.tokenizer(batch_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n            outputs = self.model(inputs)\n            batch_embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n            embeddings.append(batch_embeddings)\n        embeddings = np.vstack(embeddings)\n        print(\"DeBERTa embeddings computed.\")\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:12.341691Z","iopub.execute_input":"2024-06-23T08:02:12.341953Z","iopub.status.idle":"2024-06-23T08:02:12.349821Z","shell.execute_reply.started":"2024-06-23T08:02:12.341931Z","shell.execute_reply":"2024-06-23T08:02:12.348863Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# processed_df = df['full_text'].apply(lambda x: preprocess_data(x))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:12.350938Z","iopub.execute_input":"2024-06-23T08:02:12.351219Z","iopub.status.idle":"2024-06-23T08:02:12.362464Z","shell.execute_reply.started":"2024-06-23T08:02:12.351197Z","shell.execute_reply":"2024-06-23T08:02:12.361554Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"\nembedder = DebertaEmbedder()\n#embeddings = embedder.embed(processed_df.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:12.363572Z","iopub.execute_input":"2024-06-23T08:02:12.363832Z","iopub.status.idle":"2024-06-23T08:02:28.140331Z","shell.execute_reply.started":"2024-06-23T08:02:12.363811Z","shell.execute_reply":"2024-06-23T08:02:28.139359Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Loading DeBERTa model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"360064beed0f4979b2548128d61482af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962daa46d5224681aa9cfafdf43b13b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce85228a014c4e038a63288851464fb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84fa0114558d42c797e4befecc7fd5c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tf_model.h5:   0%|          | 0.00/555M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a903440420dd4ad6b5692617028b5edb"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFDebertaModel.\n\nAll the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded.\n","output_type":"stream"}]},{"cell_type":"code","source":"# embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.141796Z","iopub.execute_input":"2024-06-23T08:02:28.142451Z","iopub.status.idle":"2024-06-23T08:02:28.146977Z","shell.execute_reply.started":"2024-06-23T08:02:28.142390Z","shell.execute_reply":"2024-06-23T08:02:28.146027Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# num_columns = len(embeddings[0])\n# column_names = [f'deb{i+1}' for i in range(num_columns)]\n\n# # Convert 2D list to DataFrame\n# embedding_df = pd.DataFrame(embeddings, columns=column_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.148209Z","iopub.execute_input":"2024-06-23T08:02:28.149081Z","iopub.status.idle":"2024-06-23T08:02:28.164991Z","shell.execute_reply.started":"2024-06-23T08:02:28.149044Z","shell.execute_reply":"2024-06-23T08:02:28.164040Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# embedding_df.to_csv('embedding_df.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.166249Z","iopub.execute_input":"2024-06-23T08:02:28.166536Z","iopub.status.idle":"2024-06-23T08:02:28.176230Z","shell.execute_reply.started":"2024-06-23T08:02:28.166513Z","shell.execute_reply":"2024-06-23T08:02:28.175399Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# processed_train_df = pd.read_csv('/kaggle/input/processed-test-and-train/train_processed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.177192Z","iopub.execute_input":"2024-06-23T08:02:28.177462Z","iopub.status.idle":"2024-06-23T08:02:28.185535Z","shell.execute_reply.started":"2024-06-23T08:02:28.177416Z","shell.execute_reply":"2024-06-23T08:02:28.184690Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#processed_train_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.186594Z","iopub.execute_input":"2024-06-23T08:02:28.186926Z","iopub.status.idle":"2024-06-23T08:02:28.194932Z","shell.execute_reply.started":"2024-06-23T08:02:28.186895Z","shell.execute_reply":"2024-06-23T08:02:28.194042Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#merged_train = pd.concat([processed_train_df,embedding_df ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.195907Z","iopub.execute_input":"2024-06-23T08:02:28.196187Z","iopub.status.idle":"2024-06-23T08:02:28.205212Z","shell.execute_reply.started":"2024-06-23T08:02:28.196165Z","shell.execute_reply":"2024-06-23T08:02:28.204483Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"#merged_train","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.206233Z","iopub.execute_input":"2024-06-23T08:02:28.206645Z","iopub.status.idle":"2024-06-23T08:02:28.215100Z","shell.execute_reply.started":"2024-06-23T08:02:28.206614Z","shell.execute_reply":"2024-06-23T08:02:28.214354Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Embedding for test data","metadata":{}},{"cell_type":"code","source":"processed__test_df = test['full_text'].apply(lambda x: preprocess_data(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.216165Z","iopub.execute_input":"2024-06-23T08:02:28.216490Z","iopub.status.idle":"2024-06-23T08:02:28.228664Z","shell.execute_reply.started":"2024-06-23T08:02:28.216461Z","shell.execute_reply":"2024-06-23T08:02:28.227916Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"processed__test_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.229752Z","iopub.execute_input":"2024-06-23T08:02:28.230321Z","iopub.status.idle":"2024-06-23T08:02:28.241930Z","shell.execute_reply.started":"2024-06-23T08:02:28.230290Z","shell.execute_reply":"2024-06-23T08:02:28.240929Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"shape: (3,)\nSeries: 'full_text' [str]\n[\n\t\"many people ha…\n\t\"i am a scienti…\n\t\"people always …\n]","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (3,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>full_text</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;many people ha…</td></tr><tr><td>&quot;i am a scienti…</td></tr><tr><td>&quot;people always …</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"embeddings_test = embedder.embed(processed__test_df.to_list())\nnum_columns_test = len(embeddings_test[0])\ncolumn_names = [f'deb{i+1}' for i in range(num_columns_test)]\n\n# Convert 2D list to DataFrame\nembedding_df_test = pd.DataFrame(embeddings_test, columns=column_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:28.243050Z","iopub.execute_input":"2024-06-23T08:02:28.243381Z","iopub.status.idle":"2024-06-23T08:02:31.830585Z","shell.execute_reply.started":"2024-06-23T08:02:28.243350Z","shell.execute_reply":"2024-06-23T08:02:31.829671Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Embedding texts using DeBERTa...\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 1/1 [00:03<00:00,  3.57s/it]","output_type":"stream"},{"name":"stdout","text":"DeBERTa embeddings computed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#embedding_df_test.to_csv('embedding_df_test.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.831810Z","iopub.execute_input":"2024-06-23T08:02:31.832211Z","iopub.status.idle":"2024-06-23T08:02:31.836922Z","shell.execute_reply.started":"2024-06-23T08:02:31.832176Z","shell.execute_reply":"2024-06-23T08:02:31.835697Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# preprocessed_test_df = pd.read_csv('/kaggle/input/processed-test-and-train/test_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.837975Z","iopub.execute_input":"2024-06-23T08:02:31.838233Z","iopub.status.idle":"2024-06-23T08:02:31.848940Z","shell.execute_reply.started":"2024-06-23T08:02:31.838210Z","shell.execute_reply":"2024-06-23T08:02:31.847994Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"merged_test = pd.concat([test_features,embedding_df_test ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.850115Z","iopub.execute_input":"2024-06-23T08:02:31.850483Z","iopub.status.idle":"2024-06-23T08:02:31.860361Z","shell.execute_reply.started":"2024-06-23T08:02:31.850415Z","shell.execute_reply":"2024-06-23T08:02:31.859659Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"merged_test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.861999Z","iopub.execute_input":"2024-06-23T08:02:31.862480Z","iopub.status.idle":"2024-06-23T08:02:31.884020Z","shell.execute_reply.started":"2024-06-23T08:02:31.862431Z","shell.execute_reply":"2024-06-23T08:02:31.882977Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"  essay_id  paragraph_len_between_1_100_cnt  \\\n0  000d118                                0   \n1  000fe60                                0   \n2  001ab80                                0   \n\n   paragraph_len_between_101_200_cnt  paragraph_len_between_201_300_cnt  \\\n0                                  0                                  0   \n1                                  1                                  1   \n2                                  0                                  0   \n\n   paragraph_len_between_301_400_cnt  paragraph_len_between_401_500_cnt  \\\n0                                  0                                  0   \n1                                  2                                  1   \n2                                  0                                  1   \n\n   paragraph_len_between_501_600_cnt  paragraph_len_between_601_800_cnt  \\\n0                                  0                                  0   \n1                                  0                                  0   \n2                                  1                                  0   \n\n   paragraph_len_geq_100_cnt  paragraph_len_geq_150_cnt  ...    deb759  \\\n0                          1                          1  ... -0.191920   \n1                          5                          5  ...  0.190499   \n2                          4                          4  ...  0.094390   \n\n     deb760    deb761    deb762    deb763    deb764    deb765    deb766  \\\n0 -0.201104  0.087497 -0.392641 -0.401659 -0.046133 -0.051493  0.499923   \n1 -0.094903  0.088488 -0.078170 -0.038633  0.153840  0.166372  0.661949   \n2  0.018927 -0.025737 -0.494094  0.171774  0.162165 -0.178303  0.629336   \n\n     deb767    deb768  \n0  0.116558  0.284129  \n1 -0.288759  0.188611  \n2 -0.039709  0.199430  \n\n[3 rows x 3108 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>paragraph_len_between_1_100_cnt</th>\n      <th>paragraph_len_between_101_200_cnt</th>\n      <th>paragraph_len_between_201_300_cnt</th>\n      <th>paragraph_len_between_301_400_cnt</th>\n      <th>paragraph_len_between_401_500_cnt</th>\n      <th>paragraph_len_between_501_600_cnt</th>\n      <th>paragraph_len_between_601_800_cnt</th>\n      <th>paragraph_len_geq_100_cnt</th>\n      <th>paragraph_len_geq_150_cnt</th>\n      <th>...</th>\n      <th>deb759</th>\n      <th>deb760</th>\n      <th>deb761</th>\n      <th>deb762</th>\n      <th>deb763</th>\n      <th>deb764</th>\n      <th>deb765</th>\n      <th>deb766</th>\n      <th>deb767</th>\n      <th>deb768</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.191920</td>\n      <td>-0.201104</td>\n      <td>0.087497</td>\n      <td>-0.392641</td>\n      <td>-0.401659</td>\n      <td>-0.046133</td>\n      <td>-0.051493</td>\n      <td>0.499923</td>\n      <td>0.116558</td>\n      <td>0.284129</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.190499</td>\n      <td>-0.094903</td>\n      <td>0.088488</td>\n      <td>-0.078170</td>\n      <td>-0.038633</td>\n      <td>0.153840</td>\n      <td>0.166372</td>\n      <td>0.661949</td>\n      <td>-0.288759</td>\n      <td>0.188611</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.094390</td>\n      <td>0.018927</td>\n      <td>-0.025737</td>\n      <td>-0.494094</td>\n      <td>0.171774</td>\n      <td>0.162165</td>\n      <td>-0.178303</td>\n      <td>0.629336</td>\n      <td>-0.039709</td>\n      <td>0.199430</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 3108 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# merged_train['score'] = df['score']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.885252Z","iopub.execute_input":"2024-06-23T08:02:31.885615Z","iopub.status.idle":"2024-06-23T08:02:31.890419Z","shell.execute_reply.started":"2024-06-23T08:02:31.885556Z","shell.execute_reply":"2024-06-23T08:02:31.889604Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# merged_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.891504Z","iopub.execute_input":"2024-06-23T08:02:31.891819Z","iopub.status.idle":"2024-06-23T08:02:31.899672Z","shell.execute_reply.started":"2024-06-23T08:02:31.891797Z","shell.execute_reply":"2024-06-23T08:02:31.898840Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"#set(merged_test.columns)-set(merged_train.columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.900552Z","iopub.execute_input":"2024-06-23T08:02:31.900830Z","iopub.status.idle":"2024-06-23T08:02:31.908965Z","shell.execute_reply.started":"2024-06-23T08:02:31.900808Z","shell.execute_reply":"2024-06-23T08:02:31.908254Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## Add k-fold details","metadata":{}},{"cell_type":"code","source":"n_splits = 5\n\nseed = 42","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.909880Z","iopub.execute_input":"2024-06-23T08:02:31.910111Z","iopub.status.idle":"2024-06-23T08:02:31.918683Z","shell.execute_reply.started":"2024-06-23T08:02:31.910084Z","shell.execute_reply":"2024-06-23T08:02:31.917809Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n# for i, (_, val_index) in enumerate(skf.split(merged_train, merged_train[\"score\"])):\n#     merged_train.loc[val_index, \"fold\"] = i\n    \n    \n# print(merged_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.919925Z","iopub.execute_input":"2024-06-23T08:02:31.920301Z","iopub.status.idle":"2024-06-23T08:02:31.927260Z","shell.execute_reply.started":"2024-06-23T08:02:31.920272Z","shell.execute_reply":"2024-06-23T08:02:31.926470Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":" ## Feature selection","metadata":{}},{"cell_type":"code","source":"# target = \"score\"\n# train_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"text_tokens\", \"processed_text\"] + [target]\ntest_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.928213Z","iopub.execute_input":"2024-06-23T08:02:31.928488Z","iopub.status.idle":"2024-06-23T08:02:31.937223Z","shell.execute_reply.started":"2024-06-23T08:02:31.928466Z","shell.execute_reply":"2024-06-23T08:02:31.936313Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def sanitize_feature_names(df):\n    sanitized_columns = {col: re.sub(r'[^\\w]', '_', col) for col in df.columns}\n    df.rename(columns=sanitized_columns, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.938333Z","iopub.execute_input":"2024-06-23T08:02:31.938602Z","iopub.status.idle":"2024-06-23T08:02:31.945976Z","shell.execute_reply.started":"2024-06-23T08:02:31.938580Z","shell.execute_reply":"2024-06-23T08:02:31.945059Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# train_features = sanitize_feature_names(merged_train)\ntest_features = sanitize_feature_names(merged_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.947160Z","iopub.execute_input":"2024-06-23T08:02:31.947901Z","iopub.status.idle":"2024-06-23T08:02:31.964605Z","shell.execute_reply.started":"2024-06-23T08:02:31.947875Z","shell.execute_reply":"2024-06-23T08:02:31.963829Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# train_features","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.965641Z","iopub.execute_input":"2024-06-23T08:02:31.965914Z","iopub.status.idle":"2024-06-23T08:02:31.973221Z","shell.execute_reply.started":"2024-06-23T08:02:31.965893Z","shell.execute_reply":"2024-06-23T08:02:31.972483Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"### Loss  and evaluation methods","metadata":{}},{"cell_type":"code","source":"# # idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n# def quadratic_weighted_kappa(y_true, y_pred):\n#     y_true = (y_true + a).round()\n#     y_pred = (y_pred + a).clip(1, 6).round()\n#     qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    \n#     return 'QWK', qwk, True\n\n# def qwk_obj(y_true, y_pred):\n#     labels = y_true + a\n#     preds = y_pred + a\n#     preds = preds.clip(1, 6)\n#     f = 1/2 * np.sum((preds-labels)**2)\n#     g = 1/2 * np.sum((preds-a)**2 + b)\n#     df = preds - labels\n#     dg = preds - a\n#     grad = (df/g - f*dg / g**2) * len(labels)\n#     hess = np.ones(len(labels))\n    \n#     return grad, hess\n\n# def qwk_param_calc(y):\n#     a = y.mean()\n#     b = (y ** 2).mean() - a ** 2\n    \n#     return np.round(a, 4), np.round(b, 4)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.974315Z","iopub.execute_input":"2024-06-23T08:02:31.974666Z","iopub.status.idle":"2024-06-23T08:02:31.982226Z","shell.execute_reply.started":"2024-06-23T08:02:31.974636Z","shell.execute_reply":"2024-06-23T08:02:31.981478Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# models = []\n\n# callbacks = [\n#     lgb.log_evaluation(period=25), \n#     lgb.early_stopping(stopping_rounds=75, first_metric_only=True)\n# ]\n\n# for fold in range(n_splits):\n\n#     model = lgb.LGBMRegressor(\n#                             objective = qwk_obj,\n#                             metrics = 'None',\n#                             learning_rate = 0.05,\n#                             max_depth = 5,\n#                             num_leaves = 10, \n#                             colsample_bytree = 0.5,   \n#                             reg_alpha = 0.1,  \n#                             reg_lambda = 0.8,\n#                             n_estimators = 1024,\n#                             random_state = seed, \n#                             extra_trees=True,\n#                             class_weight='balanced',\n#                             verbosity = - 1\n#                             )\n    \n#     a, b = qwk_param_calc(train_features[train_features[\"fold\"] != fold][\"score\"])\n    \n#     # Take out the training and validation sets for 5 kfold segmentation separately\n#     X_train = train_features[train_features[\"fold\"] != fold].drop(columns=train_drop_columns)\n#     y_train = train_features[train_features[\"fold\"] != fold][\"score\"] - a\n\n#     X_eval = train_features[train_features[\"fold\"] == fold].drop(columns=train_drop_columns)\n#     y_eval = train_features[train_features[\"fold\"] == fold][\"score\"] - a\n\n#     print('\\nFold_{} Training ================================\\n'.format(fold+1))\n#     print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n    \n#     # Training model\n#     lgb_model = model.fit(\n#                             X_train, y_train,\n#                             eval_names = ['train', 'valid'],\n#                             eval_set = [(X_train, y_train), (X_eval, y_eval)],\n#                             eval_metric = quadratic_weighted_kappa,\n#                             callbacks = callbacks\n#                         )\n    \n#     models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.983452Z","iopub.execute_input":"2024-06-23T08:02:31.984127Z","iopub.status.idle":"2024-06-23T08:02:31.993233Z","shell.execute_reply.started":"2024-06-23T08:02:31.984086Z","shell.execute_reply":"2024-06-23T08:02:31.992269Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"### Validating model","metadata":{}},{"cell_type":"code","source":"# preds, trues = [], []\n\n# for fold, model in enumerate(models):\n#     X_eval_cv = train_features[train_features[\"fold\"] == fold].drop(columns=train_drop_columns)\n#     y_eval_cv = train_features[train_features[\"fold\"] == fold][\"score\"]    \n\n#     pred = model.predict(X_eval_cv) + a\n    \n#     pred[pred < 1] = 1\n#     pred[pred > 6] = 6\n    \n#     trues.extend(y_eval_cv)\n#     preds.extend(np.round(pred, 0))\n    \n#     v_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n    \n#     print(f\"Validation score {fold} : {v_score}\")\n\n# v_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n# print(f\"Validation score : {v_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:31.994292Z","iopub.execute_input":"2024-06-23T08:02:31.994584Z","iopub.status.idle":"2024-06-23T08:02:32.005891Z","shell.execute_reply.started":"2024-06-23T08:02:31.994562Z","shell.execute_reply":"2024-06-23T08:02:32.005010Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"### Analyize prediction","metadata":{}},{"cell_type":"code","source":"# def analyze_preds(trues, preds):\n#     # Create dataframe\n#     model_prec = pd.DataFrame([trues, preds]).T\n#     model_prec.rename(columns = {0: 'trues', 1: 'preds'}, inplace=True)\n#     model_prec['correct'] = model_prec['trues'] == model_prec['preds']\n#     model_prec['count'] = model_prec.groupby('trues')['trues'].transform('count')\n#     model_prec['correct_count'] = model_prec.groupby('trues')['correct'].transform('sum')\n#     model_prec['correct_rate'] = model_prec['correct_count'] / model_prec['count']\n    \n#     # Print binary correction rate\n#     print(model_prec[['trues', 'correct_rate', 'correct_count', 'count']].drop_duplicates().sort_values(by='trues'))\n    \n#     # Plot predictions by score    \n#     def plot_model(ax, counts, true):\n#         bars = ax.bar(counts.index, counts.values, color='skyblue')\n\n#         # Find the index of the column with the specified label\n#         highlight_index = counts.index.get_loc(true)\n\n#         # Highlight the specified column\n#         bars[highlight_index].set_color('orange')\n\n#         ax.set_xlabel('Predicted Values')\n#         ax.set_ylabel('Count')\n#         ax.set_title(\"Score \" + str(true))\n    \n#     score_list = [1,2,3,4,5,6]\n#     test_pred_by_score = [model_prec[model_prec['trues'] ==  score]['preds'].value_counts() for score in score_list]\n\n#     # Create a figure and six subplots arranged in a 2x3 grid\n#     fig, axs = plt.subplots(2, 3, figsize=(15, 10))    \n#     plot_model(axs[0, 0], test_pred_by_score[0], 1)\n#     plot_model(axs[0, 1], test_pred_by_score[1], 2)\n#     plot_model(axs[0, 2], test_pred_by_score[2], 3)\n#     plot_model(axs[1, 0], test_pred_by_score[3], 4)\n#     plot_model(axs[1, 1], test_pred_by_score[4], 5)\n#     plot_model(axs[1, 2], test_pred_by_score[5], 6)\n    \n# analyze_preds(trues, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.007007Z","iopub.execute_input":"2024-06-23T08:02:32.007709Z","iopub.status.idle":"2024-06-23T08:02:32.019083Z","shell.execute_reply.started":"2024-06-23T08:02:32.007676Z","shell.execute_reply":"2024-06-23T08:02:32.018359Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"### See how model workds","metadata":{}},{"cell_type":"code","source":"# X_train_tmp = train_features.drop(columns=train_drop_columns)\n# y_train_tmp = train_features[\"score\"]\n\n# def interact_tree(example_index, tree_index): \n#     example = pd.DataFrame(X_train_tmp.iloc[example_index]).T\n#     score = y_train_tmp.iloc[example_index]\n    \n#     print(\"Score: \", score)\n#     # Plot the first tree\n#     ax = lgb.plot_tree(models[1], tree_index=tree_index, figsize=(20, 8), show_info='data_percentage', example_case=example)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.019992Z","iopub.execute_input":"2024-06-23T08:02:32.020242Z","iopub.status.idle":"2024-06-23T08:02:32.031894Z","shell.execute_reply.started":"2024-06-23T08:02:32.020220Z","shell.execute_reply":"2024-06-23T08:02:32.031129Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# interact_tree(0,200)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.033039Z","iopub.execute_input":"2024-06-23T08:02:32.033344Z","iopub.status.idle":"2024-06-23T08:02:32.043095Z","shell.execute_reply.started":"2024-06-23T08:02:32.033320Z","shell.execute_reply":"2024-06-23T08:02:32.042183Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# top_features = ['splling_err_ratio', 'lexical_diversity', 'paragraph_count', 'word_count', 'DistanceToCentroid', 'word_len_sum']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.055101Z","iopub.execute_input":"2024-06-23T08:02:32.055340Z","iopub.status.idle":"2024-06-23T08:02:32.058948Z","shell.execute_reply.started":"2024-06-23T08:02:32.055320Z","shell.execute_reply":"2024-06-23T08:02:32.058091Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# ax = lgb.plot_importance(models[1], figsize=(20, 200), importance_type=\"split\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.060052Z","iopub.execute_input":"2024-06-23T08:02:32.060316Z","iopub.status.idle":"2024-06-23T08:02:32.067133Z","shell.execute_reply.started":"2024-06-23T08:02:32.060294Z","shell.execute_reply":"2024-06-23T08:02:32.066264Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# import csv\n# feature_importance_df = pd.DataFrame()\n# feature_names = train_features.drop(columns=train_drop_columns).columns\n# feature_importance_df['Feature'] = feature_names\n\n# for i in range(0, 5):\n#     importance = models[i].feature_importances_\n#     feature_importance_df[f'Importance_{i}'] = importance\n\n# feature_importance_df.to_csv(\"feature_importance.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.068342Z","iopub.execute_input":"2024-06-23T08:02:32.068694Z","iopub.status.idle":"2024-06-23T08:02:32.076404Z","shell.execute_reply.started":"2024-06-23T08:02:32.068670Z","shell.execute_reply":"2024-06-23T08:02:32.075573Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# def show_feat_use(feat):\n#     ax = lgb.plot_split_value_histogram(models[2], figsize=(20, 8), feature=feat)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.077584Z","iopub.execute_input":"2024-06-23T08:02:32.077853Z","iopub.status.idle":"2024-06-23T08:02:32.084830Z","shell.execute_reply.started":"2024-06-23T08:02:32.077832Z","shell.execute_reply":"2024-06-23T08:02:32.084039Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# show_feat_use('tfidf_luke')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.085808Z","iopub.execute_input":"2024-06-23T08:02:32.086105Z","iopub.status.idle":"2024-06-23T08:02:32.093662Z","shell.execute_reply.started":"2024-06-23T08:02:32.086078Z","shell.execute_reply":"2024-06-23T08:02:32.092805Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### How overfit model is ","metadata":{}},{"cell_type":"code","source":"# def check_model_fit(model_id):\n#     ax = lgb.plot_metric(models[model_id])\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.094664Z","iopub.execute_input":"2024-06-23T08:02:32.094989Z","iopub.status.idle":"2024-06-23T08:02:32.102301Z","shell.execute_reply.started":"2024-06-23T08:02:32.094959Z","shell.execute_reply":"2024-06-23T08:02:32.101617Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# check_model_fit(2)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.103268Z","iopub.execute_input":"2024-06-23T08:02:32.103551Z","iopub.status.idle":"2024-06-23T08:02:32.111731Z","shell.execute_reply.started":"2024-06-23T08:02:32.103529Z","shell.execute_reply":"2024-06-23T08:02:32.110760Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"# for i in range(len(models)):\n#     models[i].booster_.save_model(f'model_{i}.txt')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.112824Z","iopub.execute_input":"2024-06-23T08:02:32.113091Z","iopub.status.idle":"2024-06-23T08:02:32.121025Z","shell.execute_reply.started":"2024-06-23T08:02:32.113068Z","shell.execute_reply":"2024-06-23T08:02:32.120285Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"models_path = [f\"{models_base_path}/model_0.txt\",f\"{models_base_path}/model_1.txt\",f\"{models_base_path}/model_2.txt\",f\"{models_base_path}/model_3.txt\",f\"{models_base_path}/model_4.txt\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.122054Z","iopub.execute_input":"2024-06-23T08:02:32.122376Z","iopub.status.idle":"2024-06-23T08:02:32.129868Z","shell.execute_reply.started":"2024-06-23T08:02:32.122353Z","shell.execute_reply":"2024-06-23T08:02:32.129089Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"models = []\nfor i in range(n_splits):\n    loaded_model  = lgb.Booster(model_file=models_path[i])\n    models.append(loaded_model)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.130973Z","iopub.execute_input":"2024-06-23T08:02:32.131728Z","iopub.status.idle":"2024-06-23T08:02:32.230986Z","shell.execute_reply.started":"2024-06-23T08:02:32.131702Z","shell.execute_reply":"2024-06-23T08:02:32.230136Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.232197Z","iopub.execute_input":"2024-06-23T08:02:32.232490Z","iopub.status.idle":"2024-06-23T08:02:32.238639Z","shell.execute_reply.started":"2024-06-23T08:02:32.232466Z","shell.execute_reply":"2024-06-23T08:02:32.237640Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"[<lightgbm.basic.Booster at 0x7e900de6f190>,\n <lightgbm.basic.Booster at 0x7e900deaa9b0>,\n <lightgbm.basic.Booster at 0x7e900dea94e0>,\n <lightgbm.basic.Booster at 0x7e900dea9330>,\n <lightgbm.basic.Booster at 0x7e900deaab30>]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"#test_features","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.239559Z","iopub.execute_input":"2024-06-23T08:02:32.239823Z","iopub.status.idle":"2024-06-23T08:02:32.246548Z","shell.execute_reply.started":"2024-06-23T08:02:32.239801Z","shell.execute_reply":"2024-06-23T08:02:32.245853Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"a = 2.9484","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.247441Z","iopub.execute_input":"2024-06-23T08:02:32.247723Z","iopub.status.idle":"2024-06-23T08:02:32.255664Z","shell.execute_reply.started":"2024-06-23T08:02:32.247696Z","shell.execute_reply":"2024-06-23T08:02:32.254914Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor fold, model in enumerate(models):\n    X_eval_cv = test_features.drop(columns=test_drop_columns)\n    pred = model.predict(X_eval_cv) + a    \n    pred[pred < 1] = 1\n    pred[pred > 6] = 6\n    preds.append(pred)\n\n# Combining the 5 model results\nfor i, pred in enumerate(preds):\n    test_features[f\"score_pred_{i}\"] = pred\ntest_features[\"score\"] = np.round(test_features[[f\"score_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1),0).astype('int32')\n\n# Submit to CSV\ntest_features[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.256766Z","iopub.execute_input":"2024-06-23T08:02:32.257914Z","iopub.status.idle":"2024-06-23T08:02:32.334397Z","shell.execute_reply.started":"2024-06-23T08:02:32.257887Z","shell.execute_reply":"2024-06-23T08:02:32.333432Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"test_features.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T08:02:32.335710Z","iopub.execute_input":"2024-06-23T08:02:32.336448Z","iopub.status.idle":"2024-06-23T08:02:32.357066Z","shell.execute_reply.started":"2024-06-23T08:02:32.336398Z","shell.execute_reply":"2024-06-23T08:02:32.356123Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"  essay_id  paragraph_len_between_1_100_cnt  \\\n0  000d118                                0   \n1  000fe60                                0   \n2  001ab80                                0   \n\n   paragraph_len_between_101_200_cnt  paragraph_len_between_201_300_cnt  \\\n0                                  0                                  0   \n1                                  1                                  1   \n2                                  0                                  0   \n\n   paragraph_len_between_301_400_cnt  paragraph_len_between_401_500_cnt  \\\n0                                  0                                  0   \n1                                  2                                  1   \n2                                  0                                  1   \n\n   paragraph_len_between_501_600_cnt  paragraph_len_between_601_800_cnt  \\\n0                                  0                                  0   \n1                                  0                                  0   \n2                                  1                                  0   \n\n   paragraph_len_geq_100_cnt  paragraph_len_geq_150_cnt  ...    deb765  \\\n0                          1                          1  ... -0.051493   \n1                          5                          5  ...  0.166372   \n2                          4                          4  ... -0.178303   \n\n     deb766    deb767    deb768  score_pred_0  score_pred_1  score_pred_2  \\\n0  0.499923  0.116558  0.284129      1.997255      1.915427      1.691940   \n1  0.661949 -0.288759  0.188611      2.979210      2.970032      2.908859   \n2  0.629336 -0.039709  0.199430      4.297535      4.378106      4.214624   \n\n   score_pred_3  score_pred_4  score  \n0      1.751573      2.192802      2  \n1      2.975928      2.875494      3  \n2      4.228871      4.331328      4  \n\n[3 rows x 3114 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>paragraph_len_between_1_100_cnt</th>\n      <th>paragraph_len_between_101_200_cnt</th>\n      <th>paragraph_len_between_201_300_cnt</th>\n      <th>paragraph_len_between_301_400_cnt</th>\n      <th>paragraph_len_between_401_500_cnt</th>\n      <th>paragraph_len_between_501_600_cnt</th>\n      <th>paragraph_len_between_601_800_cnt</th>\n      <th>paragraph_len_geq_100_cnt</th>\n      <th>paragraph_len_geq_150_cnt</th>\n      <th>...</th>\n      <th>deb765</th>\n      <th>deb766</th>\n      <th>deb767</th>\n      <th>deb768</th>\n      <th>score_pred_0</th>\n      <th>score_pred_1</th>\n      <th>score_pred_2</th>\n      <th>score_pred_3</th>\n      <th>score_pred_4</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.051493</td>\n      <td>0.499923</td>\n      <td>0.116558</td>\n      <td>0.284129</td>\n      <td>1.997255</td>\n      <td>1.915427</td>\n      <td>1.691940</td>\n      <td>1.751573</td>\n      <td>2.192802</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.166372</td>\n      <td>0.661949</td>\n      <td>-0.288759</td>\n      <td>0.188611</td>\n      <td>2.979210</td>\n      <td>2.970032</td>\n      <td>2.908859</td>\n      <td>2.975928</td>\n      <td>2.875494</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-0.178303</td>\n      <td>0.629336</td>\n      <td>-0.039709</td>\n      <td>0.199430</td>\n      <td>4.297535</td>\n      <td>4.378106</td>\n      <td>4.214624</td>\n      <td>4.228871</td>\n      <td>4.331328</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 3114 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}