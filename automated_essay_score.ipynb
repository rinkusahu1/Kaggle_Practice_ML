{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8759299,"sourceType":"datasetVersion","datasetId":5262554},{"sourceId":8765673,"sourceType":"datasetVersion","datasetId":5267056},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684},{"sourceId":6065,"sourceType":"modelInstanceVersion","modelInstanceId":4686},{"sourceId":59742,"sourceType":"modelInstanceVersion","modelInstanceId":49982},{"sourceId":68330,"sourceType":"modelInstanceVersion","modelInstanceId":56841}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-29T07:43:29.255920Z","iopub.execute_input":"2024-06-29T07:43:29.256768Z","iopub.status.idle":"2024-06-29T07:43:29.691268Z","shell.execute_reply.started":"2024-06-29T07:43:29.256732Z","shell.execute_reply":"2024-06-29T07:43:29.690159Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/required-packages/textstat-0.7.3-py3-none-any.whl\n/kaggle/input/required-packages/pyspellchecker-0.8.1-py3-none-any.whl\n/kaggle/input/required-packages/Pyphen-0.9.3-py2.py3-none-any.whl\n/kaggle/input/processed-test-and-train/train_processed.csv\n/kaggle/input/processed-test-and-train/test_preprocessed.csv\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n/kaggle/input/automated-essay-scoring-2.0/other/essayscoring/1/model_checkpoint.weights.h5\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/kmeans_model.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_3.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/character_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_1.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/word_tfidf_feature_names.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/count_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_2.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/word_tfidf_vectorizer.pkl\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_0.txt\n/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4/model_4.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install gensim ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:19:45.359046Z","iopub.execute_input":"2024-06-29T07:19:45.360027Z","iopub.status.idle":"2024-06-29T07:19:45.364637Z","shell.execute_reply.started":"2024-06-29T07:19:45.359990Z","shell.execute_reply":"2024-06-29T07:19:45.363215Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn.decomposition import LatentDirichletAllocation, NMF\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# import nltk\n# import gensim\n# from gensim import corpora\n\n# # Download stopwords\n# nltk.download('punkt')\n# nltk.download('stopwords')\n\n# # Sample data\n# documents = [\n#     'I love programming in Python. Python is great for machine learning.',\n#     'Machine learning and deep learning are subsets of artificial intelligence.',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.'\n# ]\n\n# # Preprocessing the text\n# stop_words = set(stopwords.words('english'))\n\n# def preprocess(text):\n#     tokens = word_tokenize(text.lower())\n#     filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n#     return ' '.join(filtered_tokens)\n\n# processed_docs = [preprocess(doc) for doc in documents]\n\n# # Vectorization\n# vectorizer = CountVectorizer()\n# X = vectorizer.fit_transform(processed_docs)\n\n# # LDA using gensim\n# def lda_gensim(docs, num_topics=2, num_words=5):\n#     # Tokenize and create dictionary\n#     tokenized_docs = [doc.split() for doc in docs]\n#     dictionary = corpora.Dictionary(tokenized_docs)\n#     corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n\n#     # Train LDA model\n#     lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n#     topics = lda_model.print_topics(num_words=num_words)\n#     return topics\n\n# lda_topics = lda_gensim(processed_docs)\n# print(\"LDA Topics:\")\n# for topic in lda_topics:\n#     print(topic)\n\n# # NMF using sklearn\n# num_topics = 2\n\n# # TF-IDF Vectorization for NMF\n# tfidf_vectorizer = TfidfVectorizer()\n# tfidf = tfidf_vectorizer.fit_transform(processed_docs)\n\n# nmf_model = NMF(n_components=num_topics, random_state=1)\n# nmf_topics = nmf_model.fit_transform(tfidf)\n\n# def display_topics(model, feature_names, num_top_words):\n#     for topic_idx, topic in enumerate(model.components_):\n#         print(f\"Topic #{topic_idx}:\")\n#         print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n\n# print(\"\\nNMF Topics:\")\n# display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out(), 5)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:27:27.407251Z","iopub.execute_input":"2024-06-29T06:27:27.407666Z","iopub.status.idle":"2024-06-29T06:27:39.130676Z","shell.execute_reply.started":"2024-06-29T06:27:27.407630Z","shell.execute_reply":"2024-06-29T06:27:39.129651Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nLDA Topics:\n(0, '0.147*\"data\" + 0.111*\"python\" + 0.087*\"machine\" + 0.086*\"learning\" + 0.071*\"programming\"')\n(1, '0.100*\"artificial\" + 0.100*\"intelligence\" + 0.100*\"learning\" + 0.060*\"subsets\" + 0.060*\"deep\"')\n\nNMF Topics:\nTopic #0:\ndata python programming science popular\nTopic #1:\nartificial intelligence learning technology future\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.decomposition import LatentDirichletAllocation\n# from sklearn.feature_extraction.text import CountVectorizer\n# from scipy.stats import entropy\n\n# # Sample data (replace with your essays)\n# documents = [\n#     'I love programming in Python. Python is great for machine learning.',\n#     'Machine learning and deep learning are subsets of artificial intelligence.',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.'\n# ]\n\n# # Vectorization\n# vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n# tf = vectorizer.fit_transform(documents)\n\n# # LDA Topic Modeling\n# lda = LatentDirichletAllocation(n_components=3, random_state=42)\n# lda.fit(tf)\n# topic_distributions = lda.transform(tf)\n\n# # Feature Engineering\n# features = []\n# for doc_topics in topic_distributions:\n#     # Topic proportions\n#     proportions = doc_topics\n#     print(doc_topics)\n#     # Dominant topic (index of the maximum proportion)\n#     dominant_topic = np.argmax(proportions)\n    \n#     # Topic entropy\n#     topic_entropy = entropy(proportions)\n\n#     # Combine features\n#     features.append(np.append(proportions, [dominant_topic, topic_entropy]))\n\n# # Convert to numpy array for use in models\n# features = np.array(features)\n\n# print(\"Features for each essay:\")\n# print(features)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:58:12.474900Z","iopub.execute_input":"2024-06-29T06:58:12.475306Z","iopub.status.idle":"2024-06-29T06:58:12.506108Z","shell.execute_reply.started":"2024-06-29T06:58:12.475269Z","shell.execute_reply":"2024-06-29T06:58:12.504908Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[0.06213537 0.88011914 0.05774549]\n[0.06226708 0.8790663  0.05866662]\n[0.76079625 0.12730445 0.1118993 ]\n[0.82138296 0.07351269 0.10510435]\n[0.04994467 0.05340541 0.89664992]\nFeatures for each essay:\n[[0.06213537 0.88011914 0.05774549 1.         0.44970222]\n [0.06226708 0.8790663  0.05866662 1.         0.45255245]\n [0.76079625 0.12730445 0.1118993  0.         0.71546737]\n [0.82138296 0.07351269 0.10510435 0.         0.5902893 ]\n [0.04994467 0.05340541 0.89664992 2.         0.40396098]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:57:59.169042Z","iopub.execute_input":"2024-06-23T15:57:59.169472Z","iopub.status.idle":"2024-06-23T15:58:28.013808Z","shell.execute_reply.started":"2024-06-23T15:57:59.169440Z","shell.execute_reply":"2024-06-23T15:58:28.012467Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d5c85f2b5e0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/vadersentiment/\u001b[0m\u001b[33m\n\u001b[0mCollecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2024.2.2)\nDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install textstat","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:17:03.506398Z","iopub.execute_input":"2024-06-29T07:17:03.506913Z","iopub.status.idle":"2024-06-29T07:17:03.511832Z","shell.execute_reply.started":"2024-06-29T07:17:03.506868Z","shell.execute_reply":"2024-06-29T07:17:03.510645Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Sample text dependency tree","metadata":{}},{"cell_type":"code","source":"# import spacy\n\n# # Load the spaCy English model\n# nlp = spacy.load(\"en_core_web_sm\")\n\n# # Sample data\n# documents = [\n#     'I love programming in Python. Python is great for machine learning.',\n#     'Machine learning and deep learning are subsets of artificial intelligence.',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.'\n# ]\n\n# # Function to calculate parse tree depth and complexity\n# def calculate_parse_tree_depth_and_complexity(doc):\n#     max_depth = 0\n#     def traverse_tree(node, depth):\n#         nonlocal max_depth\n#         if depth > max_depth:\n#             max_depth = depth\n#         for child in node.children:\n#             traverse_tree(child, depth + 1)\n    \n#     for sent in doc.sents:\n#         for token in sent:\n#             traverse_tree(token, 1)\n    \n#     return max_depth\n\n# # Function to extract dependency trees and grammatical relationships\n# def extract_dependency_tree(doc):\n#     dependencies = []\n#     for token in doc:\n#         dependencies.append((token.text, token.dep_, token.head.text))\n#     return dependencies\n\n# # Process each document and display syntactic features\n# for doc in documents:\n#     doc_nlp = nlp(doc)\n#     print(f\"Document: {doc}\")\n    \n#     # Parse Tree Depth and Complexity\n#     parse_tree_depth = calculate_parse_tree_depth_and_complexity(doc_nlp)\n#     print(f\"Parse Tree Depth: {parse_tree_depth}\")\n    \n#     # Dependency Tree\n#     dependency_tree = extract_dependency_tree(doc_nlp)\n#     print(\"Dependency Tree:\")\n#     for dep in dependency_tree:\n#         print(f\"  {dep[0]} ({dep[1]}) -> {dep[2]}\")\n    \n#     print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T16:07:47.931281Z","iopub.execute_input":"2024-06-23T16:07:47.931686Z","iopub.status.idle":"2024-06-23T16:07:53.945187Z","shell.execute_reply.started":"2024-06-23T16:07:47.931652Z","shell.execute_reply":"2024-06-23T16:07:53.943909Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Document: I love programming in Python. Python is great for machine learning.\nParse Tree Depth: 5\nDependency Tree:\n  I (nsubj) -> love\n  love (ROOT) -> love\n  programming (dobj) -> love\n  in (prep) -> programming\n  Python (pobj) -> in\n  . (punct) -> love\n  Python (nsubj) -> is\n  is (ROOT) -> is\n  great (acomp) -> is\n  for (prep) -> great\n  machine (compound) -> learning\n  learning (pobj) -> for\n  . (punct) -> is\n\n\nDocument: Machine learning and deep learning are subsets of artificial intelligence.\nParse Tree Depth: 5\nDependency Tree:\n  Machine (compound) -> learning\n  learning (nsubj) -> are\n  and (cc) -> learning\n  deep (amod) -> learning\n  learning (conj) -> learning\n  are (ROOT) -> are\n  subsets (attr) -> are\n  of (prep) -> subsets\n  artificial (amod) -> intelligence\n  intelligence (pobj) -> of\n  . (punct) -> are\n\n\nDocument: Artificial intelligence is the future of technology.\nParse Tree Depth: 4\nDependency Tree:\n  Artificial (amod) -> intelligence\n  intelligence (nsubj) -> is\n  is (ROOT) -> is\n  the (det) -> future\n  future (attr) -> is\n  of (prep) -> future\n  technology (pobj) -> of\n  . (punct) -> is\n\n\nDocument: Python and R are popular programming languages for data science.\nParse Tree Depth: 5\nDependency Tree:\n  Python (nsubj) -> are\n  and (cc) -> Python\n  R (conj) -> Python\n  are (ROOT) -> are\n  popular (amod) -> languages\n  programming (compound) -> languages\n  languages (attr) -> are\n  for (prep) -> languages\n  data (compound) -> science\n  science (pobj) -> for\n  . (punct) -> are\n\n\nDocument: Data science encompasses machine learning, data analysis, and data visualization.\nParse Tree Depth: 5\nDependency Tree:\n  Data (compound) -> science\n  science (nsubj) -> encompasses\n  encompasses (ROOT) -> encompasses\n  machine (compound) -> learning\n  learning (dobj) -> encompasses\n  , (punct) -> learning\n  data (compound) -> analysis\n  analysis (conj) -> learning\n  , (punct) -> analysis\n  and (cc) -> analysis\n  data (compound) -> visualization\n  visualization (conj) -> analysis\n  . (punct) -> encompasses\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sample  persuasive and engagement indicators","metadata":{}},{"cell_type":"code","source":"# import spacy\n\n# # Load the spaCy English model\n# nlp = spacy.load(\"en_core_web_sm\")\n\n# # Sample data\n# documents = [\n#     'I love programming in Python. Python is great for machine learning.',\n#     'Machine learning and deep learning are subsets of artificial intelligence.',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.',\n#     'She hesitated at the cliffs edge. One step meant freedom or failure. The crowd below held its breath. Her dreams or doubts? She closed her eyes and leapt. The world changed forever.'\n    \n# ]\n\n# # Define persuasive and engagement indicators\n# persuasive_words = {\"therefore\", \"thus\", \"consequently\", \"hence\", \"in conclusion\", \"clearly\", \"obviously\", \"undoubtedly\"}\n# engagement_words = {\"you\", \"your\", \"we\", \"our\"}\n# question_words = {\"what\", \"how\", \"why\", \"when\", \"where\", \"who\"}\n\n# # Function to calculate pragmatic features\n# def calculate_pragmatic_features(text):\n#     doc = nlp(text)\n#     persuasive_count = 0\n#     engagement_count = 0\n#     question_count = 0\n\n#     for token in doc:\n#         # Check for persuasive words\n#         if token.text.lower() in persuasive_words:\n#             persuasive_count += 1\n        \n#         # Check for engagement words\n#         if token.text.lower() in engagement_words:\n#             engagement_count += 1\n\n#     # Check for questions\n#     for sent in doc.sents:\n#         if sent[-1].text == \"?\":\n#             question_count += 1\n#         else:\n#             for token in sent:\n#                 if token.text.lower() in question_words and token.dep_ == \"advmod\":\n#                     question_count += 1\n#                     break\n\n#     return {\n#         \"persuasive_count\": persuasive_count,\n#         \"engagement_count\": engagement_count,\n#         \"question_count\": question_count\n#     }\n\n# # Process each document and display the pragmatic features\n# for doc in documents:\n#     print(f\"Document: {doc}\")\n    \n#     pragmatic_features = calculate_pragmatic_features(doc)\n#     for feature, count in pragmatic_features.items():\n#         print(f\"{feature}: {count}\")\n    \n#     print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T16:13:04.432485Z","iopub.execute_input":"2024-06-23T16:13:04.432893Z","iopub.status.idle":"2024-06-23T16:13:05.357520Z","shell.execute_reply.started":"2024-06-23T16:13:04.432863Z","shell.execute_reply":"2024-06-23T16:13:05.356231Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Document: I love programming in Python. Python is great for machine learning.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 0\n\n\nDocument: Machine learning and deep learning are subsets of artificial intelligence.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 0\n\n\nDocument: Artificial intelligence is the future of technology.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 0\n\n\nDocument: Python and R are popular programming languages for data science.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 0\n\n\nDocument: Data science encompasses machine learning, data analysis, and data visualization.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 0\n\n\nDocument: She hesitated at the cliffs edge. One step meant freedom or failure. The crowd below held its breath. Her dreams or doubts? She closed her eyes and leapt. The world changed forever.\npersuasive_count: 0\nengagement_count: 0\nquestion_count: 1\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sample code readability","metadata":{}},{"cell_type":"code","source":"# import textstat\n\n# # Sample data\n# documents = [\n#     'I love programming in Python. Python is great for machine learning.',\n#     'Machine learning and deep learning are subsets of artificial intelligence.',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.',\n#     'I Love you my Boy'\n# ]\n\n# # Function to calculate readability metrics\n# def calculate_readability_metrics(text):\n#     readability_metrics = {}\n    \n#     # Flesch-Kincaid Readability Score\n#     readability_metrics['Flesch-Kincaid'] = textstat.flesch_kincaid_grade(text)\n    \n#     # Gunning Fog Index\n#     readability_metrics['Gunning Fog'] = textstat.gunning_fog(text)\n    \n#     # Coleman-Liau Index\n#     readability_metrics['Coleman-Liau'] = textstat.coleman_liau_index(text)\n    \n#     return readability_metrics\n\n# # Process each document and display the readability metrics\n# for doc in documents:\n#     print(f\"Document: {doc}\")\n    \n#     metrics = calculate_readability_metrics(doc)\n#     for metric, score in metrics.items():\n#         print(f\"{metric}: {score}\")\n    \n#     print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T16:04:14.587975Z","iopub.execute_input":"2024-06-23T16:04:14.588413Z","iopub.status.idle":"2024-06-23T16:04:14.598088Z","shell.execute_reply.started":"2024-06-23T16:04:14.588378Z","shell.execute_reply":"2024-06-23T16:04:14.596505Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Document: I love programming in Python. Python is great for machine learning.\nFlesch-Kincaid: 3.1\nGunning Fog: 5.84\nColeman-Liau: 7.87\nSMOG: 0.0\n\n\nDocument: Machine learning and deep learning are subsets of artificial intelligence.\nFlesch-Kincaid: 11.9\nGunning Fog: 12.0\nColeman-Liau: 18.36\nSMOG: 0.0\n\n\nDocument: Artificial intelligence is the future of technology.\nFlesch-Kincaid: 15.5\nGunning Fog: 19.94\nColeman-Liau: 17.35\nSMOG: 0.0\n\n\nDocument: Python and R are popular programming languages for data science.\nFlesch-Kincaid: 8.4\nGunning Fog: 12.0\nColeman-Liau: 12.56\nSMOG: 0.0\n\n\nDocument: Data science encompasses machine learning, data analysis, and data visualization.\nFlesch-Kincaid: 17.8\nGunning Fog: 16.0\nColeman-Liau: 21.26\nSMOG: 0.0\n\n\nDocument: I Love you my Boy\nFlesch-Kincaid: -1.9\nGunning Fog: 2.0\nColeman-Liau: -6.65\nSMOG: 0.0\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sample Polarity and sentiment analysis ","metadata":{}},{"cell_type":"code","source":"# import nltk\n# from textblob import TextBlob\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# # Download required nltk data\n# nltk.download('punkt')\n\n# # Sample data\n# documents = [\n#     'I hate programming in Python, it is so mean. Python is great for machine learning.',\n#     'Machine learning and deep learning have a great feature',\n#     'Artificial intelligence is the future of technology.',\n#     'Python and R are popular programming languages for data science.',\n#     'Data science encompasses machine learning, data analysis, and data visualization.'\n# ]\n\n# # Sentiment Analysis using TextBlob\n# def sentiment_textblob(text):\n#     blob = TextBlob(text)\n#     return blob.sentiment.polarity\n\n# # Sentiment Analysis using VADER\n# vader_analyzer = SentimentIntensityAnalyzer()\n\n# def sentiment_vader(text):\n#     scores = vader_analyzer.polarity_scores(text)\n#     return scores['compound']  # Return the compound score as polarity\n\n# # Process each document and display the polarity scores\n# for doc in documents:\n#     print(f\"Document: {doc}\")\n    \n#     # Sentiment Polarity using TextBlob\n#     polarity_textblob = sentiment_textblob(doc)\n#     print(f\"TextBlob Sentiment Polarity: {polarity_textblob}\")\n    \n#     # Sentiment Polarity using VADER\n#     polarity_vader = sentiment_vader(doc)\n#     print(f\"VADER Sentiment Polarity: {polarity_vader}\")\n    \n#     print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:59:36.787985Z","iopub.execute_input":"2024-06-23T15:59:36.789153Z","iopub.status.idle":"2024-06-23T15:59:36.815585Z","shell.execute_reply.started":"2024-06-23T15:59:36.789107Z","shell.execute_reply":"2024-06-23T15:59:36.814315Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nDocument: I hate programming in Python, it is so mean. Python is great for machine learning.\nTextBlob Sentiment Polarity: -0.10416666666666667\nVADER Sentiment Polarity: 0.1027\n\n\nDocument: Machine learning and deep learning have a great feature\nTextBlob Sentiment Polarity: 0.4\nVADER Sentiment Polarity: 0.6249\n\n\nDocument: Artificial intelligence is the future of technology.\nTextBlob Sentiment Polarity: -0.3\nVADER Sentiment Polarity: 0.4767\n\n\nDocument: Python and R are popular programming languages for data science.\nTextBlob Sentiment Polarity: 0.6\nVADER Sentiment Polarity: 0.4215\n\n\nDocument: Data science encompasses machine learning, data analysis, and data visualization.\nTextBlob Sentiment Polarity: 0.0\nVADER Sentiment Polarity: 0.0\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nimport keras\nimport keras.backend as K\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:43:36.230044Z","iopub.execute_input":"2024-06-29T07:43:36.230516Z","iopub.status.idle":"2024-06-29T07:43:49.126827Z","shell.execute_reply.started":"2024-06-29T07:43:36.230485Z","shell.execute_reply":"2024-06-29T07:43:49.126030Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-29 07:43:37.858317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 07:43:37.858466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 07:43:37.983053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install /kaggle/input/required-packages/Pyphen-0.9.3-py2.py3-none-any.whl >> none\n!pip install /kaggle/input/required-packages/pyspellchecker-0.8.1-py3-none-any.whl >> none\n!pip install /kaggle/input/required-packages/textstat-0.7.3-py3-none-any.whl >> none\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:43:52.520560Z","iopub.execute_input":"2024-06-29T07:43:52.521541Z","iopub.status.idle":"2024-06-29T07:44:30.131756Z","shell.execute_reply.started":"2024-06-29T07:43:52.521507Z","shell.execute_reply":"2024-06-29T07:44:30.130560Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:32.335258Z","iopub.execute_input":"2024-06-29T07:44:32.336089Z","iopub.status.idle":"2024-06-29T07:44:32.340795Z","shell.execute_reply.started":"2024-06-29T07:44:32.336051Z","shell.execute_reply":"2024-06-29T07:44:32.339852Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nimport spacy\nfrom spellchecker import SpellChecker\n\nfrom collections import OrderedDict, Counter, defaultdict\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:35.831080Z","iopub.execute_input":"2024-06-29T07:44:35.831835Z","iopub.status.idle":"2024-06-29T07:44:41.671865Z","shell.execute_reply.started":"2024-06-29T07:44:35.831803Z","shell.execute_reply":"2024-06-29T07:44:41.671042Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nimport json, string\nfrom typing import List\n\nimport spacy\nfrom spellchecker import SpellChecker\n\nfrom collections import OrderedDict, Counter, defaultdict\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import log_evaluation, early_stopping\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:44.589177Z","iopub.execute_input":"2024-06-29T07:44:44.590093Z","iopub.status.idle":"2024-06-29T07:44:46.946790Z","shell.execute_reply.started":"2024-06-29T07:44:44.590058Z","shell.execute_reply":"2024-06-29T07:44:46.945857Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"import textstat\nfrom textblob import TextBlob\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:50.367349Z","iopub.execute_input":"2024-06-29T07:44:50.367718Z","iopub.status.idle":"2024-06-29T07:44:50.533868Z","shell.execute_reply.started":"2024-06-29T07:44:50.367689Z","shell.execute_reply":"2024-06-29T07:44:50.533130Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:53.516396Z","iopub.execute_input":"2024-06-29T07:44:53.517479Z","iopub.status.idle":"2024-06-29T07:44:54.798369Z","shell.execute_reply.started":"2024-06-29T07:44:53.517443Z","shell.execute_reply":"2024-06-29T07:44:54.797240Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:44:58.697495Z","iopub.execute_input":"2024-06-29T07:44:58.697873Z","iopub.status.idle":"2024-06-29T07:44:59.434080Z","shell.execute_reply.started":"2024-06-29T07:44:58.697840Z","shell.execute_reply":"2024-06-29T07:44:59.433315Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of score","metadata":{}},{"cell_type":"code","source":"score_distribution = df['score'].value_counts().sort_index()\n\n# Plot the distribution using a bar chart\nscore_distribution.plot(kind='bar', color='skyblue', edgecolor='black')\n\n# Add titles and labels\nplt.title('Distribution of Scores')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:03.197814Z","iopub.execute_input":"2024-06-29T07:45:03.198196Z","iopub.status.idle":"2024-06-29T07:45:03.523940Z","shell.execute_reply.started":"2024-06-29T07:45:03.198168Z","shell.execute_reply":"2024-06-29T07:45:03.523000Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# This is the list for stop words\nstopwords_list = [\n    \"a\", \"about\", \"above\", \"according\", \"across\", \"actually\", \"adj\", \"after\", \"afterwards\", \"again\",\n    \"all\", \"almost\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"an\",\n    \"am\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anywhere\", \"are\", \"aren\",\n    \"aren't\", \"around\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"been\",\n    \"beforehand\", \"begin\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"both\", \"but\", \"by\",\n    \"can\", \"cannot\", \"can't\", \"caption\", \"co\", \"come\", \"could\", \"couldn\", \"couldn't\", \"did\", \"didn\",\n    \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"early\",\n    \"eg\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"etc\", \"even\", \"ever\", \"every\",\n    \"everywhere\", \"except\", \"few\", \"for\", \"found\", \"from\", \"further\", \"had\", \"has\", \"hasn\", \"hasn't\",\n    \"have\", \"haven\", \"haven't\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\",\n    \"hereupon\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"ie\", \"i.e.\", \"if\", \"in\", \"inc\", \"inc.\",\n    \"indeed\", \"instead\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"its\", \"itself\", \"last\", \"late\",\n    \"later\", \"less\", \"let\", \"like\", \"likely\", \"ll\", \"ltd\", \"made\", \"make\", \"makes\", \"many\", \"may\",\n    \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"might\", \"miss\", \"more\", \"most\", \"mostly\", \"mr\", \"mrs\",\n    \"much\", \"must\", \"my\", \"myself\", \"namely\", \"near\", \"neither\", \"never\", \"nevertheless\", \"new\",\n    \"next\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"not\", \"now\", \"NULL\",\n    \"of\", \"off\", \"often\", \"on\", \"once\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"per\", \"perhaps\", \"rather\", \"re\", \"said\", \"same\",\n    \"say\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"several\", \"she\", \"should\", \"shouldn\", \"shouldn't\",\n    \"since\", \"so\", \"some\", \"still\", \"stop\", \"such\", \"taking\", \"ten\", \"than\", \"that\", \"the\", \"their\",\n    \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\",\n    \"therein\", \"thereupon\", \"these\", \"they\", \"this\", \"those\", \"though\", \"thousand\", \"through\",\n    \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\", \"under\", \"unless\",\n    \"unlike\", \"unlikely\", \"until\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"using\", \"ve\", \"very\", \"via\",\n    \"was\", \"wasn\", \"we\", \"well\", \"were\", \"weren\", \"weren't\", \"what\", \"whatever\", \"when\", \"whence\",\n    \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n    \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whose\",\n    \"why\", \"will\", \"with\", \"within\", \"without\", \"won\", \"would\", \"wouldn\", \"wouldn't\", \"yes\", \"yet\",\n    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n]\n\nfinal_stopwords_list = list(set(stopwords.words('english')) | set(stopwords_list))\nprint(len(final_stopwords_list))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:06.481297Z","iopub.execute_input":"2024-06-29T07:45:06.481635Z","iopub.status.idle":"2024-06-29T07:45:06.501973Z","shell.execute_reply.started":"2024-06-29T07:45:06.481611Z","shell.execute_reply":"2024-06-29T07:45:06.501161Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"337\n","output_type":"stream"}]},{"cell_type":"code","source":"contractions = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\n\ncontraction_pattern = re.compile('(%s)' % '|'.join(contractions .keys()))\n\ndef expand_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contraction_pattern.sub(replace, text)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:09.575445Z","iopub.execute_input":"2024-06-29T07:45:09.576200Z","iopub.status.idle":"2024-06-29T07:45:09.593554Z","shell.execute_reply.started":"2024-06-29T07:45:09.576168Z","shell.execute_reply":"2024-06-29T07:45:09.592633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n\ndef remove_stopwords(text):\n    tokens = nltk.word_tokenize(text)\n    stop_words = set(final_stopwords_list)\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\ndef lemmatization(text):\n    words = nltk.word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if len(word) > 1]\n    return ' '.join(lemmatized_words)\n\ndef preprocess_with_contractions_and_punctuation_removal(text):\n    try:\n        text = text.lower() # Convert words to lowercase\n        text = re.sub(r'<.*?>', '', text)  # Remove HTML\n        text = expand_contractions(text)\n        text = remove_stopwords(text) # Remove stopwords\n        text = re.sub(\"@\\w+\", '',text) # Delete strings starting with @\n        text = text.replace(u'\\xa0',' ') # Remove \\xa0\n        text = re.sub(\"'\\d+\", '',text) # Delete Numbers\n        text = re.sub(\"\\d+\", '',text)\n        text = re.sub(r'_+', ' ', text)\n        text = re.sub(\"http\\w+\", '',text)     # Delete URL\n        text = remove_punctuation(text) # Remove punctuation\n        text = re.sub(r\"\\s+\", \" \", text) # Replace consecutive empty spaces with a single space character\n        text = lemmatization(text) # Lemmatizing\n        text = text.strip()\n        return text\n    except Exception as e:\n        return ''\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:18.575623Z","iopub.execute_input":"2024-06-29T07:45:18.576269Z","iopub.status.idle":"2024-06-29T07:45:18.586752Z","shell.execute_reply.started":"2024-06-29T07:45:18.576237Z","shell.execute_reply":"2024-06-29T07:45:18.585879Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# text = \"This is a test string with an 3?@@@..,,431111 \\y21`trqz htrrrt invalid 89 <></>Unicode character: \\uD800\"\n# preprocess_with_contractions_and_punctuation_removal(None)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:05.882561Z","iopub.execute_input":"2024-06-23T14:38:05.882893Z","iopub.status.idle":"2024-06-23T14:38:05.892928Z","shell.execute_reply.started":"2024-06-23T14:38:05.882863Z","shell.execute_reply":"2024-06-23T14:38:05.892019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocess_data(text):\n    try:\n        text = text.lower()\n        text = re.sub(r'<.*?>', '', text) # Remove HTML\n        text = re.sub(\"@\\w+\", '',text)     # Delete strings starting with @\n        text = re.sub(\"\\d+\", '',text)\n        text = re.sub(\"'\\d+\", '',text) # Delete Numbers\n        text = re.sub(\"http\\w+\", '',text) # Delete URL\n        text = text.replace(u'\\xa0',' ') # Remove \\xa0\n        text = re.sub(r'_+', ' ', text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        text = expand_contractions(text)\n        # Replace consecutive commas and periods with one comma and period character\n        text = re.sub(r\"\\.+\", \".\", text)\n        text = re.sub(r\"\\,+\", \",\", text)\n        text = re.sub(r\"\\s+\", \" \", text) # Replace consecutive empty spaces with a single space character\n        text = text.strip()\n        return text\n    except Exception as e:\n        return ''","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:22.224960Z","iopub.execute_input":"2024-06-29T07:45:22.225324Z","iopub.status.idle":"2024-06-29T07:45:22.233128Z","shell.execute_reply.started":"2024-06-29T07:45:22.225297Z","shell.execute_reply":"2024-06-29T07:45:22.232162Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### remove_duplicates:\nused to remove duplicating sentence in an essay. There is a case in score 2 where there is a very long essay but the content is repeated setences. To process that kind of behavior, I'm removing duplicating sentences in essays.\n\n### preprocessing_for_paragraphs: \njust normalize the text so that each \\n\\n is garantee to be the separator betweens paragraphs. It handles case where there are abundance of \\n\\n and if the text after the \\n\\n is undercase - which it will just treat as a another sentence in the paragrahph (not as a paragraph).\n\n### extract_paragraphs:\nExtract paragraph by using \\n\\n as delimitor.","metadata":{}},{"cell_type":"code","source":"def remove_duplicates(text):\n    sentences = text.split('. ')\n    \n    # Use an OrderedDict to remove duplicates while preserving order\n    unique_sentences = list(OrderedDict.fromkeys(sentences))\n    \n    # Join the unique sentences back into a single string\n    result = '. '.join(unique_sentences)\n    \n    # Ensure the final sentence ends with a period if it originally did\n    if text.endswith('.'):\n        result += '.'\n    \n    return result\n\ndef extract_sentences(text):\n    # Use a regular expression to split the text into sentences\n    # This will handle periods, exclamation marks, and question marks as sentence terminators\n    sentences = re.split(r'[.!?]+\\s*', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return sentences\n\ndef extract_words(text):\n    words = re.findall(r\"\\w+(?:[-']\\w+)*\", text)\n    return words\n\ndef preprocessing_for_paragraphs(text):\n    # If before /n/n is not a mark, this is not the end of a paragraph    \n    text = re.sub(r'(?<![\\.\\!\\?])\\n\\n', ' ', text)\n\n    #If after \\n\\n is an normal case, replace with space\n    text = re.sub(r'\\n\\n([a-z])', ' ', text)\n    \n    return text.strip()\n\ndef extract_paragraphs(text):\n    processed_text = preprocessing_for_paragraphs(text)\n    paragraphs = processed_text.split('\\n\\n')\n    \n    return paragraphs","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:27.012568Z","iopub.execute_input":"2024-06-29T07:45:27.013183Z","iopub.status.idle":"2024-06-29T07:45:27.021402Z","shell.execute_reply.started":"2024-06-29T07:45:27.013152Z","shell.execute_reply":"2024-06-29T07:45:27.020460Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# columns = [(pl.col(\"full_text\").apply(extract_paragraphs).alias(\"paragraph\"))]\n# df = pl.from_pandas(df).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\n\n# df = df.with_columns(columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:05.915833Z","iopub.execute_input":"2024-06-23T14:38:05.916088Z","iopub.status.idle":"2024-06-23T14:38:05.925075Z","shell.execute_reply.started":"2024-06-23T14:38:05.916062Z","shell.execute_reply":"2024-06-23T14:38:05.924329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Paragraph_Preprocess: Calculate features paragraph_len, paragraph_sentence_cnt and paragraph_word_cnt.\n* Paragraph_Eng: Calculate more advanced features that based on classification of paragraph_len - by bucket (the size probably change a lot but there are 2 ways to say this:\n   - Accumalative: The paragraph is > 500, >600...\n   - Bucked: The paragrah is between 0-200, 200-400...\n   The code I'm using bucket classification.\n* Features that it calculate:\n  - Bucket (count accurance) of each paragraph to each bucket defined above.\n  - Calculate features: max, mean, sum, min for each features created in Paragraph_Preprocess","metadata":{}},{"cell_type":"code","source":"# paragraph features\ndef paragraph_preprocess(temp_df):\n    # Expand the paragraph list into several lines of data\n    temp_df = temp_df.explode('paragraph')\n    \n    # Paragraph preprocessing\n    temp_df = temp_df.with_columns(pl.col('paragraph').map_elements(preprocess_data))\n    \n    # Calculate the length of each paragraph\n    temp_df = temp_df.with_columns(\n        pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"),\n        pl.col('paragraph').map_elements(lambda x: len(extract_sentences(x))).alias('paragraph_sentence_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(extract_words(x))).alias('paragraph_word_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(set(extract_words(x)))).alias('paragraph_unique_word_cnt')\n    )\n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:31.637317Z","iopub.execute_input":"2024-06-29T07:45:31.637683Z","iopub.status.idle":"2024-06-29T07:45:31.644787Z","shell.execute_reply.started":"2024-06-29T07:45:31.637655Z","shell.execute_reply":"2024-06-29T07:45:31.643873Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"length_ranges = [(1, 100), (101, 200), (201, 300), (301, 400), (401, 500), (501, 600), (601, 800)]\nparagraph_fea = ['paragraph_len', 'paragraph_sentence_cnt', 'paragraph_word_cnt', 'paragraph_unique_word_cnt']\n\ndef paragraph_feature_engineering(train_df):\n    count_aggs = [\n        pl.col('paragraph').filter((pl.col('paragraph_len') >= start) & (pl.col('paragraph_len') <= end)).count().alias(f\"paragraph_len_between_{start}_{end}_cnt\")\n        for start, end in length_ranges\n    ]\n\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_len_geq_{i}_cnt\") for i in [100,150,200,300,350,400,500,600,700] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        ]\n\n    df = train_df.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:33.335688Z","iopub.execute_input":"2024-06-29T07:45:33.336285Z","iopub.status.idle":"2024-06-29T07:45:33.346130Z","shell.execute_reply.started":"2024-06-29T07:45:33.336253Z","shell.execute_reply":"2024-06-29T07:45:33.345157Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = paragraph_preprocess(df)\n# train_features = paragraph_feature_engineering(train_preprocessed)\n\n# # Obtain feature names\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n\n# print(f'Features Number: {len(feature_names)}')\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:05.947844Z","iopub.execute_input":"2024-06-23T14:38:05.948111Z","iopub.status.idle":"2024-06-23T14:38:05.955626Z","shell.execute_reply.started":"2024-06-23T14:38:05.948080Z","shell.execute_reply":"2024-06-23T14:38:05.954857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence based features¶\n**Now we will do some sentence processing. The steps is pretty similar to paragraph, which only a few differences:**\n\n* Sentence_Preprocess: Calculate features sentence_len, sentence_word_cnt\n* Sentence_Eng: Calculate bucketed ranges of sentence + max, min, mean of features created in Sentence_Preprocess.","metadata":{}},{"cell_type":"code","source":"def sentence_preprocess(temp_df):\n    # Preprocess full_text and use periods to segment sentences in the text\n    temp_df = temp_df.with_columns( pl.col('full_text').map_elements(preprocess_data).map_elements(extract_sentences).alias(\"sentences\"))\n    temp_df = temp_df.explode('sentences')\n    \n    temp_df = temp_df.with_columns(\n        pl.col('sentences').map_elements(lambda x: len(x)).alias(\"sentence_len\"),\n        pl.col('sentences').map_elements(lambda x: len(extract_words(x))).alias(\"sentence_word_cnt\"),\n        pl.col('sentences').map_elements(lambda x: len(set(extract_words(x)))).alias(\"sentence_unique_word_cnt\")\n    )\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:35.639063Z","iopub.execute_input":"2024-06-29T07:45:35.639957Z","iopub.status.idle":"2024-06-29T07:45:35.646479Z","shell.execute_reply.started":"2024-06-29T07:45:35.639923Z","shell.execute_reply":"2024-06-29T07:45:35.645432Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"sentence_length_ranges = [(1, 50), (51, 100), (101, 150), (151, 300)]\nsentence_fea = ['sentence_len','sentence_word_cnt', 'sentence_unique_word_cnt']\n\ndef sentence_feature_engineering(train_tmp):\n    \n    count_aggs = [\n        pl.col('sentences').filter((pl.col('sentence_len') >= start) & (pl.col('sentence_len') <= end)).count().alias(f\"sentence_len_between_{start}_{end}_cnt\")\n        for start, end in sentence_length_ranges\n    ]\n    \n    aggs = [\n        *[pl.col('sentences').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_len_geq_{i}_cnt\") for i in [50,100,150,300] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n    ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:40.198489Z","iopub.execute_input":"2024-06-29T07:45:40.198849Z","iopub.status.idle":"2024-06-29T07:45:40.207417Z","shell.execute_reply.started":"2024-06-29T07:45:40.198822Z","shell.execute_reply":"2024-06-29T07:45:40.206257Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = sentence_preprocess(df)\n# train_features = train_features.merge(sentence_feature_engineering(train_preprocessed), on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:05.978710Z","iopub.execute_input":"2024-06-23T14:38:05.979332Z","iopub.status.idle":"2024-06-23T14:38:05.986971Z","shell.execute_reply.started":"2024-06-23T14:38:05.979301Z","shell.execute_reply":"2024-06-23T14:38:05.986064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we will do some sentence processing. The steps is pretty similar to paragraph and sentence, which only a few differences:**\n\n* Word_Preprocess: Calculate features word_len (making sure word is not empty)\n* Word_Eng: Calulate bucket length for word_len + max, mean, std, sum for features created in Word_Preprocess.","metadata":{}},{"cell_type":"code","source":"def word_preprocess(temp_df):\n    # Preprocess full_text and use spaces to separate words from the text\n    temp_df = temp_df.with_columns(pl.col('full_text').map_elements(preprocess_data).map_elements(extract_words).alias('word'))\n    temp_df = temp_df.explode('word')\n    \n    # Calculate the length of each word\n    temp_df = temp_df.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    \n    # Delete data with a word length of 0\n    temp_df = temp_df.filter(pl.col('word_len') != 0)\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:43.948503Z","iopub.execute_input":"2024-06-29T07:45:43.949085Z","iopub.status.idle":"2024-06-29T07:45:43.954981Z","shell.execute_reply.started":"2024-06-29T07:45:43.949053Z","shell.execute_reply":"2024-06-29T07:45:43.953951Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"word_length_ranges = [(1, 5), (6, 10), (11, 15)]\n\ndef word_feature_engineering(train_tmp):\n    \n    count_aggs = [\n        pl.col('word').filter((pl.col('word_len') >= start) & (pl.col('word_len') <= end)).count().alias(f\"word_len_between_{start}_{end}_cnt\")\n        for start, end in word_length_ranges\n    ]\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_len_geq_{i+1}_cnt\") for i in range(15) ], \n\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    \n    df = df.to_pandas()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:45.767882Z","iopub.execute_input":"2024-06-29T07:45:45.768524Z","iopub.status.idle":"2024-06-29T07:45:45.776986Z","shell.execute_reply.started":"2024-06-29T07:45:45.768491Z","shell.execute_reply":"2024-06-29T07:45:45.775927Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# train_preprocessed = word_preprocess(df)\n\n# # Merge the newly generated feature data with the previously generated feature data\n# train_features = train_features.merge(word_feature_engineering(train_preprocessed), on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:48:41.432440Z","iopub.execute_input":"2024-06-23T14:48:41.432813Z","iopub.status.idle":"2024-06-23T14:48:41.436762Z","shell.execute_reply.started":"2024-06-23T14:48:41.432786Z","shell.execute_reply":"2024-06-23T14:48:41.435899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  train_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:48:41.471236Z","iopub.execute_input":"2024-06-23T14:48:41.471594Z","iopub.status.idle":"2024-06-23T14:48:41.475975Z","shell.execute_reply.started":"2024-06-23T14:48:41.471565Z","shell.execute_reply":"2024-06-23T14:48:41.474900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Character TFIDF features¶","metadata":{}},{"cell_type":"code","source":"def tokenizer_function(x):\n    return x\n\ndef preprocessor_function(x):\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:45:48.383896Z","iopub.execute_input":"2024-06-29T07:45:48.384621Z","iopub.status.idle":"2024-06-29T07:45:48.388957Z","shell.execute_reply.started":"2024-06-29T07:45:48.384589Z","shell.execute_reply":"2024-06-29T07:45:48.387860Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# character_tfidf_vectorizer = TfidfVectorizer(\n#             tokenizer = tokenizer_function,\n#             preprocessor = preprocessor_function,\n#             token_pattern = None,\n#             strip_accents = 'unicode',\n#             analyzer = 'word',\n#             ngram_range = (1,3),\n#             min_df = 0.1,\n#             max_df = 0.95,\n#             sublinear_tf = True,\n# )\n# # Processed text\n# processed_text = df[\"full_text\"].apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n# character_train_tfidf = character_tfidf_vectorizer.fit_transform([i for i in processed_text])\n# joblib.dump(character_tfidf_vectorizer, 'character_tfidf_vectorizer.pkl')\n# character_tfidf_feature_names = character_tfidf_vectorizer.get_feature_names_out()\n# tfidf_features = pd.DataFrame(character_train_tfidf.toarray(), columns=[f\"tfidf_{name}\" for name in character_tfidf_feature_names ])\n# tfidf_features['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(tfidf_features, on='essay_id', how='left')\n\n# print('Character tf-idf features:')\n# print(character_tfidf_feature_names[0:100])\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.044757Z","iopub.execute_input":"2024-06-23T14:38:06.045020Z","iopub.status.idle":"2024-06-23T14:38:06.053498Z","shell.execute_reply.started":"2024-06-23T14:38:06.044999Z","shell.execute_reply":"2024-06-23T14:38:06.052650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word TF-IDF","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # TfidfVectorizer parameter\n# word_tfidf_vectorizer = TfidfVectorizer(\n#     preprocessor = preprocessor_function,\n#     strip_accents = 'unicode',\n#     analyzer = 'word',\n#     ngram_range = (1, 3),\n#     min_df = 0.05,\n#     max_df = 0.85,\n#     sublinear_tf = True,\n#     stop_words = final_stopwords_list,\n# )\n\n# # Fit all datasets into TfidfVectorizer\n# train_tfidf = word_tfidf_vectorizer.fit_transform([i for i in processed_text])\n# word_tfidf_feature_names = word_tfidf_vectorizer.get_feature_names_out()\n# print('Word tf-idf features:')\n# print(word_tfidf_feature_names[0:100])\n\n# df_temp = pd.DataFrame(train_tfidf.toarray(), columns=[f\"tfidf_{name}\" for name in word_tfidf_feature_names])\n# df_temp['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(df_temp, on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.054674Z","iopub.execute_input":"2024-06-23T14:38:06.054988Z","iopub.status.idle":"2024-06-23T14:38:06.063075Z","shell.execute_reply.started":"2024-06-23T14:38:06.054959Z","shell.execute_reply":"2024-06-23T14:38:06.062128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(word_tfidf_vectorizer, 'word_tfidf_vectorizer.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.064121Z","iopub.execute_input":"2024-06-23T14:38:06.064418Z","iopub.status.idle":"2024-06-23T14:38:06.074839Z","shell.execute_reply.started":"2024-06-23T14:38:06.064392Z","shell.execute_reply":"2024-06-23T14:38:06.073979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(word_tfidf_feature_names, 'word_tfidf_feature_names.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.075875Z","iopub.execute_input":"2024-06-23T14:38:06.076210Z","iopub.status.idle":"2024-06-23T14:38:06.083923Z","shell.execute_reply.started":"2024-06-23T14:38:06.076178Z","shell.execute_reply":"2024-06-23T14:38:06.083015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processed_text = df[\"full_text\"].apply(lambda x: preprocess_data(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.085062Z","iopub.execute_input":"2024-06-23T14:38:06.085391Z","iopub.status.idle":"2024-06-23T14:38:06.091625Z","shell.execute_reply.started":"2024-06-23T14:38:06.085361Z","shell.execute_reply":"2024-06-23T14:38:06.090863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processed_text","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.092628Z","iopub.execute_input":"2024-06-23T14:38:06.092945Z","iopub.status.idle":"2024-06-23T14:38:06.099702Z","shell.execute_reply.started":"2024-06-23T14:38:06.092915Z","shell.execute_reply":"2024-06-23T14:38:06.098919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Centroid Features\nThe logic for this feature is simple:\n\n* We use TFIDF for the traind data and it looks like it works in the test data -> The essays in the test data must have similar words -> Might have the same topics as the train data.\n* As we have encovered the essay topics in the dataset, we can use K-means to cluster them and calculate the distance between it and the centroid - effectively measure if the essays is close to its topic, and therefore - better.","metadata":{}},{"cell_type":"code","source":"# tfidf_w_columns = [ f'tfidf_{i}' for i in word_tfidf_feature_names]\n# test_tfidf = train_features[tfidf_w_columns]\n# test_tfidf[test_tfidf > 0].agg(['count', 'min', 'max', 'std', 'mean']).T.to_csv('tfidf_test.csv')\n# # Create test copy of dataframe\n# kmean_test = train_features[tfidf_w_columns]\n# # Initialize KMeans with the number of clusters you want\n# kmeans = KMeans(n_clusters=7, random_state=42)\n\n# # Fit the model to the data\n# kmeans.fit(kmean_test)\n\n\n# # Predict the clusters for the data points\n# labels = kmeans.labels_\n\n# # Get the centroids\n# centroids = kmeans.cluster_centers_\n\n# joblib.dump(kmeans, 'kmeans_model.pkl')\n\n# # Calculate the distance to the centroid\n# distances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\n\n# cosine_distances_to_centroid = [\n#     cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n#     for i, label in enumerate(labels)\n# ]\n\n# # Add the distances to the DataFrame\n# kmean_test['DistanceToCentroid'] = distances\n# kmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\n# train_features['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\n# train_features['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.100661Z","iopub.execute_input":"2024-06-23T14:38:06.100902Z","iopub.status.idle":"2024-06-23T14:38:06.109379Z","shell.execute_reply.started":"2024-06-23T14:38:06.100881Z","shell.execute_reply":"2024-06-23T14:38:06.108627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CountVectorizer Features","metadata":{}},{"cell_type":"code","source":"# count_vectorizer = CountVectorizer(\n#             strip_accents='unicode',\n#             analyzer = 'word',\n#             ngram_range=(2,3),\n#             min_df=0.05, \n#             max_df=0.85,\n# )\n\n# train_count = count_vectorizer.fit_transform([i for i in processed_text])\n\n# dense_matrix = train_count.toarray()\n# word_count_feature_names = count_vectorizer.get_feature_names_out()\n# print(word_count_feature_names[0:100])\n\n# df = pd.DataFrame(dense_matrix,  columns=[f\"count_{name}\" for name in word_count_feature_names])\n# df['essay_id'] = train_features['essay_id']\n# train_features = train_features.merge(df, on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n# print('Features Number: ',len(feature_names))\n# train_features.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.110349Z","iopub.execute_input":"2024-06-23T14:38:06.110573Z","iopub.status.idle":"2024-06-23T14:38:06.120792Z","shell.execute_reply.started":"2024-06-23T14:38:06.110553Z","shell.execute_reply":"2024-06-23T14:38:06.120011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(count_vectorizer, 'count_tfidf_vectorizer.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:06.121891Z","iopub.execute_input":"2024-06-23T14:38:06.122204Z","iopub.status.idle":"2024-06-23T14:38:06.132494Z","shell.execute_reply.started":"2024-06-23T14:38:06.122173Z","shell.execute_reply":"2024-06-23T14:38:06.131647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Extra features:¶\nFollowings are some extra features.\n\n* spelling: Calculate spelling mistakes in an essay\n* count_sym: Calculate synnonym\n* run: Calculate a bunch of features like unique_word_count, splling_err_num, full_stop_ratio, comma_ratio","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:36:39.991395Z","iopub.execute_input":"2024-06-29T07:36:39.991779Z","iopub.status.idle":"2024-06-29T07:36:41.190031Z","shell.execute_reply.started":"2024-06-29T07:36:39.991748Z","shell.execute_reply":"2024-06-29T07:36:41.189125Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self) -> None:\n        self.STOP_WORDS = set(final_stopwords_list)\n        self.spellchecker = SpellChecker()\n\n    def spelling(self, text):\n        text_2 = re.sub(r'[^\\w\\s]', ' ', text)\n        amount_miss = len(list(self.spellchecker.unknown(text_2.split())))\n        return amount_miss\n    \n    def find_wrong_punctuation(self, text):\n        punctuations = ['.', ',', ';', '?', '!', ':']\n        lowercase_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n        uppercase_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n\n        # find punctuation in the text\n        wrong_punctuations = 0\n        length = len(text)\n\n        for i in range(length):\n            if text[i] in punctuations and i < length - 1:\n                if text[i + 1] in lowercase_list or text[i + 1] in uppercase_list:\n                    wrong_punctuations += 1\n\n        return wrong_punctuations\n    \n    def noun_verb_adj_adv_adp_others(self, text):\n        doc = nlp(text)\n        pos_counts = doc.count_by(spacy.attrs.POS)\n        nouns = 0\n        verbs = 0\n        adj = 0\n        adv = 0\n        adp_conj = 0\n        others = 0\n\n        for pos_id, count in pos_counts.items():\n            pos_tag = doc.vocab.strings[pos_id]\n            if pos_tag in ['NOUN', 'PROPN', 'PRON']:\n                nouns += count\n            elif pos_tag in ['VERB', 'AUX']:\n                verbs += count\n            elif pos_tag == 'ADJ':\n                adj += count\n            elif pos_tag == 'ADV':\n                adv += count\n            elif pos_tag in ['ADP', 'CONJ']:\n                adp_conj += count\n            else:\n                others += count\n\n        return nouns, verbs, adj, adv, adp_conj, others\n    \n    def count_sym(self, text, sym):\n        sym_count = 0\n        for l in text:\n            if l == sym:\n                sym_count += 1\n        return sym_count\n    \n    def lexical_diversity(self,text):\n        # Tokenize the text into words\n        words = nltk.word_tokenize(text)\n\n        # Calculate the number of unique words (types) and total number of words (tokens)\n        num_types = len(set(words))\n        num_tokens = len(words)\n\n        # Calculate the Type-Token Ratio (TTR)\n        ttr = num_types / num_tokens\n\n        return ttr\n    \n    def calculate_collocation_diversity(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        return len(finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(tokens))\n\n    def calculate_collocation_strength(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        collocations = finder.nbest(BigramAssocMeasures.mi_like, 10)  # Get top 10 collocations\n        return sum(score for bigram, score in finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(collocations))    \n\n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        \n          # preprocessing the text\n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n\n            \n        data[['nouns', 'verbs', 'adj', 'adv', 'adp_conj', 'others']] = data['processed_text'].apply(lambda x: pd.Series(self.noun_verb_adj_adv_adp_others(x)))\n        \n         # distinct word count\n        data['distinct_word_count'] = data['processed_text'].apply(lambda x: len(set(word_tokenize(x))))\n        \n        # coleman\n        data['coleman_liau'] = data['processed_text'].apply(lambda x: textstat.coleman_liau_index(x))\n        \n        # Text tokenization\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        \n         # lexical diversity\n        data['lexical_diversity'] = data['full_text'].apply(lambda x: self.lexical_diversity(x))\n        \n        # collocation diversity\n        data['collocation_diversity'] = data['processed_text'].apply(lambda x: self.calculate_collocation_diversity(x))\n                \n        # collocation strength\n        data['collocation_strength'] = data['processed_text'].apply(lambda x: self.calculate_collocation_strength(x))\n        \n        # essay length\n        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"full_text_length\"] = data[\"full_text\"].apply(lambda x: len(x))\n        \n        # essay word count\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        \n        # essay unique word count\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        \n        # essay sentence count\n        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(extract_sentences(x)))\n        \n        # essay paragraph count\n        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(extract_paragraphs(x)))\n        \n        # count misspelling\n        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        data[\"splling_err_ratio\"] = data[\"splling_err_num\"] / data[\"text_length\"]\n        \n        # ratio fullstop / text_length \n        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n        \n        # ratio comma / text_length\n        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n        \n        return data","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:48:51.490287Z","iopub.execute_input":"2024-06-23T14:48:51.490668Z","iopub.status.idle":"2024-06-23T14:48:51.518754Z","shell.execute_reply.started":"2024-06-23T14:48:51.490638Z","shell.execute_reply":"2024-06-23T14:48:51.517793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:07.377500Z","iopub.execute_input":"2024-06-23T14:38:07.377888Z","iopub.status.idle":"2024-06-23T14:38:07.388994Z","shell.execute_reply.started":"2024-06-23T14:38:07.377857Z","shell.execute_reply":"2024-06-23T14:38:07.388076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor = Preprocessor()\n# tmp = preprocessor.run(df, mode=\"train\")\n# train_features = train_features.merge(tmp, on='essay_id', how='left')\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_features.columns))\n\n# print('Features Number: ',len(feature_names))\n# train_features.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:07.390078Z","iopub.execute_input":"2024-06-23T14:38:07.390414Z","iopub.status.idle":"2024-06-23T14:38:07.399464Z","shell.execute_reply.started":"2024-06-23T14:38:07.390391Z","shell.execute_reply":"2024-06-23T14:38:07.398533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_features.to_csv('output.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:07.400801Z","iopub.execute_input":"2024-06-23T14:38:07.401297Z","iopub.status.idle":"2024-06-23T14:38:07.409507Z","shell.execute_reply.started":"2024-06-23T14:38:07.401256Z","shell.execute_reply":"2024-06-23T14:38:07.408491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Readabilty metrics","metadata":{}},{"cell_type":"code","source":"def preprocess_full_text_with_readability(temp_df):\n    # Preprocess the full text\n    temp_df = temp_df.with_columns(pl.col('full_text').map_elements(preprocess_data))\n    \n    # Calculate readability metrics\n    temp_df = temp_df.with_columns(\n        pl.col('full_text').map_elements(lambda x: textstat.flesch_kincaid_grade(x)).alias('Flesch-Kincaid'),\n        pl.col('full_text').map_elements(lambda x: textstat.gunning_fog(x)).alias('Gunning Fog'),\n        pl.col('full_text').map_elements(lambda x: textstat.coleman_liau_index(x)).alias('Coleman-Liau')\n    )\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:59:23.851414Z","iopub.execute_input":"2024-06-29T07:59:23.852040Z","iopub.status.idle":"2024-06-29T07:59:23.858303Z","shell.execute_reply.started":"2024-06-29T07:59:23.852008Z","shell.execute_reply":"2024-06-29T07:59:23.857311Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Test Data","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:46:25.544080Z","iopub.execute_input":"2024-06-29T07:46:25.544809Z","iopub.status.idle":"2024-06-29T07:46:25.548693Z","shell.execute_reply.started":"2024-06-29T07:46:25.544778Z","shell.execute_reply":"2024-06-29T07:46:25.547756Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(test_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:47:26.578935Z","iopub.execute_input":"2024-06-29T07:47:26.579815Z","iopub.status.idle":"2024-06-29T07:47:26.589798Z","shell.execute_reply.started":"2024-06-29T07:47:26.579780Z","shell.execute_reply":"2024-06-29T07:47:26.588919Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# test = test.drop(columns=['score'])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:07.443233Z","iopub.execute_input":"2024-06-23T14:38:07.443571Z","iopub.status.idle":"2024-06-23T14:38:07.447396Z","shell.execute_reply.started":"2024-06-23T14:38:07.443541Z","shell.execute_reply":"2024-06-23T14:38:07.446478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:37:41.792652Z","iopub.execute_input":"2024-06-29T07:37:41.793040Z","iopub.status.idle":"2024-06-29T07:37:41.808666Z","shell.execute_reply.started":"2024-06-29T07:37:41.793012Z","shell.execute_reply":"2024-06-29T07:37:41.807716Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"  essay_id                                          full_text\n0  000d118  Many people have car where they live. The thin...\n1  000fe60  I am a scientist at NASA that is discussing th...\n2  001ab80  People always wish they had the same technolog...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>Many people have car where they live. The thin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>I am a scientist at NASA that is discussing th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ab80</td>\n      <td>People always wish they had the same technolog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"try: \n    columns = [(pl.col(\"full_text\").apply(extract_paragraphs).alias(\"paragraph\"))]\n    test = pl.from_pandas(test).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\n    test = test.with_columns(columns)\nexcept Exception as e:\n    logging.error(f\"Error during conversion to polars dataframe and removal of duplicates: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:03:40.672455Z","iopub.execute_input":"2024-06-23T15:03:40.673046Z","iopub.status.idle":"2024-06-23T15:03:40.776392Z","shell.execute_reply.started":"2024-06-23T15:03:40.673014Z","shell.execute_reply":"2024-06-23T15:03:40.775453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:03:43.709453Z","iopub.execute_input":"2024-06-23T15:03:43.710443Z","iopub.status.idle":"2024-06-23T15:03:43.724444Z","shell.execute_reply.started":"2024-06-23T15:03:43.710406Z","shell.execute_reply":"2024-06-23T15:03:43.723443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_base_path = '/kaggle/input/automated-essay-scoring-2.0/other/lgbm/4'","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:03:52.586920Z","iopub.execute_input":"2024-06-23T15:03:52.587548Z","iopub.status.idle":"2024-06-23T15:03:52.591571Z","shell.execute_reply.started":"2024-06-23T15:03:52.587513Z","shell.execute_reply":"2024-06-23T15:03:52.590599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # Paragraph\n    logging.info(\"Starting paragraph preprocessing\")\n    tmp = paragraph_preprocess(test)\n    test_features = paragraph_feature_engineering(tmp)\n    logging.info(\"Paragraph preprocessing completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during paragraph preprocessing: {e}\")\nprint(test_features)\ntry:\n    # Sentence\n    logging.info(\"Starting sentence preprocessing\")\n    tmp = sentence_preprocess(test)\n    test_features = test_features.merge(sentence_feature_engineering(tmp), on='essay_id', how='left')\n    logging.info(\"Sentence preprocessing completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during sentence preprocessing: {e}\")\n\ntry:\n    # Word\n    logging.info(\"Starting word preprocessing\")\n    tmp = word_preprocess(test)\n    test_features = test_features.merge(word_feature_engineering(tmp), on='essay_id', how='left')\n    logging.info(\"Word preprocessing completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during word preprocessing: {e}\")\n\ntry:\n    # Character Tfidf\n    logging.info(\"Starting character TF-IDF vectorization\")\n    processed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: preprocess_with_contractions_and_punctuation_removal(x))\n\n    character_tfidf_vectorizer = joblib.load(f\"{models_base_path}/character_tfidf_vectorizer.pkl\")\n    test_tfidf = character_tfidf_vectorizer.transform([i for i in processed_text])\n    feature_names = character_tfidf_vectorizer.get_feature_names_out()\n    df = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\n    df['essay_id'] = test_features['essay_id']\n    test_features = test_features.merge(df, on='essay_id', how='left')\n    logging.info(\"Character TF-IDF vectorization completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during character TF-IDF vectorization: {e}\")\n\ntry:\n    # Word Tfidf\n    logging.info(\"Starting word TF-IDF vectorization\")\n    word_tfidf_vectorizer = joblib.load(f\"{models_base_path}/word_tfidf_vectorizer.pkl\")\n    test_tfidf = word_tfidf_vectorizer.transform([i for i in processed_text])\n    feature_names = word_tfidf_vectorizer.get_feature_names_out()\n    df = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\n    df['essay_id'] = test_features['essay_id']\n    test_features = test_features.merge(df, on='essay_id', how='left')\n    logging.info(\"Word TF-IDF vectorization completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during word TF-IDF vectorization: {e}\")\n\ntry:\n    # KMeans\n    logging.info(\"Starting KMeans clustering\")\n    word_tfidf_feature_names = joblib.load(f\"{models_base_path}/word_tfidf_feature_names.pkl\")\n    tfidf_w_columns = [f'tfidf_{i}' for i in word_tfidf_feature_names]\n\n    kmean_test = test_features[tfidf_w_columns]\n    kmeans = joblib.load(f\"{models_base_path}/kmeans_model.pkl\")\n\n    labels = kmeans.predict(kmean_test)\n    centroids = kmeans.cluster_centers_\n    logging.info(f\"Number of centroids: {len(centroids)}\")\n    distances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\n    cosine_distances_to_centroid = [\n        cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n        for i, label in enumerate(labels)\n    ]\n\n    kmean_test['DistanceToCentroid'] = distances\n    kmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\n    test_features['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\n    test_features['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']\n    logging.info(\"KMeans clustering completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during KMeans clustering: {e}\")\n\ntry:\n    # Word vectorize with Count Vectorizer\n    logging.info(\"Starting word vectorization with Count Vectorizer\")\n    count_vectorizer = joblib.load(f\"{models_base_path}/count_tfidf_vectorizer.pkl\")\n    test_count = count_vectorizer.transform([i for i in processed_text])\n    feature_names = count_vectorizer.get_feature_names_out()\n    test_count_df = pd.DataFrame(test_count.toarray(), columns=[f'count_{name}' for name in feature_names])\n    test_count_df['essay_id'] = test_features['essay_id']\n    test_features = test_features.merge(test_count_df, on='essay_id', how='left')\n    logging.info(\"Word vectorization with Count Vectorizer completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during word vectorization with Count Vectorizer: {e}\")\n\ntry:\n    # Extra feature\n    logging.info(\"Starting extra feature preprocessing\")\n    preprocessor = Preprocessor()\n    tmp = preprocessor.run(test.to_pandas(), mode=\"train\")\n    test_features = test_features.merge(tmp, on='essay_id', how='left')\n    logging.info(\"Extra feature preprocessing completed successfully\")\nexcept Exception as e:\n    logging.error(f\"Error during extra feature preprocessing: {e}\")\n\ntry:\n    # Features number\n    feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_features.columns))\n    logging.info(f'Features number: {len(feature_names)}')\n    logging.info(f\"First few rows of test_features: \\n{test_features.head(3)}\")\nexcept Exception as e:\n    logging.error(f\"Error during feature extraction or final merging: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:04:03.174499Z","iopub.execute_input":"2024-06-23T15:04:03.174897Z","iopub.status.idle":"2024-06-23T15:04:26.498855Z","shell.execute_reply.started":"2024-06-23T15:04:03.174865Z","shell.execute_reply":"2024-06-23T15:04:26.498072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:04:53.382980Z","iopub.execute_input":"2024-06-23T15:04:53.383868Z","iopub.status.idle":"2024-06-23T15:04:53.400362Z","shell.execute_reply.started":"2024-06-23T15:04:53.383833Z","shell.execute_reply":"2024-06-23T15:04:53.399455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Embedding","metadata":{}},{"cell_type":"code","source":"#from transformers import DebertaTokenizer, TFDebertaModel","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:31.501704Z","iopub.execute_input":"2024-06-23T14:38:31.502386Z","iopub.status.idle":"2024-06-23T14:38:33.148424Z","shell.execute_reply.started":"2024-06-23T14:38:31.502343Z","shell.execute_reply":"2024-06-23T14:38:33.147646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DebertaEmbedder:\n    def __init__(self):\n        print(\"Loading DeBERTa model...\")\n        \n        self.tokenizer = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n                                        \"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2\",\n                                        sequence_length=512)\n        self.model = keras_nlp.models.DebertaV3Backbone.from_preset(\n                        \"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2\",\n                        load_weights=True\n                    )\n        print(\"Model loaded.\")\n\n    def embed(self, texts, batch_size=64):\n        print(\"Embedding texts using DeBERTa...\")\n        embeddings = []\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n            batch_texts = texts[i:i + batch_size]\n            inputs = self.tokenizer(batch_texts)\n            outputs = self.model(inputs)\n            batch_embeddings = tf.reduce_mean(outputs, axis=1).numpy()\n            embeddings.append(batch_embeddings)\n        embeddings = np.vstack(embeddings)\n        print(\"DeBERTa embeddings computed.\")\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:47:45.907050Z","iopub.execute_input":"2024-06-29T07:47:45.907774Z","iopub.status.idle":"2024-06-29T07:47:45.916413Z","shell.execute_reply.started":"2024-06-29T07:47:45.907740Z","shell.execute_reply":"2024-06-29T07:47:45.915200Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"processed_df = df['full_text'].apply(lambda x: preprocess_data(x))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:47:53.492147Z","iopub.execute_input":"2024-06-29T07:47:53.492525Z","iopub.status.idle":"2024-06-29T07:48:10.853029Z","shell.execute_reply.started":"2024-06-29T07:47:53.492498Z","shell.execute_reply":"2024-06-29T07:48:10.851995Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\nembedder = DebertaEmbedder()\nembeddings = embedder.embed(processed_df.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:48:14.834667Z","iopub.execute_input":"2024-06-29T07:48:14.835390Z","iopub.status.idle":"2024-06-29T07:58:51.167092Z","shell.execute_reply.started":"2024-06-29T07:48:14.835354Z","shell.execute_reply":"2024-06-29T07:58:51.166154Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Loading DeBERTa model...\nModel loaded.\nEmbedding texts using DeBERTa...\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 271/271 [10:24<00:00,  2.30s/it]","output_type":"stream"},{"name":"stdout","text":"DeBERTa embeddings computed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:03:05.088915Z","iopub.execute_input":"2024-06-29T08:03:05.089780Z","iopub.status.idle":"2024-06-29T08:03:05.095506Z","shell.execute_reply.started":"2024-06-29T08:03:05.089746Z","shell.execute_reply":"2024-06-29T08:03:05.094616Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(17307, 768)"},"metadata":{}}]},{"cell_type":"code","source":"num_columns = len(embeddings[0])\ncolumn_names = [f'deb{i+1}' for i in range(num_columns)]\n\n# Convert 2D list to DataFrame\nembedding_df = pd.DataFrame(embeddings, columns=column_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:03:07.376541Z","iopub.execute_input":"2024-06-29T08:03:07.376915Z","iopub.status.idle":"2024-06-29T08:03:07.382494Z","shell.execute_reply.started":"2024-06-29T08:03:07.376886Z","shell.execute_reply":"2024-06-29T08:03:07.381578Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"embedding_df.to_csv('embedding_df_train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:03:14.524028Z","iopub.execute_input":"2024-06-29T08:03:14.524411Z","iopub.status.idle":"2024-06-29T08:03:31.946142Z","shell.execute_reply.started":"2024-06-29T08:03:14.524383Z","shell.execute_reply":"2024-06-29T08:03:31.945081Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"processed_train_df = pd.read_csv('/kaggle/input/processed-test-and-train/train_processed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:02:35.062500Z","iopub.execute_input":"2024-06-23T15:02:35.062859Z","iopub.status.idle":"2024-06-23T15:02:48.124390Z","shell.execute_reply.started":"2024-06-23T15:02:35.062832Z","shell.execute_reply":"2024-06-23T15:02:48.123579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_train_df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:02:51.146382Z","iopub.execute_input":"2024-06-23T15:02:51.146995Z","iopub.status.idle":"2024-06-23T15:02:51.181293Z","shell.execute_reply.started":"2024-06-23T15:02:51.146962Z","shell.execute_reply":"2024-06-23T15:02:51.180408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train = pd.concat([processed_train_df,embedding_df ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:02:59.826771Z","iopub.execute_input":"2024-06-23T15:02:59.827753Z","iopub.status.idle":"2024-06-23T15:03:00.027285Z","shell.execute_reply.started":"2024-06-23T15:02:59.827715Z","shell.execute_reply":"2024-06-23T15:03:00.026153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:03:01.833999Z","iopub.execute_input":"2024-06-23T15:03:01.834375Z","iopub.status.idle":"2024-06-23T15:03:01.865036Z","shell.execute_reply.started":"2024-06-23T15:03:01.834347Z","shell.execute_reply":"2024-06-23T15:03:01.864137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding for test data","metadata":{}},{"cell_type":"code","source":"processed__test_df = test['full_text'].apply(lambda x: preprocess_data(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:04:56.692982Z","iopub.execute_input":"2024-06-29T08:04:56.693358Z","iopub.status.idle":"2024-06-29T08:04:56.702424Z","shell.execute_reply.started":"2024-06-29T08:04:56.693332Z","shell.execute_reply":"2024-06-29T08:04:56.701474Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"processed__test_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:04:58.720746Z","iopub.execute_input":"2024-06-29T08:04:58.721330Z","iopub.status.idle":"2024-06-29T08:04:58.728345Z","shell.execute_reply.started":"2024-06-29T08:04:58.721300Z","shell.execute_reply":"2024-06-29T08:04:58.727300Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0    many people have car where they live. the thin...\n1    i am a scientist at nasa that is discussing th...\n2    people always wish they had the same technolog...\nName: full_text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"processed__test_df","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:05:00.474626Z","iopub.execute_input":"2024-06-29T08:05:00.474991Z","iopub.status.idle":"2024-06-29T08:05:00.483147Z","shell.execute_reply.started":"2024-06-29T08:05:00.474962Z","shell.execute_reply":"2024-06-29T08:05:00.482142Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0    many people have car where they live. the thin...\n1    i am a scientist at nasa that is discussing th...\n2    people always wish they had the same technolog...\nName: full_text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"embedding_df_test = None\ntry:\n    print(\"Hello\")\n    logging.info(f\"bedrta embedding start: \")\n    embeddings_test = embedder.embed(processed__test_df.to_list())\n    print(embeddings_test)\n    num_columns_test = len(embeddings_test[0])\n    column_names = [f'deb{i+1}' for i in range(num_columns_test)]\n\n    # Convert 2D list to DataFrame\n    embedding_df_test = pd.DataFrame(embeddings_test, columns=column_names)\nexcept Exception as e:\n    logging.error(f\"Error during embedding deberta: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:05:09.494338Z","iopub.execute_input":"2024-06-29T08:05:09.494729Z","iopub.status.idle":"2024-06-29T08:05:09.859978Z","shell.execute_reply.started":"2024-06-29T08:05:09.494701Z","shell.execute_reply":"2024-06-29T08:05:09.858976Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Hello\nEmbedding texts using DeBERTa...\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]","output_type":"stream"},{"name":"stdout","text":"DeBERTa embeddings computed.\n[[-0.12210955 -0.02981236  0.2710009  ... -0.22081536 -0.04706628\n  -0.22053224]\n [-0.06428286 -0.04472257  0.23339188 ... -0.10864227 -0.07053053\n  -0.01639501]\n [-0.05931892 -0.06240807  0.33283776 ...  0.00240716 -0.09783541\n  -0.17576145]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_df_test","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:05:29.197636Z","iopub.execute_input":"2024-06-29T08:05:29.197982Z","iopub.status.idle":"2024-06-29T08:05:29.226870Z","shell.execute_reply.started":"2024-06-29T08:05:29.197956Z","shell.execute_reply":"2024-06-29T08:05:29.225661Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"       deb1      deb2      deb3      deb4      deb5      deb6      deb7  \\\n0 -0.122110 -0.029812  0.271001 -0.054723 -0.225351 -0.012073  0.229846   \n1 -0.064283 -0.044723  0.233392  0.015479  0.026259  0.024546  0.260620   \n2 -0.059319 -0.062408  0.332838  0.007388  0.033437 -0.174701  0.256190   \n\n       deb8      deb9     deb10  ...    deb759    deb760    deb761    deb762  \\\n0 -0.057254  0.155538 -0.147127  ... -0.217344  0.018540 -0.282586 -0.157145   \n1 -0.059868  0.080771  0.025266  ... -0.141937  0.087901 -0.330568 -0.236012   \n2 -0.033434  0.021591  0.009860  ... -0.246763  0.169429 -0.384459 -0.209756   \n\n     deb763    deb764    deb765    deb766    deb767    deb768  \n0  0.170728 -0.166559  0.246083 -0.220815 -0.047066 -0.220532  \n1  0.032010 -0.179224  0.215996 -0.108642 -0.070531 -0.016395  \n2 -0.003857 -0.085820  0.395393  0.002407 -0.097835 -0.175761  \n\n[3 rows x 768 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deb1</th>\n      <th>deb2</th>\n      <th>deb3</th>\n      <th>deb4</th>\n      <th>deb5</th>\n      <th>deb6</th>\n      <th>deb7</th>\n      <th>deb8</th>\n      <th>deb9</th>\n      <th>deb10</th>\n      <th>...</th>\n      <th>deb759</th>\n      <th>deb760</th>\n      <th>deb761</th>\n      <th>deb762</th>\n      <th>deb763</th>\n      <th>deb764</th>\n      <th>deb765</th>\n      <th>deb766</th>\n      <th>deb767</th>\n      <th>deb768</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.122110</td>\n      <td>-0.029812</td>\n      <td>0.271001</td>\n      <td>-0.054723</td>\n      <td>-0.225351</td>\n      <td>-0.012073</td>\n      <td>0.229846</td>\n      <td>-0.057254</td>\n      <td>0.155538</td>\n      <td>-0.147127</td>\n      <td>...</td>\n      <td>-0.217344</td>\n      <td>0.018540</td>\n      <td>-0.282586</td>\n      <td>-0.157145</td>\n      <td>0.170728</td>\n      <td>-0.166559</td>\n      <td>0.246083</td>\n      <td>-0.220815</td>\n      <td>-0.047066</td>\n      <td>-0.220532</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.064283</td>\n      <td>-0.044723</td>\n      <td>0.233392</td>\n      <td>0.015479</td>\n      <td>0.026259</td>\n      <td>0.024546</td>\n      <td>0.260620</td>\n      <td>-0.059868</td>\n      <td>0.080771</td>\n      <td>0.025266</td>\n      <td>...</td>\n      <td>-0.141937</td>\n      <td>0.087901</td>\n      <td>-0.330568</td>\n      <td>-0.236012</td>\n      <td>0.032010</td>\n      <td>-0.179224</td>\n      <td>0.215996</td>\n      <td>-0.108642</td>\n      <td>-0.070531</td>\n      <td>-0.016395</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.059319</td>\n      <td>-0.062408</td>\n      <td>0.332838</td>\n      <td>0.007388</td>\n      <td>0.033437</td>\n      <td>-0.174701</td>\n      <td>0.256190</td>\n      <td>-0.033434</td>\n      <td>0.021591</td>\n      <td>0.009860</td>\n      <td>...</td>\n      <td>-0.246763</td>\n      <td>0.169429</td>\n      <td>-0.384459</td>\n      <td>-0.209756</td>\n      <td>-0.003857</td>\n      <td>-0.085820</td>\n      <td>0.395393</td>\n      <td>0.002407</td>\n      <td>-0.097835</td>\n      <td>-0.175761</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 768 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"embedding_df_test.to_csv('embedding_df_test.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:05:16.657752Z","iopub.execute_input":"2024-06-29T08:05:16.658518Z","iopub.status.idle":"2024-06-29T08:05:16.670775Z","shell.execute_reply.started":"2024-06-29T08:05:16.658485Z","shell.execute_reply":"2024-06-29T08:05:16.669913Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"preprocessed_test_df = pd.read_csv('/kaggle/input/processed-test-and-train/test_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:06:06.990039Z","iopub.execute_input":"2024-06-23T15:06:06.991037Z","iopub.status.idle":"2024-06-23T15:06:07.077502Z","shell.execute_reply.started":"2024-06-23T15:06:06.990978Z","shell.execute_reply":"2024-06-23T15:06:07.076496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_df_test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:06:55.698516Z","iopub.execute_input":"2024-06-23T15:06:55.698886Z","iopub.status.idle":"2024-06-23T15:06:55.720063Z","shell.execute_reply.started":"2024-06-23T15:06:55.698859Z","shell.execute_reply":"2024-06-23T15:06:55.719142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test = pd.concat([preprocessed_test_df,embedding_df_test ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:07:11.121147Z","iopub.execute_input":"2024-06-23T15:07:11.121748Z","iopub.status.idle":"2024-06-23T15:07:11.127631Z","shell.execute_reply.started":"2024-06-23T15:07:11.121716Z","shell.execute_reply":"2024-06-23T15:07:11.126509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:07:13.258306Z","iopub.execute_input":"2024-06-23T15:07:13.259140Z","iopub.status.idle":"2024-06-23T15:07:13.278671Z","shell.execute_reply.started":"2024-06-23T15:07:13.259108Z","shell.execute_reply":"2024-06-23T15:07:13.277739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"merged_train['score'] = df['score']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:13.626004Z","iopub.execute_input":"2024-06-23T15:08:13.626396Z","iopub.status.idle":"2024-06-23T15:08:13.635340Z","shell.execute_reply.started":"2024-06-23T15:08:13.626366Z","shell.execute_reply":"2024-06-23T15:08:13.634401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train['score']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:22.772637Z","iopub.execute_input":"2024-06-23T15:08:22.773644Z","iopub.status.idle":"2024-06-23T15:08:22.780774Z","shell.execute_reply.started":"2024-06-23T15:08:22.773610Z","shell.execute_reply":"2024-06-23T15:08:22.779946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:37.673212Z","iopub.execute_input":"2024-06-23T15:08:37.673945Z","iopub.status.idle":"2024-06-23T15:08:37.806074Z","shell.execute_reply.started":"2024-06-23T15:08:37.673915Z","shell.execute_reply":"2024-06-23T15:08:37.805171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(merged_test.columns)-set(merged_train.columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:42.210404Z","iopub.execute_input":"2024-06-23T15:08:42.210775Z","iopub.status.idle":"2024-06-23T15:08:42.218308Z","shell.execute_reply.started":"2024-06-23T15:08:42.210741Z","shell.execute_reply":"2024-06-23T15:08:42.217342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add k-fold details","metadata":{}},{"cell_type":"code","source":"n_splits = 5\n\nseed = 42","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:45.494306Z","iopub.execute_input":"2024-06-23T15:08:45.494901Z","iopub.status.idle":"2024-06-23T15:08:45.499014Z","shell.execute_reply.started":"2024-06-23T15:08:45.494867Z","shell.execute_reply":"2024-06-23T15:08:45.498076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\nfor i, (_, val_index) in enumerate(skf.split(merged_train, merged_train[\"score\"])):\n    merged_train.loc[val_index, \"fold\"] = i\n    \n    \nprint(merged_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:50.034669Z","iopub.execute_input":"2024-06-23T15:08:50.035386Z","iopub.status.idle":"2024-06-23T15:08:50.058267Z","shell.execute_reply.started":"2024-06-23T15:08:50.035351Z","shell.execute_reply":"2024-06-23T15:08:50.057350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Feature selection","metadata":{}},{"cell_type":"code","source":"target = \"score\"\ntrain_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"text_tokens\", \"processed_text\"] + [target]\ntest_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:54.047673Z","iopub.execute_input":"2024-06-23T15:08:54.048050Z","iopub.status.idle":"2024-06-23T15:08:54.053327Z","shell.execute_reply.started":"2024-06-23T15:08:54.048021Z","shell.execute_reply":"2024-06-23T15:08:54.052143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sanitize_feature_names(df):\n    sanitized_columns = {col: re.sub(r'[^\\w]', '_', col) for col in df.columns}\n    df.rename(columns=sanitized_columns, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:56.434169Z","iopub.execute_input":"2024-06-23T15:08:56.434557Z","iopub.status.idle":"2024-06-23T15:08:56.440269Z","shell.execute_reply.started":"2024-06-23T15:08:56.434528Z","shell.execute_reply":"2024-06-23T15:08:56.439086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = sanitize_feature_names(merged_train)\ntest_features = sanitize_feature_names(merged_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:08:58.425190Z","iopub.execute_input":"2024-06-23T15:08:58.425552Z","iopub.status.idle":"2024-06-23T15:08:58.447991Z","shell.execute_reply.started":"2024-06-23T15:08:58.425525Z","shell.execute_reply":"2024-06-23T15:08:58.446965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:09:00.851918Z","iopub.execute_input":"2024-06-23T15:09:00.852582Z","iopub.status.idle":"2024-06-23T15:09:00.859259Z","shell.execute_reply.started":"2024-06-23T15:09:00.852546Z","shell.execute_reply":"2024-06-23T15:09:00.858157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"### Loss  and evaluation methods","metadata":{}},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = (y_true + a).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    \n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2 * np.sum((preds-labels)**2)\n    g = 1/2 * np.sum((preds-a)**2 + b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg / g**2) * len(labels)\n    hess = np.ones(len(labels))\n    \n    return grad, hess\n\ndef qwk_param_calc(y):\n    a = y.mean()\n    b = (y ** 2).mean() - a ** 2\n    \n    return np.round(a, 4), np.round(b, 4)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:09:05.637702Z","iopub.execute_input":"2024-06-23T15:09:05.638072Z","iopub.status.idle":"2024-06-23T15:09:05.646835Z","shell.execute_reply.started":"2024-06-23T15:09:05.638042Z","shell.execute_reply":"2024-06-23T15:09:05.645896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75, first_metric_only=True)\n]\n\nfor fold in range(n_splits):\n\n    model = lgb.LGBMRegressor(\n                            objective = qwk_obj,\n                            metrics = 'None',\n                            learning_rate = 0.05,\n                            max_depth = 5,\n                            num_leaves = 10, \n                            colsample_bytree = 0.5,   \n                            reg_alpha = 0.1,  \n                            reg_lambda = 0.8,\n                            n_estimators = 1024,\n                            random_state = seed, \n                            extra_trees=True,\n                            class_weight='balanced',\n                            verbosity = - 1\n                            )\n    \n    a, b = qwk_param_calc(train_features[train_features[\"fold\"] != fold][\"score\"])\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_features[train_features[\"fold\"] != fold].drop(columns=train_drop_columns)\n    y_train = train_features[train_features[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_features[train_features[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval = train_features[train_features[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n    \n    # Training model\n    lgb_model = model.fit(\n                            X_train, y_train,\n                            eval_names = ['train', 'valid'],\n                            eval_set = [(X_train, y_train), (X_eval, y_eval)],\n                            eval_metric = quadratic_weighted_kappa,\n                            callbacks = callbacks\n                        )\n    \n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:09:31.630671Z","iopub.execute_input":"2024-06-23T15:09:31.631157Z","iopub.status.idle":"2024-06-23T15:15:45.222712Z","shell.execute_reply.started":"2024-06-23T15:09:31.631121Z","shell.execute_reply":"2024-06-23T15:15:45.221655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validating model","metadata":{}},{"cell_type":"code","source":"preds, trues = [], []\n\nfor fold, model in enumerate(models):\n    X_eval_cv = train_features[train_features[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval_cv = train_features[train_features[\"fold\"] == fold][\"score\"]    \n\n    pred = model.predict(X_eval_cv) + a\n    \n    pred[pred < 1] = 1\n    pred[pred > 6] = 6\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n    \n    v_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n    \n    print(f\"Validation score {fold} : {v_score}\")\n\nv_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\nprint(f\"Validation score : {v_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:15:55.451888Z","iopub.execute_input":"2024-06-23T15:15:55.452301Z","iopub.status.idle":"2024-06-23T15:15:57.128769Z","shell.execute_reply.started":"2024-06-23T15:15:55.452266Z","shell.execute_reply":"2024-06-23T15:15:57.127329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyize prediction","metadata":{}},{"cell_type":"code","source":"def analyze_preds(trues, preds):\n    # Create dataframe\n    model_prec = pd.DataFrame([trues, preds]).T\n    model_prec.rename(columns = {0: 'trues', 1: 'preds'}, inplace=True)\n    model_prec['correct'] = model_prec['trues'] == model_prec['preds']\n    model_prec['count'] = model_prec.groupby('trues')['trues'].transform('count')\n    model_prec['correct_count'] = model_prec.groupby('trues')['correct'].transform('sum')\n    model_prec['correct_rate'] = model_prec['correct_count'] / model_prec['count']\n    \n    # Print binary correction rate\n    print(model_prec[['trues', 'correct_rate', 'correct_count', 'count']].drop_duplicates().sort_values(by='trues'))\n    \n    # Plot predictions by score    \n    def plot_model(ax, counts, true):\n        bars = ax.bar(counts.index, counts.values, color='skyblue')\n\n        # Find the index of the column with the specified label\n        highlight_index = counts.index.get_loc(true)\n\n        # Highlight the specified column\n        bars[highlight_index].set_color('orange')\n\n        ax.set_xlabel('Predicted Values')\n        ax.set_ylabel('Count')\n        ax.set_title(\"Score \" + str(true))\n    \n    score_list = [1,2,3,4,5,6]\n    test_pred_by_score = [model_prec[model_prec['trues'] ==  score]['preds'].value_counts() for score in score_list]\n\n    # Create a figure and six subplots arranged in a 2x3 grid\n    fig, axs = plt.subplots(2, 3, figsize=(15, 10))    \n    plot_model(axs[0, 0], test_pred_by_score[0], 1)\n    plot_model(axs[0, 1], test_pred_by_score[1], 2)\n    plot_model(axs[0, 2], test_pred_by_score[2], 3)\n    plot_model(axs[1, 0], test_pred_by_score[3], 4)\n    plot_model(axs[1, 1], test_pred_by_score[4], 5)\n    plot_model(axs[1, 2], test_pred_by_score[5], 6)\n    \nanalyze_preds(trues, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:16:59.226731Z","iopub.execute_input":"2024-06-23T15:16:59.227154Z","iopub.status.idle":"2024-06-23T15:17:00.894311Z","shell.execute_reply.started":"2024-06-23T15:16:59.227122Z","shell.execute_reply":"2024-06-23T15:17:00.893257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### See how model workds","metadata":{}},{"cell_type":"code","source":"X_train_tmp = train_features.drop(columns=train_drop_columns)\ny_train_tmp = train_features[\"score\"]\n\ndef interact_tree(example_index, tree_index): \n    example = pd.DataFrame(X_train_tmp.iloc[example_index]).T\n    score = y_train_tmp.iloc[example_index]\n    \n    print(\"Score: \", score)\n    # Plot the first tree\n    ax = lgb.plot_tree(models[1], tree_index=tree_index, figsize=(20, 8), show_info='data_percentage', example_case=example)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:17:57.289983Z","iopub.execute_input":"2024-06-23T15:17:57.290609Z","iopub.status.idle":"2024-06-23T15:17:57.412811Z","shell.execute_reply.started":"2024-06-23T15:17:57.290577Z","shell.execute_reply":"2024-06-23T15:17:57.411656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interact_tree(0,200)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:18:03.314139Z","iopub.execute_input":"2024-06-23T15:18:03.315054Z","iopub.status.idle":"2024-06-23T15:18:04.119744Z","shell.execute_reply.started":"2024-06-23T15:18:03.315021Z","shell.execute_reply":"2024-06-23T15:18:04.118806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_features = ['splling_err_ratio', 'lexical_diversity', 'paragraph_count', 'word_count', 'DistanceToCentroid', 'word_len_sum']","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:18:16.728690Z","iopub.execute_input":"2024-06-23T15:18:16.729601Z","iopub.status.idle":"2024-06-23T15:18:16.733980Z","shell.execute_reply.started":"2024-06-23T15:18:16.729564Z","shell.execute_reply":"2024-06-23T15:18:16.732945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = lgb.plot_importance(models[1], figsize=(20, 200), importance_type=\"split\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:18:27.780567Z","iopub.execute_input":"2024-06-23T15:18:27.781290Z","iopub.status.idle":"2024-06-23T15:18:48.179439Z","shell.execute_reply.started":"2024-06-23T15:18:27.781254Z","shell.execute_reply":"2024-06-23T15:18:48.177939Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nfeature_importance_df = pd.DataFrame()\nfeature_names = train_features.drop(columns=train_drop_columns).columns\nfeature_importance_df['Feature'] = feature_names\n\nfor i in range(0, 5):\n    importance = models[i].feature_importances_\n    feature_importance_df[f'Importance_{i}'] = importance\n\nfeature_importance_df.to_csv(\"feature_importance.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:00.461609Z","iopub.execute_input":"2024-06-23T15:19:00.461995Z","iopub.status.idle":"2024-06-23T15:19:00.603048Z","shell.execute_reply.started":"2024-06-23T15:19:00.461963Z","shell.execute_reply":"2024-06-23T15:19:00.602057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def show_feat_use(feat):\n#     ax = lgb.plot_split_value_histogram(models[2], figsize=(20, 8), feature=feat)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:50.187595Z","iopub.execute_input":"2024-06-23T14:38:50.187860Z","iopub.status.idle":"2024-06-23T14:38:50.194818Z","shell.execute_reply.started":"2024-06-23T14:38:50.187838Z","shell.execute_reply":"2024-06-23T14:38:50.194024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_feat_use('tfidf_luke')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:50.195847Z","iopub.execute_input":"2024-06-23T14:38:50.196117Z","iopub.status.idle":"2024-06-23T14:38:50.203772Z","shell.execute_reply.started":"2024-06-23T14:38:50.196084Z","shell.execute_reply":"2024-06-23T14:38:50.203030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How overfit model is ","metadata":{}},{"cell_type":"code","source":"def check_model_fit(model_id):\n    ax = lgb.plot_metric(models[model_id])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:09.033555Z","iopub.execute_input":"2024-06-23T15:19:09.034186Z","iopub.status.idle":"2024-06-23T15:19:09.038655Z","shell.execute_reply.started":"2024-06-23T15:19:09.034145Z","shell.execute_reply":"2024-06-23T15:19:09.037710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_model_fit(2)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:10.821130Z","iopub.execute_input":"2024-06-23T15:19:10.821504Z","iopub.status.idle":"2024-06-23T15:19:12.230326Z","shell.execute_reply.started":"2024-06-23T15:19:10.821475Z","shell.execute_reply":"2024-06-23T15:19:12.229434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"for i in range(len(models)):\n    models[i].booster_.save_model(f'model_{i}.txt')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:16.968401Z","iopub.execute_input":"2024-06-23T15:19:16.969189Z","iopub.status.idle":"2024-06-23T15:19:17.046835Z","shell.execute_reply.started":"2024-06-23T15:19:16.969149Z","shell.execute_reply":"2024-06-23T15:19:17.046040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"# models_path = [f\"{models_base_path}/model_0.txt\",f\"{models_base_path}/model_1.txt\",f\"{models_base_path}/model_2.txt\",f\"{models_base_path}/model_3.txt\",f\"{models_base_path}/model_4.txt\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:50.231129Z","iopub.execute_input":"2024-06-23T14:38:50.231431Z","iopub.status.idle":"2024-06-23T14:38:50.239553Z","shell.execute_reply.started":"2024-06-23T14:38:50.231408Z","shell.execute_reply":"2024-06-23T14:38:50.238607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# models = []\n# try:\n#     logging.info(f\"Model declaration:\")\n#     for i in range(n_splits):\n#         loaded_model  = lgb.Booster(model_file=models_path[i])\n#         models.append(loaded_model)\n# except Exception as e:\n#     logging.error(f\"Error during model declaration: {e}\")    ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:50.240720Z","iopub.execute_input":"2024-06-23T14:38:50.242547Z","iopub.status.idle":"2024-06-23T14:38:50.380718Z","shell.execute_reply.started":"2024-06-23T14:38:50.242523Z","shell.execute_reply":"2024-06-23T14:38:50.379911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:22.385486Z","iopub.execute_input":"2024-06-23T15:19:22.386190Z","iopub.status.idle":"2024-06-23T15:19:22.397997Z","shell.execute_reply.started":"2024-06-23T15:19:22.386158Z","shell.execute_reply":"2024-06-23T15:19:22.397136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"#test_features","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:38:50.391082Z","iopub.execute_input":"2024-06-23T14:38:50.391436Z","iopub.status.idle":"2024-06-23T14:38:50.397068Z","shell.execute_reply.started":"2024-06-23T14:38:50.391405Z","shell.execute_reply":"2024-06-23T14:38:50.396320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:33.034365Z","iopub.execute_input":"2024-06-23T15:19:33.035104Z","iopub.status.idle":"2024-06-23T15:19:33.040757Z","shell.execute_reply.started":"2024-06-23T15:19:33.035062Z","shell.execute_reply":"2024-06-23T15:19:33.039859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\ntry:\n    logging.info(f\"prediction start: \")\n    for fold, model in enumerate(models):\n        X_eval_cv = test_features.drop(columns=test_drop_columns)\n        pred = model.predict(X_eval_cv) + a    \n        pred[pred < 1] = 1\n        pred[pred > 6] = 6\n        preds.append(pred)\n\n    # Combining the 5 model results\n    for i, pred in enumerate(preds):\n        test_features[f\"score_pred_{i}\"] = pred\n    test_features[\"score\"] = np.round(test_features[[f\"score_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1),0).astype('int32')\n\n    # Submit to CSV\n    test_features[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)\nexcept Exception as e:\n    logging.error(f\"Error during prediction: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:43.902855Z","iopub.execute_input":"2024-06-23T15:19:43.903754Z","iopub.status.idle":"2024-06-23T15:19:43.986270Z","shell.execute_reply.started":"2024-06-23T15:19:43.903719Z","shell.execute_reply":"2024-06-23T15:19:43.985310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:19:48.204623Z","iopub.execute_input":"2024-06-23T15:19:48.205025Z","iopub.status.idle":"2024-06-23T15:19:48.226492Z","shell.execute_reply.started":"2024-06-23T15:19:48.204983Z","shell.execute_reply":"2024-06-23T15:19:48.225496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}