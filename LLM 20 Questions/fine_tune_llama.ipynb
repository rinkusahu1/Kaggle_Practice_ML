{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"},{"sourceId":9123670,"sourceType":"datasetVersion","datasetId":5501826},{"sourceId":9125833,"sourceType":"datasetVersion","datasetId":5509470},{"sourceId":9125973,"sourceType":"datasetVersion","datasetId":5508336},{"sourceId":90176,"sourceType":"modelInstanceVersion","modelInstanceId":75606,"modelId":100320}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T15:09:41.107851Z","iopub.execute_input":"2024-08-08T15:09:41.108644Z","iopub.status.idle":"2024-08-08T15:09:43.094489Z","shell.execute_reply.started":"2024-08-08T15:09:41.108613Z","shell.execute_reply":"2024-08-08T15:09:43.093553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -U accelerate\n! pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-08-08T15:09:43.096339Z","iopub.execute_input":"2024-08-08T15:09:43.096934Z","iopub.status.idle":"2024-08-08T15:10:50.821433Z","shell.execute_reply.started":"2024-08-08T15:09:43.096897Z","shell.execute_reply":"2024-08-08T15:10:50.820484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:14:03.475599Z","iopub.execute_input":"2024-08-08T09:14:03.475916Z","iopub.status.idle":"2024-08-08T09:14:16.721876Z","shell.execute_reply.started":"2024-08-08T09:14:03.475874Z","shell.execute_reply":"2024-08-08T09:14:16.720974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install \\\n    torch==2.2.2 \\\n    tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:14:16.723864Z","iopub.execute_input":"2024-08-08T09:14:16.724231Z","iopub.status.idle":"2024-08-08T09:20:08.600757Z","shell.execute_reply.started":"2024-08-08T09:14:16.724188Z","shell.execute_reply":"2024-08-08T09:20:08.599604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install \\\n    datasets \\\n    evaluate \\\n    trl \\\n    peft ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:20:08.602256Z","iopub.execute_input":"2024-08-08T09:20:08.602580Z","iopub.status.idle":"2024-08-08T09:20:24.757987Z","shell.execute_reply.started":"2024-08-08T09:20:08.602549Z","shell.execute_reply":"2024-08-08T09:20:24.756963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:11:48.288768Z","iopub.execute_input":"2024-08-08T07:11:48.289210Z","iopub.status.idle":"2024-08-08T07:11:54.263076Z","shell.execute_reply.started":"2024-08-08T07:11:48.289156Z","shell.execute_reply":"2024-08-08T07:11:54.262019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!mkdir /kaggle/working/model","metadata":{"execution":{"iopub.status.busy":"2024-08-06T12:56:14.670051Z","iopub.execute_input":"2024-08-06T12:56:14.670426Z","iopub.status.idle":"2024-08-06T12:56:15.711497Z","shell.execute_reply.started":"2024-08-06T12:56:14.670396Z","shell.execute_reply":"2024-08-06T12:56:15.710463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!tar -xzvf /kaggle/input/llm-20-questions-v2-submission/submission.tar.gz -C /kaggle/working/model/","metadata":{"execution":{"iopub.status.busy":"2024-08-06T12:56:18.987101Z","iopub.execute_input":"2024-08-06T12:56:18.988008Z","iopub.status.idle":"2024-08-06T12:58:00.776362Z","shell.execute_reply.started":"2024-08-06T12:56:18.987974Z","shell.execute_reply":"2024-08-06T12:58:00.775251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Just for testing","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig, GenerationConfig\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-08-08T15:10:50.822829Z","iopub.execute_input":"2024-08-08T15:10:50.823149Z","iopub.status.idle":"2024-08-08T15:10:58.505356Z","shell.execute_reply.started":"2024-08-08T15:10:50.823118Z","shell.execute_reply":"2024-08-08T15:10:58.504545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"microsoft/Phi-3-mini-4k-instruct\"\n\n\ntorch_dtype = torch.float16\nquant_storage_dtype = torch.float16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        quantization_config = bnb_config,\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        use_auth_token=\"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\"\n    )\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH,use_auth_token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T15:11:26.026188Z","iopub.execute_input":"2024-08-08T15:11:26.027196Z","iopub.status.idle":"2024-08-08T15:12:20.763867Z","shell.execute_reply.started":"2024-08-08T15:11:26.027162Z","shell.execute_reply":"2024-08-08T15:12:20.762923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/submission/model\")\ntokenizer.save_pretrained(\"/kaggle/working/submission/model\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T15:13:00.782536Z","iopub.execute_input":"2024-08-08T15:13:00.783089Z","iopub.status.idle":"2024-08-08T15:13:06.534861Z","shell.execute_reply.started":"2024-08-08T15:13:00.783058Z","shell.execute_reply":"2024-08-08T15:13:06.533871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile llama_3_70b_fsdp_qlora.yaml\n# script parameters\n# script parameters\n# training parameters\noutput_dir: \"/kaggle/working/Questioner-robot\" # Temporary output directory for model checkpoints      # report metrics to tensorboard\nlearning_rate: 0.0002                  # learning rate 2e-4\nlr_scheduler_type: \"constant\"          # learning rate scheduler\nnum_train_epochs: 2                    # number of training epochs\nper_device_train_batch_size: 1         # batch size per device during training\nper_device_eval_batch_size: 1          # batch size for evaluation\ngradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\noptim: adamw_torch                     # use torch adamw optimizer\nlogging_steps: 10                      # log every 10 steps\nsave_strategy: epoch                   # save checkpoint every epoch\nevaluation_strategy: epoch             # evaluate every epoch\nmax_grad_norm: 0.3                     # max gradient norm\nwarmup_ratio: 0.03                     # warmup ratio\n# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\nfsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\nfsdp_config:\n  backward_prefetch: \"backward_pre\"\n  forward_prefetch: \"false\"\n  use_orig_params: \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:20:33.422517Z","iopub.execute_input":"2024-08-08T09:20:33.423378Z","iopub.status.idle":"2024-08-08T09:20:33.430402Z","shell.execute_reply.started":"2024-08-08T09:20:33.423341Z","shell.execute_reply":"2024-08-08T09:20:33.429559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile training.py\n\nimport torch\nfrom dataclasses import dataclass, field\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,AutoModelForSeq2SeqLM,  BitsAndBytesConfig\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom datasets import load_from_disk\nfrom transformers import GenerationConfig, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport time\nfrom trl import SFTTrainer, SFTConfig\nfrom trl.commands.cli_utils import  TrlParser\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\n\n@dataclass\nclass ScriptArguments:\n    dataset_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to the dataset\"\n        },\n    )\n    model_id: str = field(\n        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n    )\n    max_seq_length: int = field(\n        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n    )\n        \ndef train_model(training_args):\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    torch.backends.cuda.enable_flash_sdp(False)\n\n    MODEL_PATH = \"google/flan-t5-base\"\n  \n    \n    config_dict = {\n        \"output_dir\": \"/kaggle/working/Questioner-robot\", \n        \"learning_rate\": 0.0002,    \n        \"report_to\":\"none\",\n        \"num_train_epochs\": 2,                    \n        \"per_device_train_batch_size\": 1,         \n        \"per_device_eval_batch_size\": 1,          \n        \"optim\": \"adamw_torch\",                   \n        \"logging_steps\": 10,                      \n        \"save_strategy\": \"epoch\",                 \n        \"evaluation_strategy\": \"epoch\",           \n        \"max_grad_norm\": 0.3,                     \n        \"warmup_ratio\": 0.03,                     \n        \"packing\": \"true\",\n        \"fp16\": \"true\",\n        \"fsdp\": \"full_shard auto_wrap offload\",\n        \"fsdp_config\": {\n            \"backward_prefetch\": \"backward_pre\",\n            \"forward_prefetch\": \"false\",\n            \"use_orig_params\": \"false\"\n        }\n    }\n\n    sft_config = SFTConfig(**config_dict)\n    \n    torch_dtype = torch.float16\n    quant_storage_dtype = torch.float16\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype,\n        bnb_4bit_quant_storage=quant_storage_dtype,\n    )\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        MODEL_PATH,\n        quantization_config = bnb_config,\n        torch_dtype=quant_storage_dtype,\n        trust_remote_code=True,\n        use_auth_token=\"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\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH,use_auth_token=\"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\")\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    \n    datadf= pd.read_csv('/kaggle/input/llm-20-keywords/datadf.csv')\n    shuffled_datadf = datadf.sample(frac=1,random_state=42).reset_index(drop=True)\n\n    train_df, val_df = train_test_split(shuffled_datadf, test_size=0.1, random_state=42)\n\n    from datasets import Dataset\n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset =  Dataset.from_pandas(val_df)\n    \n    def tokenize_function(example):\n        prompt = example[\"prompt\"]\n        example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n        example['labels'] = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n        return example\n    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n    tokenized_datasets = tokenized_datasets.remove_columns(['prompt','question'])\n\n    tokenized_val_datasets = val_dataset.map(tokenize_function, batched=True)\n    tokenized_val_datasets = tokenized_val_datasets.remove_columns(['prompt','question'])\n#     for name, param in model.named_parameters():\n#         if param.dtype == torch.float32:\n#             param.data = param.data.to(torch.bfloat16)\n\n#     tokenized_datasets = load_from_disk(\"/kaggle/input/tokenizedataset\")\n#     tokenized_val_datasets = load_from_disk(\"/kaggle/input/validationtokenizedataset\")\n\n\n    lora_config = LoraConfig(\n        r=2, # Rank\n        lora_alpha=1,\n        target_modules=[\"q\", \"v\"],\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=TaskType.SEQ_2_SEQ_LM  # FLAN-T5\n    )\n\n    output_dir = f'/kaggle/working/peft-Questioner-training-{str(int(time.time()))}'\n    \n    trainer = SFTTrainer(\n        model=model,\n        args=sft_config,\n        train_dataset=tokenized_datasets,\n        eval_dataset=tokenized_val_datasets,\n        peft_config=lora_config,\n        max_seq_length=3000,\n        tokenizer=tokenizer,\n        dataset_kwargs={\n            \"add_special_tokens\": False,  # We template with special tokens\n            \"append_concat_token\": False,  # No need to add additional separator token\n        },\n    )\n    \n    for param in trainer.model.parameters():\n        param.data = param.data.to(torch.float16)\n    if trainer.accelerator.is_main_process:\n        trainer.model.print_trainable_parameters()\n\n    ##########################\n    # Train model\n    ##########################\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    ##########################\n    # SAVE MODEL FOR SAGEMAKER\n    ##########################\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n\nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, TrainingArguments))\n    script_args, training_args = parser.parse_args_and_config()    \n    train_model(training_args)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:28:43.295699Z","iopub.execute_input":"2024-08-08T10:28:43.296153Z","iopub.status.idle":"2024-08-08T10:28:43.307053Z","shell.execute_reply.started":"2024-08-08T10:28:43.296117Z","shell.execute_reply":"2024-08-08T10:28:43.305984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass, field\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom datasets import load_from_disk\nfrom transformers import GenerationConfig, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport time\nfrom trl import SFTTrainer, SFTConfig\nfrom trl.commands.cli_utils import  TrlParser\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:04:18.759479Z","iopub.execute_input":"2024-08-08T10:04:18.759949Z","iopub.status.idle":"2024-08-08T10:04:25.378772Z","shell.execute_reply.started":"2024-08-08T10:04:18.759886Z","shell.execute_reply":"2024-08-08T10:04:25.377961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\nquant_storage_dtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:05:41.222736Z","iopub.execute_input":"2024-08-08T10:05:41.223593Z","iopub.status.idle":"2024-08-08T10:05:41.227724Z","shell.execute_reply.started":"2024-08-08T10:05:41.223559Z","shell.execute_reply":"2024-08-08T10:05:41.226749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" MODEL_PATH = \"facebook/opt-350m\"","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:09:47.820516Z","iopub.execute_input":"2024-08-08T10:09:47.820955Z","iopub.status.idle":"2024-08-08T10:09:47.825353Z","shell.execute_reply.started":"2024-08-08T10:09:47.820922Z","shell.execute_reply":"2024-08-08T10:09:47.824437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_quant_storage=quant_storage_dtype,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    quantization_config = bnb_config,\n    torch_dtype=quant_storage_dtype,\n    attn_implementation=\"sdpa\",\n    trust_remote_code=True,\n    use_auth_token=\"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\"\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH,use_auth_token=\"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\")\ntokenizer.pad_token = tokenizer.eos_token\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:09:49.729553Z","iopub.execute_input":"2024-08-08T10:09:49.730434Z","iopub.status.idle":"2024-08-08T10:09:50.850369Z","shell.execute_reply.started":"2024-08-08T10:09:49.730400Z","shell.execute_reply":"2024-08-08T10:09:50.848800Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, module in model.named_modules():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:07:02.617017Z","iopub.execute_input":"2024-08-08T10:07:02.617775Z","iopub.status.idle":"2024-08-08T10:07:02.625095Z","shell.execute_reply.started":"2024-08-08T10:07:02.617742Z","shell.execute_reply":"2024-08-08T10:07:02.623944Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ACCELERATE_USE_FSDP=1  FSDP_CPU_RAM_EFFICIENT_LOADING=1  torchrun --nproc_per_node=2 /kaggle/working/training.py --config /kaggle/working/llama_3_70b_fsdp_qlora.yaml","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:28:47.213383Z","iopub.execute_input":"2024-08-08T10:28:47.213750Z","iopub.status.idle":"2024-08-08T10:46:06.199392Z","shell.execute_reply.started":"2024-08-08T10:28:47.213721Z","shell.execute_reply":"2024-08-08T10:46:06.198278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n# with open(\"/kaggle/working/submission/model/config.json\", \"r\") as file:\n#     config = json.load(file)\n# config[\"rope_scaling\"] = {\"factor\":8.0,\"type\":\"dynamic\"}\n# with open(\"/kaggle/working/submission/model/config.json\", \"w\") as file:\n#     json.dump(config, file)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:53:09.472156Z","iopub.execute_input":"2024-08-08T09:53:09.472552Z","iopub.status.idle":"2024-08-08T09:53:09.476901Z","shell.execute_reply.started":"2024-08-08T09:53:09.472509Z","shell.execute_reply":"2024-08-08T09:53:09.475934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/llm-20-keywords/Keywords.csv')\ndf = df.drop(df.index[450])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:50:29.119215Z","iopub.execute_input":"2024-08-07T05:50:29.119522Z","iopub.status.idle":"2024-08-07T05:50:29.151877Z","shell.execute_reply.started":"2024-08-07T05:50:29.119495Z","shell.execute_reply":"2024-08-07T05:50:29.150938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = df.groupby('Category')['Word'].count().reset_index(name='word_count')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:50:29.153925Z","iopub.execute_input":"2024-08-07T05:50:29.154206Z","iopub.status.idle":"2024-08-07T05:50:29.170272Z","shell.execute_reply.started":"2024-08-07T05:50:29.154182Z","shell.execute_reply":"2024-08-07T05:50:29.169403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:50:29.171768Z","iopub.execute_input":"2024-08-07T05:50:29.172336Z","iopub.status.idle":"2024-08-07T05:50:29.187792Z","shell.execute_reply.started":"2024-08-07T05:50:29.172302Z","shell.execute_reply":"2024-08-07T05:50:29.186968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:50:29.188913Z","iopub.execute_input":"2024-08-07T05:50:29.189229Z","iopub.status.idle":"2024-08-07T05:50:29.198210Z","shell.execute_reply.started":"2024-08-07T05:50:29.189196Z","shell.execute_reply":"2024-08-07T05:50:29.197225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df = df.groupby('Category').apply(lambda x: x.sample(50)).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:04:28.325905Z","iopub.execute_input":"2024-08-07T06:04:28.326541Z","iopub.status.idle":"2024-08-07T06:04:28.338620Z","shell.execute_reply.started":"2024-08-07T06:04:28.326510Z","shell.execute_reply":"2024-08-07T06:04:28.337689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df[sampled_df['Category'] == \"Technology\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:05:11.790103Z","iopub.execute_input":"2024-08-07T06:05:11.790726Z","iopub.status.idle":"2024-08-07T06:05:11.805667Z","shell.execute_reply.started":"2024-08-07T06:05:11.790691Z","shell.execute_reply":"2024-08-07T06:05:11.804778Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:06:50.127979Z","iopub.execute_input":"2024-08-07T06:06:50.128397Z","iopub.status.idle":"2024-08-07T06:06:50.140441Z","shell.execute_reply.started":"2024-08-07T06:06:50.128365Z","shell.execute_reply":"2024-08-07T06:06:50.139404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top6 = sampled_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:54:13.576503Z","iopub.execute_input":"2024-08-07T05:54:13.577164Z","iopub.status.idle":"2024-08-07T05:54:13.581473Z","shell.execute_reply.started":"2024-08-07T05:54:13.577130Z","shell.execute_reply":"2024-08-07T05:54:13.580491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top6 = df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:12:54.626246Z","iopub.execute_input":"2024-08-06T16:12:54.626819Z","iopub.status.idle":"2024-08-06T16:12:54.631385Z","shell.execute_reply.started":"2024-08-06T16:12:54.626786Z","shell.execute_reply":"2024-08-06T16:12:54.630366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top6","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:00:55.292517Z","iopub.execute_input":"2024-08-07T06:00:55.293206Z","iopub.status.idle":"2024-08-07T06:00:55.302574Z","shell.execute_reply.started":"2024-08-07T06:00:55.293176Z","shell.execute_reply":"2024-08-07T06:00:55.301517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constitutional AI","metadata":{}},{"cell_type":"markdown","source":"## Red Teaming","metadata":{}},{"cell_type":"code","source":"# # Clear PyTorch cache\n# torch.cuda.empty_cache()\n\n# # If you want to completely reset the GPU state\n# torch.cuda.reset_max_memory_allocated()\n# torch.cuda.reset_max_memory_cached()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T13:37:23.173640Z","iopub.execute_input":"2024-08-06T13:37:23.174348Z","iopub.status.idle":"2024-08-06T13:37:23.200665Z","shell.execute_reply.started":"2024-08-06T13:37:23.174309Z","shell.execute_reply":"2024-08-06T13:37:23.199744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:50:39.247018Z","iopub.execute_input":"2024-08-07T14:50:39.247383Z","iopub.status.idle":"2024-08-07T14:50:56.839197Z","shell.execute_reply.started":"2024-08-07T14:50:39.247352Z","shell.execute_reply":"2024-08-07T14:50:56.838326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:50:56.841914Z","iopub.execute_input":"2024-08-07T14:50:56.843131Z","iopub.status.idle":"2024-08-07T14:50:56.847828Z","shell.execute_reply.started":"2024-08-07T14:50:56.843092Z","shell.execute_reply":"2024-08-07T14:50:56.846654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/llama3.1zip/pytorch/baseline/1/model\"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:50:56.877770Z","iopub.execute_input":"2024-08-07T14:50:56.878146Z","iopub.status.idle":"2024-08-07T14:50:56.883100Z","shell.execute_reply.started":"2024-08-07T14:50:56.878108Z","shell.execute_reply":"2024-08-07T14:50:56.882094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quanty_type = \"fp4\", \n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quanty = True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:50:56.849504Z","iopub.execute_input":"2024-08-07T14:50:56.849908Z","iopub.status.idle":"2024-08-07T14:50:56.876083Z","shell.execute_reply.started":"2024-08-07T14:50:56.849870Z","shell.execute_reply":"2024-08-07T14:50:56.875194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            quantization_config = bnb_config,\n            attn_implementation=\"sdpa\",\n            torch_dtype = torch.float16,\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:50:56.884594Z","iopub.execute_input":"2024-08-07T14:50:56.884999Z","iopub.status.idle":"2024-08-07T14:52:25.653469Z","shell.execute_reply.started":"2024-08-07T14:50:56.884962Z","shell.execute_reply":"2024-08-07T14:52:25.652328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npotential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\nterminators = [tokenizer.eos_token_id]\nfor token in potential_terminators:\n    token_id = tokenizer.convert_tokens_to_ids(token)\n    if token_id is not None:\n        terminators.append(token_id)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:38:39.826066Z","iopub.execute_input":"2024-08-07T10:38:39.826484Z","iopub.status.idle":"2024-08-07T10:38:39.833976Z","shell.execute_reply.started":"2024-08-07T10:38:39.826450Z","shell.execute_reply":"2024-08-07T10:38:39.832917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing the 20 Questions game. Your task is to ask the user a series of yes/no questions to guess the word they are thinking of. Never try to guess the word yourself. Focus on asking the most informative questions to determine if the word is a person, place, or thing. Each question should be a maximum length of 30 characters. Respond only with questions and no additional text.\nFollow these steps:\n1. Start with broad questions to classify the word into a large category.\n2. Based on the user's answers, narrow down the category by asking more specific questions.\n3. Use logical reasoning to eliminate possibilities and narrow down the answer.\n4. Always consider the context of previous answers when forming your next question.\n5. Keep questions concise and directly related to narrowing down the word.\nExamples:\n'Is it a living thing?' -> 'Yes' -> 'Is it an animal?' -> 'Yes' -> 'Is it a mammal?'\n'Is it a place?' -> 'Yes' -> 'Is it a country?' -> 'No' -> 'Is it a city?'\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:53:19.737870Z","iopub.execute_input":"2024-08-07T05:53:19.738739Z","iopub.status.idle":"2024-08-07T05:53:19.743931Z","shell.execute_reply.started":"2024-08-07T05:53:19.738704Z","shell.execute_reply":"2024-08-07T05:53:19.742916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_message(conversation_history):\n    # Initialize chat_history string\n    chat_history = \"\"\n    \n    # Construct chat_history string from conversation_history\n    for entry in conversation_history:\n        question = entry[\"Question\"]\n        answer = entry[\"Answer\"]\n        chat_history += f\"Question: {question}\\nAnswer: {answer}\\n\"\n    \n    # Construct the full prompt\n    prompt = f\"so far, the current state of the game is as following:\\n{chat_history}\"\n    \n    return prompt\n\ndef generate_response(chat,max_token):\n    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(inputs, max_new_tokens=max_token, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n    response = outputs[0][inputs.shape[-1]:]\n    out = tokenizer.decode(response, skip_special_tokens=True)\n\n    return out\ndef generate_question(conversation_history):\n    message = []\n    prompt = {\"role\":\"system\",\"content\": system_prompt+conversation_history}\n    message.append(prompt)\n    #print(\"Ask Question\")\n    #print(message)\n    return prompt, generate_response(message,32)\n\ndef critique_question(messages, question, keyword, category,conversation_history):\n    message = []\n    critique_request = {\"role\": \"CritiqueRequest\", \"content\": f\"{conversation_history}.Question asked by the assistant in the previous response: {question}. Is it helpful for the keyword '{keyword}' and category '{category}'?  Provide feedback on the question's relevance and adherence to the rules. don't ask question.\"}\n    message.append(critique_request)\n    #print(\"Critque Question\")\n    #print(messages)\n    return messages, generate_response(message,100)\n\ndef revise_question( messages, original_question, critique):\n    revision_request = {\"role\": \"RevisionRequest\", \"content\": f\"Please rewrite the assistant's response to better follow the question-asking rules.\\nOriginal question: {original_question}\"}\n    messages.append(revision_request)\n    #print(\"Revised\")\n    #print(messages)\n    return messages, generate_response(messages,32)\n\n\ndef generate_and_revise_question( conversation_history, keyword, category):\n    \n    conversation_history = prepare_message(conversation_history)\n    question_prompt, question = generate_question(conversation_history)\n    messages = [question_prompt]\n    messages.append({\"role\": \"assistant\", \"content\": question})\n\n    critique_prompt, critique = critique_question(messages, question, keyword, category,conversation_history)\n    messages.append({\"role\": \"Critique\", \"content\": critique})\n\n    revision_prompt, revised_question = revise_question(messages, question, critique)\n    messages.append({\"role\": \"Revision\", \"content\": revised_question})\n\n    return question_prompt, revised_question\n\ndef answerer( question,keyword, category):\n    message = []\n    \n    # System prompt\n    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\nYour task is to answer the questions of the user to help him guess the word you're thinking of.\nYour answers must be 'yes' or 'no'.\nThe keyword is: \"{keyword}\", it is of category: \"{category}\".\nIf the question is: \"{question}\", what is your answer? \n\"\"\"\n\n    message.append({\"role\": \"answerer\", \"content\": prompt})\n    #print(message)\n    return generate_response(message,32)\n\ndef play_game(keyword, category, max_questions=3):\n    conversation_history = []\n    all_prompts_and_responses = []\n\n    for _ in range(max_questions):\n        iterPromp, revised_question = generate_and_revise_question(conversation_history, keyword, category)\n        all_prompts_and_responses.append((iterPromp,revised_question))\n        answer = answerer(revised_question,keyword,category)\n        conversation_history.append({\"Question\": revised_question, \"Answer\": answer})\n\n    return all_prompts_and_responses","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:53:22.959860Z","iopub.execute_input":"2024-08-07T05:53:22.960220Z","iopub.status.idle":"2024-08-07T05:53:22.977827Z","shell.execute_reply.started":"2024-08-07T05:53:22.960192Z","shell.execute_reply":"2024-08-07T05:53:22.976863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:53:34.074873Z","iopub.execute_input":"2024-08-07T05:53:34.075795Z","iopub.status.idle":"2024-08-07T05:53:34.079919Z","shell.execute_reply.started":"2024-08-07T05:53:34.075759Z","shell.execute_reply":"2024-08-07T05:53:34.078916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_dataframe(df, batch_size=8, max_questions=3):\n    results = []\n\n    for i in tqdm(range(0, len(df), batch_size)):\n        batch = df.iloc[i:i+batch_size]\n\n        batch_results = []\n        for _, row in batch.iterrows():\n            keyword = row['Word']\n            category = row['Category']\n            game_results = play_game(keyword, category, max_questions)\n            batch_results.append({\n                'keyword': keyword,\n                'category': category,\n                'results': game_results\n            })\n\n        results.extend(batch_results)\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-08-07T05:54:25.150151Z","iopub.execute_input":"2024-08-07T05:54:25.150974Z","iopub.status.idle":"2024-08-07T05:54:25.157361Z","shell.execute_reply.started":"2024-08-07T05:54:25.150943Z","shell.execute_reply":"2024-08-07T05:54:25.156363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = process_dataframe(sampled_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:07:03.912141Z","iopub.execute_input":"2024-08-07T06:07:03.912744Z","iopub.status.idle":"2024-08-07T09:27:13.411161Z","shell.execute_reply.started":"2024-08-07T06:07:03.912711Z","shell.execute_reply":"2024-08-07T09:27:13.410227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:30:40.899110Z","iopub.execute_input":"2024-08-07T09:30:40.899491Z","iopub.status.idle":"2024-08-07T09:30:40.903666Z","shell.execute_reply.started":"2024-08-07T09:30:40.899458Z","shell.execute_reply":"2024-08-07T09:30:40.902709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/working/redteaming.joblib'\n\n# Save the list of dictionaries using joblib\njoblib.dump(res, file_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:31:07.568584Z","iopub.execute_input":"2024-08-07T09:31:07.568956Z","iopub.status.idle":"2024-08-07T09:31:07.643100Z","shell.execute_reply.started":"2024-08-07T09:31:07.568927Z","shell.execute_reply":"2024-08-07T09:31:07.642336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(res)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:32:07.187447Z","iopub.execute_input":"2024-08-07T09:32:07.188086Z","iopub.status.idle":"2024-08-07T09:32:07.193687Z","shell.execute_reply.started":"2024-08-07T09:32:07.188055Z","shell.execute_reply":"2024-08-07T09:32:07.192733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/working/redteaming.joblib'\n\n# Load the list of dictionaries using joblib\nloaded_data = joblib.load(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:32:38.512995Z","iopub.execute_input":"2024-08-07T09:32:38.513534Z","iopub.status.idle":"2024-08-07T09:32:38.561131Z","shell.execute_reply.started":"2024-08-07T09:32:38.513501Z","shell.execute_reply":"2024-08-07T09:32:38.560281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_data[-4]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:34:39.709013Z","iopub.execute_input":"2024-08-07T09:34:39.709904Z","iopub.status.idle":"2024-08-07T09:34:39.716048Z","shell.execute_reply.started":"2024-08-07T09:34:39.709871Z","shell.execute_reply":"2024-08-07T09:34:39.715043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(loaded_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:32:46.514790Z","iopub.execute_input":"2024-08-07T09:32:46.515156Z","iopub.status.idle":"2024-08-07T09:32:46.521415Z","shell.execute_reply.started":"2024-08-07T09:32:46.515126Z","shell.execute_reply":"2024-08-07T09:32:46.520474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset for training\n","metadata":{}},{"cell_type":"code","source":"rows = []\nfor item in res:\n    for result in item['results']:\n        prompt = result[0]\n        question = result[1]\n        rows.append({'prompt': prompt, 'question': question})","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:36:28.109172Z","iopub.execute_input":"2024-08-07T09:36:28.109542Z","iopub.status.idle":"2024-08-07T09:36:28.116429Z","shell.execute_reply.started":"2024-08-07T09:36:28.109508Z","shell.execute_reply":"2024-08-07T09:36:28.115402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datadf = pd.DataFrame(rows)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:36:58.389670Z","iopub.execute_input":"2024-08-07T09:36:58.389995Z","iopub.status.idle":"2024-08-07T09:36:58.396497Z","shell.execute_reply.started":"2024-08-07T09:36:58.389971Z","shell.execute_reply":"2024-08-07T09:36:58.395460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datadf.to_csv(\"/kaggle/working/datadf.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T09:37:58.560187Z","iopub.execute_input":"2024-08-07T09:37:58.560554Z","iopub.status.idle":"2024-08-07T09:37:58.645538Z","shell.execute_reply.started":"2024-08-07T09:37:58.560524Z","shell.execute_reply":"2024-08-07T09:37:58.644675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datadf= pd.read_csv('/kaggle/input/llm-20-keywords/datadf.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:17:36.278693Z","iopub.execute_input":"2024-08-07T14:17:36.279095Z","iopub.status.idle":"2024-08-07T14:17:36.323643Z","shell.execute_reply.started":"2024-08-07T14:17:36.279063Z","shell.execute_reply":"2024-08-07T14:17:36.322346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass QADataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        prompt = row['prompt']\n        question = row['question']\n        return {'prompt': prompt, 'question': question}\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:06.293980Z","iopub.execute_input":"2024-08-07T14:18:06.294979Z","iopub.status.idle":"2024-08-07T14:18:06.302272Z","shell.execute_reply.started":"2024-08-07T14:18:06.294944Z","shell.execute_reply":"2024-08-07T14:18:06.300997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_datadf = datadf.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:10.245697Z","iopub.execute_input":"2024-08-07T14:18:10.246636Z","iopub.status.idle":"2024-08-07T14:18:10.256138Z","shell.execute_reply.started":"2024-08-07T14:18:10.246597Z","shell.execute_reply":"2024-08-07T14:18:10.254893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_datadf","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:16.018764Z","iopub.execute_input":"2024-08-07T14:18:16.019177Z","iopub.status.idle":"2024-08-07T14:18:16.042076Z","shell.execute_reply.started":"2024-08-07T14:18:16.019145Z","shell.execute_reply":"2024-08-07T14:18:16.040902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(shuffled_datadf, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:20.100940Z","iopub.execute_input":"2024-08-07T14:18:20.101364Z","iopub.status.idle":"2024-08-07T14:18:20.121223Z","shell.execute_reply.started":"2024-08-07T14:18:20.101332Z","shell.execute_reply":"2024-08-07T14:18:20.119982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch datasets\ntrain_dataset = QADataset(train_df)\nval_dataset = QADataset(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:31:23.820946Z","iopub.execute_input":"2024-08-07T10:31:23.821379Z","iopub.status.idle":"2024-08-07T10:31:23.827842Z","shell.execute_reply.started":"2024-08-07T10:31:23.821349Z","shell.execute_reply":"2024-08-07T10:31:23.826700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader, RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:23:55.540006Z","iopub.execute_input":"2024-08-07T10:23:55.541098Z","iopub.status.idle":"2024-08-07T10:23:55.547678Z","shell.execute_reply.started":"2024-08-07T10:23:55.541059Z","shell.execute_reply":"2024-08-07T10:23:55.546272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create data loaders\n# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:09:40.970847Z","iopub.execute_input":"2024-08-07T10:09:40.971221Z","iopub.status.idle":"2024-08-07T10:09:40.976443Z","shell.execute_reply.started":"2024-08-07T10:09:40.971192Z","shell.execute_reply":"2024-08-07T10:09:40.975449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader = DataLoader(\n#     train_dataset,\n#     sampler=RandomSampler(train_dataset),\n#     batch_size=8  # Adjust batch size as needed\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:24:14.888042Z","iopub.execute_input":"2024-08-07T10:24:14.888457Z","iopub.status.idle":"2024-08-07T10:24:14.897166Z","shell.execute_reply.started":"2024-08-07T10:24:14.888413Z","shell.execute_reply":"2024-08-07T10:24:14.895911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffled_data = []\n# shuffled_targets = []\n\n# for batch_data, batch_targets in train_loader:\n#     shuffled_data.extend(batch_data.tolist())\n#     shuffled_targets.extend(batch_targets.tolist())\n\n# # Create a new shuffled dataset\n# shuffled_dataset = QADataset(shuffled_data, shuffled_targets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:29.158269Z","iopub.execute_input":"2024-08-07T14:18:29.158624Z","iopub.status.idle":"2024-08-07T14:18:29.615233Z","shell.execute_reply.started":"2024-08-07T14:18:29.158597Z","shell.execute_reply":"2024-08-07T14:18:29.614147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:30.943444Z","iopub.execute_input":"2024-08-07T14:18:30.944229Z","iopub.status.idle":"2024-08-07T14:18:30.982210Z","shell.execute_reply.started":"2024-08-07T14:18:30.944193Z","shell.execute_reply":"2024-08-07T14:18:30.981089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset =  Dataset.from_pandas(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:32.476521Z","iopub.execute_input":"2024-08-07T14:18:32.477510Z","iopub.status.idle":"2024-08-07T14:18:32.494306Z","shell.execute_reply.started":"2024-08-07T14:18:32.477462Z","shell.execute_reply":"2024-08-07T14:18:32.493027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:18:36.837262Z","iopub.execute_input":"2024-08-07T14:18:36.837657Z","iopub.status.idle":"2024-08-07T14:18:36.842786Z","shell.execute_reply.started":"2024-08-07T14:18:36.837627Z","shell.execute_reply":"2024-08-07T14:18:36.841428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(example):\n    prompt = example[\"prompt\"]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_val_datasets = val_dataset.map(tokenize_function, batched=True)\ntokenized_val_datasets = tokenized_val_datasets.remove_columns(['prompt','question'])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:19:07.135356Z","iopub.execute_input":"2024-08-07T14:19:07.135775Z","iopub.status.idle":"2024-08-07T14:19:40.322214Z","shell.execute_reply.started":"2024-08-07T14:19:07.135741Z","shell.execute_reply":"2024-08-07T14:19:40.321097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:21:04.895086Z","iopub.execute_input":"2024-08-07T14:21:04.895793Z","iopub.status.idle":"2024-08-07T14:26:00.976070Z","shell.execute_reply.started":"2024-08-07T14:21:04.895756Z","shell.execute_reply":"2024-08-07T14:26:00.974457Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns(['prompt','question'])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:28:43.483485Z","iopub.execute_input":"2024-08-07T14:28:43.484254Z","iopub.status.idle":"2024-08-07T14:28:43.492294Z","shell.execute_reply.started":"2024-08-07T14:28:43.484219Z","shell.execute_reply":"2024-08-07T14:28:43.490845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets.save_to_disk(\"/kaggle/working/traintokenizedDataset\")\ntokenized_val_datasets.save_to_disk(\"/kaggle/working/valtokenizedDataset\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:36:04.960467Z","iopub.execute_input":"2024-08-07T14:36:04.960939Z","iopub.status.idle":"2024-08-07T14:36:07.356648Z","shell.execute_reply.started":"2024-08-07T14:36:04.960896Z","shell.execute_reply":"2024-08-07T14:36:07.355263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T10:53:07.187883Z","iopub.execute_input":"2024-08-07T10:53:07.188656Z","iopub.status.idle":"2024-08-07T10:53:09.360001Z","shell.execute_reply.started":"2024-08-07T10:53:07.188610Z","shell.execute_reply":"2024-08-07T10:53:09.358720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_from_disk\n\n# tokenized_datasets = load_from_disk(\"/kaggle/input/tokenizedataset\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:40:28.079470Z","iopub.execute_input":"2024-08-07T11:40:28.079740Z","iopub.status.idle":"2024-08-07T11:40:28.631009Z","shell.execute_reply.started":"2024-08-07T11:40:28.079717Z","shell.execute_reply":"2024-08-07T11:40:28.630252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:28:56.964685Z","iopub.execute_input":"2024-08-07T14:28:56.965386Z","iopub.status.idle":"2024-08-07T14:28:56.972277Z","shell.execute_reply.started":"2024-08-07T14:28:56.965351Z","shell.execute_reply":"2024-08-07T14:28:56.971062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:29:00.744526Z","iopub.execute_input":"2024-08-07T14:29:00.745400Z","iopub.status.idle":"2024-08-07T14:29:00.755439Z","shell.execute_reply.started":"2024-08-07T14:29:00.745359Z","shell.execute_reply":"2024-08-07T14:29:00.754240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:29:04.658838Z","iopub.execute_input":"2024-08-07T14:29:04.659643Z","iopub.status.idle":"2024-08-07T14:29:04.669977Z","shell.execute_reply.started":"2024-08-07T14:29:04.659606Z","shell.execute_reply":"2024-08-07T14:29:04.668768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PEFT COnfiguration","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig, TrainingArguments, Trainer\n# from trl.commands.cli_utils import  TrlParser\n\n# from trl import (\n#    SFTTrainer)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:29:13.830365Z","iopub.execute_input":"2024-08-07T14:29:13.831465Z","iopub.status.idle":"2024-08-07T14:29:14.631529Z","shell.execute_reply.started":"2024-08-07T14:29:13.831413Z","shell.execute_reply":"2024-08-07T14:29:14.630589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=1, # Rank\n    lora_alpha=1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM # FLAN-T5\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:40:29.345235Z","iopub.execute_input":"2024-08-07T11:40:29.345505Z","iopub.status.idle":"2024-08-07T11:40:29.350594Z","shell.execute_reply.started":"2024-08-07T11:40:29.345482Z","shell.execute_reply":"2024-08-07T11:40:29.349700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model = get_peft_model(model, \n                            lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:40:29.351967Z","iopub.execute_input":"2024-08-07T11:40:29.352321Z","iopub.status.idle":"2024-08-07T11:40:29.463914Z","shell.execute_reply.started":"2024-08-07T11:40:29.352290Z","shell.execute_reply":"2024-08-07T11:40:29.463080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = f'./peft-Questioner-training-{str(int(time.time()))}'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = SFTTrainer(\n#         model=model,\n#         output_dir=output_dir,\n#         per_device_train_batch_size=1,\n#         learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n#         num_train_epochs=2,\n#         max_steps=2  ,\n#         report_to=\"none\",\n#         train_dataset=tokenized_datasets,\n#         eval_dataset=test_dataset,\n#         peft_config=lora_config,\n#         max_seq_length=tokenizer.model_max_length,\n#         tokenizer=tokenizer,\n#         packing=True,\n#         dataset_kwargs={\n#             \"add_special_tokens\": False,  # We template with special tokens\n#             \"append_concat_token\": False,  # No need to add additional separator token\n#         },\n#     )\n# if trainer.accelerator.is_main_process:\n#     trainer.model.print_trainable_parameters()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##########################\n# # Train model\n# ##########################\n# trainer.train()\n\n# ##########################\n# # SAVE MODEL FOR SAGEMAKER\n# ##########################\n# if trainer.is_fsdp_enabled:\n#     trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n# trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:40:53.107563Z","iopub.execute_input":"2024-08-07T11:40:53.108256Z","iopub.status.idle":"2024-08-07T11:40:53.112248Z","shell.execute_reply.started":"2024-08-07T11:40:53.108218Z","shell.execute_reply":"2024-08-07T11:40:53.111344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:25:31.251086Z","iopub.execute_input":"2024-08-07T11:25:31.251768Z","iopub.status.idle":"2024-08-07T11:25:31.257379Z","shell.execute_reply.started":"2024-08-07T11:25:31.251732Z","shell.execute_reply":"2024-08-07T11:25:31.256338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(torch.cuda.memory_summary())","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:29:40.017818Z","iopub.execute_input":"2024-08-07T11:29:40.018517Z","iopub.status.idle":"2024-08-07T11:29:40.025928Z","shell.execute_reply.started":"2024-08-07T11:29:40.018483Z","shell.execute_reply":"2024-08-07T11:29:40.024939Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noutput_dir = f'./peft-Questioner-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=4,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=2,\n    max_steps=2  ,\n    report_to=\"none\",\n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:40:57.488945Z","iopub.execute_input":"2024-08-07T11:40:57.489316Z","iopub.status.idle":"2024-08-07T11:40:57.530454Z","shell.execute_reply.started":"2024-08-07T11:40:57.489280Z","shell.execute_reply":"2024-08-07T11:40:57.529616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path=\"./peft-Questioner-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:41:02.584011Z","iopub.execute_input":"2024-08-07T11:41:02.584679Z","iopub.status.idle":"2024-08-07T11:41:08.783661Z","shell.execute_reply.started":"2024-08-07T11:41:02.584643Z","shell.execute_reply":"2024-08-07T11:41:08.782052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Peft model","metadata":{}},{"cell_type":"code","source":"\nfrom peft import PeftModel, PeftConfig\n\n#peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16)\n#tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\npeft_model = PeftModel.from_pretrained(model, \n                                       './peft-dialogue-summary-checkpoint-local/', \n                                       torch_dtype=torch.float16,\n                                       is_trainable=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#results = play_game(keyword=\"Tool Shed\", category=\"object\",max_questions=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:06:54.522371Z","iopub.execute_input":"2024-08-06T16:06:54.523065Z","iopub.status.idle":"2024-08-06T16:07:12.727038Z","shell.execute_reply.started":"2024-08-06T16:06:54.523033Z","shell.execute_reply":"2024-08-06T16:07:12.726209Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Print stored prompts and responses\n# for i, messages in enumerate(results, 1):\n#     print(f\"\\nQuestion {i}:\")\n#     print(f\": {messages}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:02:32.845429Z","iopub.execute_input":"2024-08-06T16:02:32.846079Z","iopub.status.idle":"2024-08-06T16:02:32.851388Z","shell.execute_reply.started":"2024-08-06T16:02:32.846046Z","shell.execute_reply":"2024-08-06T16:02:32.850359Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)\n\n# class LLaMA20QuestionsGame:\n#     def __init__(self, MODEL_PATH=\"/kaggle/input/llama3.1zip/pytorch/baseline/1/model\"):\n#         #self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n#         bnb_config = BitsAndBytesConfig(\n#             load_in_4bit = True,\n#             bnb_4bit_quanty_type = \"fp4\", \n#             bnb_4bit_compute_dtype=torch.float16,\n#             bnb_4bit_use_double_quanty = True,\n#         )\n#         self.model = AutoModelForCausalLM.from_pretrained(\n#             MODEL_PATH,\n#             device_map=\"auto\",\n#             trust_remote_code=True,\n#         )\n#         self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n#         self.pipe = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n#         potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n#         self.terminators = [self.tokenizer.eos_token_id]\n#         for token in potential_terminators:\n#             token_id = self.tokenizer.convert_tokens_to_ids(token)\n#             if token_id is not None:\n#                 self.terminators.append(token_id)\n        \n#         self.system_prompt = {\"role\": \"system\", \"content\": \"\"\"You are a helpful AI assistant with expertise in playing the 20 Questions game. Your task is to ask the user a series of yes/no questions to guess the word they are thinking of. Never try to guess the word yourself. Focus on asking the most informative questions to determine if the word is a person, place, or thing. Each question should be a maximum length of 30 characters. Respond only with questions and no additional text.\n# Follow these steps:\n# 1. Start with broad questions to classify the word into a large category.\n# 2. Based on the user's answers, narrow down the category by asking more specific questions.\n# 3. Use logical reasoning to eliminate possibilities and narrow down the answer.\n# 4. Always consider the context of previous answers when forming your next question.\n# 5. Keep questions concise and directly related to narrowing down the word.\n# Examples:\n# 'Is it a living thing?' -> 'Yes' -> 'Is it an animal?' -> 'Yes' -> 'Is it a mammal?'\n# 'Is it a place?' -> 'Yes' -> 'Is it a country?' -> 'No' -> 'Is it a city?'\"\"\"}\n\n#     def generate_question(self, conversation_history):\n#         messages = [self.system_prompt] + conversation_history + [{\"role\": \"assistant\", \"content\": \"\"}]\n#         prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        \n#         response = self.pipe(prompt, max_new_tokens=32, do_sample=True, temperature=0.7, top_p=0.95,pad_token_id=self.tokenizer.eos_token_id, eos_token_id=self.terminators)\n#         return prompt, response[0]['generated_text'].split(\"Assistant: \")[-1].strip()\n\n#     def critique_question(self, messages, question, keyword, category):\n#         critique_request = {\"role\": \"CritiqueRequest\", \"content\": f\"Question asked by the assistant in the previous response: {question}. Is it helpful for the keyword '{keyword}' and category '{category}'? As per the rules of asking questions:\"}\n#         messages = messages + [critique_request, {\"role\": \"Critique\", \"content\": \"\"}]\n#         prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        \n#         critique = self.pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.95)\n#         return prompt, critique[0]['generated_text'].split(\"Critique: \")[-1].strip()\n\n#     def revise_question(self, messages, original_question, critique):\n#         revision_request = {\"role\": \"RevisionRequest\", \"content\": f\"Please rewrite the assistant's response to better follow the question-asking rules.\\nOriginal question: {original_question}\\nCritique: {critique}\"}\n#         messages = messages + [revision_request, {\"role\": \"Revision\", \"content\": \"\"}]\n#         prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        \n#         revised_question = self.pipe(prompt, max_new_tokens=30, do_sample=True, temperature=0.7, top_p=0.95)\n#         return prompt, revised_question[0]['generated_text'].split(\"Revision: \")[-1].strip()\n\n#     def generate_and_revise_question(self, conversation_history, keyword, category):\n#         messages = [self.system_prompt] + conversation_history\n        \n#         question_prompt, question = self.generate_question(conversation_history)\n#         messages.append({\"role\": \"assistant\", \"content\": question})\n        \n#         critique_prompt, critique = self.critique_question(messages, question, keyword, category)\n#         messages.append({\"role\": \"Critique\", \"content\": critique})\n        \n#         revision_prompt, revised_question = self.revise_question(messages, question, critique)\n#         messages.append({\"role\": \"Revision\", \"content\": revised_question})\n        \n#         return messages, revised_question\n\n#     def play_game(self, keyword, category, max_questions=20):\n#         conversation_history = []\n#         all_prompts_and_responses = []\n        \n#         for _ in range(max_questions):\n#             messages, revised_question = self.generate_and_revise_question(conversation_history, keyword, category)\n#             all_prompts_and_responses.append(messages)\n            \n#             conversation_history.append({\"role\": \"assistant\", \"content\": revised_question})\n        \n#         return all_prompts_and_responses\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T13:57:42.648863Z","iopub.execute_input":"2024-08-06T13:57:42.649210Z","iopub.status.idle":"2024-08-06T13:57:59.898902Z","shell.execute_reply.started":"2024-08-06T13:57:42.649178Z","shell.execute_reply":"2024-08-06T13:57:59.898169Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Usage\n# game = LLaMA20QuestionsGame()\n# results = game.play_game(keyword=\"Thailand\", category=\"place\")\n\n# # Print stored prompts and responses\n# for i, messages in enumerate(results, 1):\n#     print(f\"\\nQuestion {i}:\")\n#     for message in messages:\n#         print(f\"{message['role']}: {message['content']}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T13:58:03.841717Z","iopub.execute_input":"2024-08-06T13:58:03.842719Z","iopub.status.idle":"2024-08-06T13:59:21.031400Z","shell.execute_reply.started":"2024-08-06T13:58:03.842684Z","shell.execute_reply":"2024-08-06T13:59:21.029824Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}