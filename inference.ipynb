{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":9578054,"sourceType":"datasetVersion","datasetId":5824402},{"sourceId":206138086,"sourceType":"kernelVersion"},{"sourceId":127817,"sourceType":"modelInstanceVersion","modelInstanceId":107642,"modelId":131980},{"sourceId":129460,"sourceType":"modelInstanceVersion","modelInstanceId":109072,"modelId":133404}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\nimport albumentations as A\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nimport pydicom\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom sklearn.model_selection import train_test_split\nimport bisect\nimport time\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T13:41:40.303326Z","iopub.execute_input":"2024-11-09T13:41:40.303740Z","iopub.status.idle":"2024-11-09T13:42:08.587782Z","shell.execute_reply.started":"2024-11-09T13:41:40.303702Z","shell.execute_reply":"2024-11-09T13:42:08.586712Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# Config\ncfg= SimpleNamespace(\n    img_dir= \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images\",\n    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    n_frames=3,\n    epochs=10,\n    lr=0.0005,\n    batch_size=16,\n    backbone=\"resnet18\",\n    seed= 0,\n    model_dir = \"/kaggle/working/\",\n    kernel_type = \"resnet18\",\n    num_workers = 8,\n    n_epochs = 5,\n    init_lr =0.0005,\n    CUDA_VISIBLE_DEVICES = \"0\",\n    sag_axial_slices = 3,\n    sag_2_slices =  1,\n    sag_1_slices = 4,\n    model_dir_point = \"/kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\",\n    model_dir_lsdd = \"/kaggle/input/training-lsdd/resnet18_final_fold0.pth\",\n    \n)\nset_seed(seed=cfg.seed) # Makes results reproducable","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.589464Z","iopub.execute_input":"2024-11-09T13:42:08.589986Z","iopub.status.idle":"2024-11-09T13:42:08.626491Z","shell.execute_reply.started":"2024-11-09T13:42:08.589950Z","shell.execute_reply":"2024-11-09T13:42:08.625713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_training_dataframe(cfg,isTrain=True):\n    \n    test_series_description = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')\n\n    sagtialla_t1_df = test_series_description[test_series_description['series_description'] == \"Sagittal T1\"]\n    sagtialla_t1_df_u = sagtialla_t1_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n    \n    axial_t2_df = test_series_description[test_series_description['series_description'] == \"Axial T2\"]\n    axial_t2_df_u = axial_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    sagtialla_t2_df = test_series_description[test_series_description['series_description'] == \"Sagittal T2/STIR\"]\n    sagtialla_t2_df_u = sagtialla_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    merge_sagital_t1 = sagtialla_t1_df_u\n    merge_sagital_t1 = merge_sagital_t1.drop(columns=['series_description'])\n    merge_sagital_t1.rename(columns={'series_id': 'series_id_sg1'}, inplace=True)\n    \n    merge_sagital_t2 = pd.merge(merge_sagital_t1,sagtialla_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_sagital_t2 = merge_sagital_t2.drop(columns=['series_description'])\n    merge_sagital_t2.rename(columns={'series_id': 'series_id_sg2'}, inplace=True)\n\n    merge_axial_t2 = pd.merge(merge_sagital_t2,axial_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_axial_t2 = merge_axial_t2.drop(columns=['series_description'])\n    merge_axial_t2.rename(columns={'series_id': 'series_id_a2'}, inplace=True)\n\n    return merge_axial_t2  \n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.627841Z","iopub.execute_input":"2024-11-09T13:42:08.628235Z","iopub.status.idle":"2024-11-09T13:42:08.640652Z","shell.execute_reply.started":"2024-11-09T13:42:08.628183Z","shell.execute_reply":"2024-11-09T13:42:08.639809Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def batch_to_device(batch, device, skip_keys=[]):\n    batch_dict= {}\n    for key in batch:\n        if key in skip_keys:\n             batch_dict[key]= batch[key]\n        else:    \n            batch_dict[key]= batch[key].to(device)\n    return batch_dict\n\ndef visualize_prediction(batch, pred, epoch):\n    \n    mid= cfg.n_frames//2\n    \n    # Plot\n    for idx in range(1):\n    \n        # Select Data\n        img= batch[\"img\"][idx, mid, :, :].cpu().numpy()*255\n        cs_true= batch[\"label\"][idx, ...].cpu().numpy()*256\n        cs= pred[idx, ...].cpu().numpy()*256\n                \n        coords_list = [(\"TRUE\", \"lightblue\", cs_true), (\"PRED\", \"orange\", cs)]\n        text_labels = [str(x) for x in range(1,21)]\n        \n        # Plot coords\n        fig, axes = plt.subplots(1, len(coords_list), figsize=(10,4))\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        for ax, (title, color, coords) in zip(axes, coords_list):\n            ax.imshow(img, cmap='gray')\n            ax.scatter(coords[0::2], coords[1::2], c=color, s=50)\n            ax.axis('off')\n            ax.set_title(title)\n\n            # Add text labels near the coordinates\n            for i, (x, y) in enumerate(zip(coords[0::2], coords[1::2])):\n                if i < len(text_labels):  # Ensure there are enough labels\n                    ax.text(x + 10, y, text_labels[i], color='white', fontsize=15, bbox=dict(facecolor='black', alpha=0.5))\n\n\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        plt.show()\n#         plt.close(fig)\n    return\n\ndef load_weights_skip_mismatch(model, weights_path, device):\n    # Load Weights\n    state_dict = torch.load(weights_path, map_location=device)\n    model_dict = model.state_dict()\n    \n    # Iter models\n    params = {}\n    for (sdk, sfv), (mdk, mdv) in zip(state_dict.items(), model_dict.items()):\n        if sfv.size() == mdv.size():\n            params[sdk] = sfv\n        else:\n            print(\"Skipping param: {}, {} != {}\".format(sdk, sfv.size(), mdv.size()))\n    \n    # Reload + Skip\n    model.load_state_dict(params, strict=False)\n    print(\"Loaded weights from:\", weights_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.643219Z","iopub.execute_input":"2024-11-09T13:42:08.643592Z","iopub.status.idle":"2024-11-09T13:42:08.661150Z","shell.execute_reply.started":"2024-11-09T13:42:08.643549Z","shell.execute_reply":"2024-11-09T13:42:08.659984Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Create csv Dataset","metadata":{}},{"cell_type":"code","source":"train_df = load_training_dataframe(cfg)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.662378Z","iopub.execute_input":"2024-11-09T13:42:08.662727Z","iopub.status.idle":"2024-11-09T13:42:08.714592Z","shell.execute_reply.started":"2024-11-09T13:42:08.662693Z","shell.execute_reply":"2024-11-09T13:42:08.713657Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.715875Z","iopub.execute_input":"2024-11-09T13:42:08.716166Z","iopub.status.idle":"2024-11-09T13:42:08.729263Z","shell.execute_reply.started":"2024-11-09T13:42:08.716135Z","shell.execute_reply":"2024-11-09T13:42:08.728168Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   study_id  series_id_sg1  series_id_sg2  series_id_a2\n0  44036939     2828203845     3844393089    3481971518","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id_sg1</th>\n      <th>series_id_sg2</th>\n      <th>series_id_a2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>3844393089</td>\n      <td>3481971518</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Co-ordinate Prediction Dataset","metadata":{}},{"cell_type":"code","source":"class PreTrainDataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids,cfg,transform,isTrain = False,is_dataset_for_t1= False):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.is_dataset_for_t1 = is_dataset_for_t1\n\n    \n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n\n\n    def load_dicom_stack(self, dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n\n        # Determine the plane for sorting (sagittal, coronal, axial)\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n\n        # Sort DICOM files based on positions (reverse sort for axial plane if needed)\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n\n        # Get the shape of each pixel array (height, width)\n        shapes = [d.pixel_array.shape for d in dicoms]\n        # Check if all DICOM images have the same shape\n        if len(set(shapes)) > 1:\n            # There's a shape mismatch, find the minimum shape (height, width)\n            min_shape = np.min(shapes, axis=0)\n            # Resize images to the minimum shape\n            resized_arrays = []\n            for d in dicoms:\n                img = d.pixel_array.astype(\"float32\")\n                if img.shape != tuple(min_shape):\n                    resized_img = cv2.resize(img, (min_shape[1], min_shape[0]))  # Resize to (width, height)\n                else:\n                    resized_img = img  # No resizing needed\n                resized_arrays.append(resized_img)\n\n            # Stack the resized images along the first axis\n            array = np.stack(resized_arrays)\n        else:\n            # If all shapes are the same, no resizing is needed\n            array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n\n        # Reorder the array according to the sorted positions\n        array = array[idx]\n\n        return {\n            \"array\": self.convert_to_8bit(array),\n            \"positions\": ipp,\n            \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")\n        }\n    \n    def pad_image(self, img):\n        n= img.shape[0]\n        if n >= self.cfg.n_frames:\n            start_idx = (n - self.cfg.n_frames) // 2\n            return img[start_idx:start_idx + self.cfg.n_frames,:, :]\n        else:\n            pad_left = (self.cfg.n_frames - n) // 2\n            pad_right = self.cfg.n_frames - n - pad_left\n            return np.pad(img, ((pad_left, pad_right),(0,0), (0,0)), 'constant', constant_values=0)\n    \n    def load_img(self, series_id):\n        fname = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(series_id)), plane=\"sagittal\")\n        img= fname[\"array\"]\n        img= self.pad_image(img)\n        img= np.transpose(img, (1,2, 0))\n        img= self.transform(image=img)[\"image\"]\n        img= np.transpose(img, (2, 0, 1))\n        img= (img / 255.0)\n        return img\n        \n        \n    def __getitem__(self, idx):\n        d= self.study_ids.iloc[idx]\n        if self.is_dataset_for_t1:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg1)])      \n        else:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg2)])      \n                \n        img= self.load_img(series_id)\n        if self.isTrain:\n            return {\n                'img': img, \n                'label': label,\n            }\n        else:\n            return {\n                'img': img \n            }\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n    \n\nresize_transform_point= A.Compose([\nA.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\nA.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n])\n\n\n\n\nds = PreTrainDataset(train_df, cfg,resize_transform_point)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\nfor k, v in ds[0].items():\n    print(k, v.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:08.730449Z","iopub.execute_input":"2024-11-09T13:42:08.730772Z","iopub.status.idle":"2024-11-09T13:42:10.364062Z","shell.execute_reply.started":"2024-11-09T13:42:08.730718Z","shell.execute_reply":"2024-11-09T13:42:10.363007Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nimg (3, 256, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load backbone for RSNA 2024 task\nmodel_path = \"/kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\"\nmodel = timm.create_model('resnet18', pretrained=False, num_classes=20)\nmodel = model.to(cfg.device)\nload_weights_skip_mismatch(model, model_path, cfg.device)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:10.365451Z","iopub.execute_input":"2024-11-09T13:42:10.365897Z","iopub.status.idle":"2024-11-09T13:42:11.454675Z","shell.execute_reply.started":"2024-11-09T13:42:10.365859Z","shell.execute_reply":"2024-11-09T13:42:11.453670Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2699087807.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(weights_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded weights from: /kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Co-ordinate Inferencing","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:35:01.530636Z","iopub.execute_input":"2024-10-07T04:35:01.531106Z","iopub.status.idle":"2024-10-07T04:35:01.554979Z","shell.execute_reply.started":"2024-10-07T04:35:01.531037Z","shell.execute_reply":"2024-10-07T04:35:01.55382Z"}}},{"cell_type":"code","source":"def coordinate_prediction(model,pred_dataloader,isSagitalT1 = False):\n    predictions = []\n    with torch.no_grad():\n        model = model.eval()\n        for batch in tqdm(pred_dataloader):\n            batch = batch_to_device(batch, cfg.device)\n\n            pred = model(batch[\"img\"].float())\n            pred = torch.sigmoid(pred)\n            predictions.append(pred)\n            \n    data_list_cpu = [tensor.cpu().numpy() for tensor in predictions]\n    combined_data = np.vstack(data_list_cpu)\n    df_cords = pd.DataFrame(combined_data)\n    if isSagitalT1:\n        df_cords.to_csv(\"predicted_cordinates_sag_t1.csv\",index=False)\n    else:\n        df_cords.to_csv(\"predicted_cordinates_sag_t2.csv\",index=False)\n    return df_cords","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:11.456162Z","iopub.execute_input":"2024-11-09T13:42:11.456991Z","iopub.status.idle":"2024-11-09T13:42:11.464150Z","shell.execute_reply.started":"2024-11-09T13:42:11.456943Z","shell.execute_reply":"2024-11-09T13:42:11.463170Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Coordinate Prediction","metadata":{}},{"cell_type":"code","source":"train_ds = PreTrainDataset(train_df, cfg,resize_transform_point)\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:11.467862Z","iopub.execute_input":"2024-11-09T13:42:11.468171Z","iopub.status.idle":"2024-11-09T13:42:11.475834Z","shell.execute_reply.started":"2024-11-09T13:42:11.468140Z","shell.execute_reply":"2024-11-09T13:42:11.474969Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cordinates_t1 = coordinate_prediction(model,train_dl,isSagitalT1 = True)\ncordinates_t2 = coordinate_prediction(model,train_dl,isSagitalT1 = False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:11.477040Z","iopub.execute_input":"2024-11-09T13:42:11.477390Z","iopub.status.idle":"2024-11-09T13:42:14.456488Z","shell.execute_reply.started":"2024-11-09T13:42:11.477357Z","shell.execute_reply":"2024-11-09T13:42:14.455567Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"cordinates_t1['study_id'] = train_df['study_id'].values\ncordinates_t2['study_id'] = train_df['study_id'].values","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.457711Z","iopub.execute_input":"2024-11-09T13:42:14.458096Z","iopub.status.idle":"2024-11-09T13:42:14.464108Z","shell.execute_reply.started":"2024-11-09T13:42:14.458061Z","shell.execute_reply":"2024-11-09T13:42:14.463161Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"cordinates_t1","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.465390Z","iopub.execute_input":"2024-11-09T13:42:14.465747Z","iopub.status.idle":"2024-11-09T13:42:14.488839Z","shell.execute_reply.started":"2024-11-09T13:42:14.465679Z","shell.execute_reply":"2024-11-09T13:42:14.487800Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         0        1         2        3         4         5         6  \\\n0  0.40336  0.31897  0.624557  0.36428  0.359472  0.430439  0.565006   \n\n          7         8         9  ...        11        12        13        14  \\\n0  0.454914  0.314881  0.540189  ...  0.563507  0.322633  0.682666  0.545385   \n\n         15        16        17        18        19  study_id  \n0  0.653523  0.385318  0.841513  0.562395  0.713457  44036939  \n\n[1 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>study_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.40336</td>\n      <td>0.31897</td>\n      <td>0.624557</td>\n      <td>0.36428</td>\n      <td>0.359472</td>\n      <td>0.430439</td>\n      <td>0.565006</td>\n      <td>0.454914</td>\n      <td>0.314881</td>\n      <td>0.540189</td>\n      <td>...</td>\n      <td>0.563507</td>\n      <td>0.322633</td>\n      <td>0.682666</td>\n      <td>0.545385</td>\n      <td>0.653523</td>\n      <td>0.385318</td>\n      <td>0.841513</td>\n      <td>0.562395</td>\n      <td>0.713457</td>\n      <td>44036939</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Utils for LSDD ","metadata":{}},{"cell_type":"code","source":"def get_padded_roi(orientation, numberOfImageFromCenter):\n    # Calculate middle index\n    middleImage = len(orientation[\"array\"]) // 2\n\n    # Calculate start and end indices\n    start_idx = middleImage - numberOfImageFromCenter\n    end_idx = middleImage + numberOfImageFromCenter + 1\n\n    # Handle bounds\n    array_length = orientation[\"array\"].shape[0]  # Slicing along the first axis (number of images)\n\n    # Ensure we don't go beyond the array's bounds\n    start_pad = max(0, -start_idx)\n    end_pad = max(0, end_idx - array_length)\n\n    # Slice the valid part of the array along the first axis\n    roi = orientation[\"array\"][max(0, start_idx):min(array_length, end_idx)]\n\n    # Pad with zeros if needed\n    if start_pad > 0 or end_pad > 0:\n        roi = np.pad(roi, ((start_pad, end_pad), (0, 0), (0, 0)), mode='constant', constant_values=0)\n\n    return roi","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.490154Z","iopub.execute_input":"2024-11-09T13:42:14.490490Z","iopub.status.idle":"2024-11-09T13:42:14.499118Z","shell.execute_reply.started":"2024-11-09T13:42:14.490456Z","shell.execute_reply":"2024-11-09T13:42:14.498193Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def prepare_level_wise_axial(sagittal_img, imgsag_y_coord_to_axial_slice, coordinates,no_of_axial_slice = 3):\n    h, w = sagittal_img.shape\n    axial_list = []\n    \n    first_key = next(iter(imgsag_y_coord_to_axial_slice))\n    # Get the first value\n    first_value = imgsag_y_coord_to_axial_slice[first_key]\n\n    keys = list(imgsag_y_coord_to_axial_slice.keys())\n    keys.sort()\n\n    for i in range(0, len(coordinates), 4):\n        #print(i)\n        category = coordinates[i:i+4]  # Extracting 4 elements at a time\n        y= [category[1]*h,category[3]*h]\n        minimum = math.floor(min(y[0],y[1]))\n        maximum  = math.ceil(max(y[0],y[1]))\n        filtered_keys = [k for k in imgsag_y_coord_to_axial_slice.keys() if minimum <= k <= maximum]\n        \n        if len(filtered_keys) >= no_of_axial_slice:\n            # Use numpy to select 3 keys at uniform intervals\n            selected_keys = np.linspace(0, len(filtered_keys)-1,no_of_axial_slice , dtype=int)\n            selected_keys = [filtered_keys[i] for i in selected_keys]\n        else:\n            if len(filtered_keys) == 0:\n                index = bisect.bisect_left(keys, minimum)\n                # Check if we can find a nearest value less than current_value\n                if index > 0:\n                    filtered_keys.append(keys[index - 1])\n\n                index = bisect.bisect_right(keys, maximum)\n                if index < len(keys):\n                    filtered_keys.append(keys[index])  # Return the nearest value greater than current_value\n                    \n\n            filtered_keys.extend([filtered_keys[-1]] * (no_of_axial_slice - len(filtered_keys)))\n            selected_keys = filtered_keys\n        \n        if len(selected_keys) ==  0:\n            result  = np.zeros((no_of_axial_slice, first_value.shape[0],first_value.shape[1]))\n        else:\n            selected_axial =[imgsag_y_coord_to_axial_slice.get(k) for k in selected_keys]\n            result  = np.array(selected_axial)\n\n        roi_copy_list  = []\n        for j in range(result.shape[0]):\n            roi_copy_list.append(resize_transform(image=result[j])[\"image\"])\n            \n        axial_list.append(np.array(roi_copy_list))\n        \n    return  np.array(axial_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.500584Z","iopub.execute_input":"2024-11-09T13:42:14.500949Z","iopub.status.idle":"2024-11-09T13:42:14.517669Z","shell.execute_reply.started":"2024-11-09T13:42:14.500907Z","shell.execute_reply":"2024-11-09T13:42:14.516812Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"resize_transform= A.Compose([\n    A.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\n    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n    A.Normalize()\n])\n\ndef angle_of_line(x1, y1, x2, y2):\n    return math.degrees(math.atan2(-(y2-y1), x2-x1))\n\ndef crop_between_keypoints(roi,img, keypoint1, keypoint2):\n    h, w = img.shape\n    x1, y1 = int(keypoint1[0]), int(keypoint1[1])\n    x2, y2 = int(keypoint2[0]), int(keypoint2[1])\n\n    # Calculate bounding box around the keypoints\n    left = int(min(x1, x2) - (w * 0.1))\n    right = int(max(x1, x2) + (w * 0.1))\n    top = int(min(y1, y2) - (h * 0.07))\n    bottom = int(max(y1, y2) + (h * 0.1))\n    \n    left = max(0, left)\n    right = min(w, right)\n    top = max(0, top)\n    bottom = min(h, bottom)\n    # Crop the image\n    return img[top:bottom, left:right],roi[:,top:bottom, left:right]\n\ndef plot_5_crops(orientataion,coords_temp,numberOfImageFromCenter = 3):\n    # Create a figure and axis for the grid\n    #fig = plt.figure(figsize=(10, 10))\n    #gs = gridspec.GridSpec(1, 5, width_ratios=[1]*5)\n    \n    #print(coords_temp)\n    # Plot the crops\n    #print(\"plot-5-crop-img\")\n    orientataion['array'] = get_padded_roi(orientataion,numberOfImageFromCenter)\n    middleImage = len(orientataion[\"array\"])//2\n    img = orientataion[\"array\"][middleImage]\n    \n    roi = orientataion[\"array\"][middleImage-numberOfImageFromCenter:middleImage+numberOfImageFromCenter+1]\n    croppedImage = []\n    #print(p)\n    for i in range(0, len(coords_temp), 4):\n        # Copy of img\n        img_copy= img.copy()\n        h, w = img.shape\n        roi_copy = roi.copy()\n        # Extract Keypoints\n        category = coords_temp[i:i+4]  # Extracting 4 elements at a time\n        a= (category[0]*w, category[1]*h)\n        b= (category[2]*w, category[3]*h)\n        \n        # Rotate\n        rotate_angle= angle_of_line(a[0], a[1], b[0], b[1])\n        transform = A.Compose([\n            A.Rotate(limit=(-rotate_angle, -rotate_angle), p=1.0),\n        ], keypoint_params= A.KeypointParams(format='xy', remove_invisible=False),\n        )\n\n        t= transform(image=img_copy, keypoints=[a,b])\n        img_copy= t[\"image\"]\n        a,b= t[\"keypoints\"]\n        #print(img_copy.shape)\n        # Crop + Resize\n        img_copy,roi_copy = crop_between_keypoints(roi_copy,img_copy, a, b)\n        #print(roi_copy.shape)\n        #print(img_copy.shape)\n        roi_copy_list  = []\n        for j in range (0,numberOfImageFromCenter*2+1):\n            roi_copy_list.append(resize_transform(image=roi_copy[j])[\"image\"])\n        roi_copy_list = np.array(roi_copy_list)\n        \n        img_copy = roi_copy_list[numberOfImageFromCenter]\n        croppedImage.append(roi_copy_list)\n        # Plot\n        #ax = plt.subplot(gs[i//4])\n        #ax.imshow(img_copy, cmap='gray')\n        #ax.set_title(f\"L{i//4+1}\")\n        #ax.axis('off')\n    #plt.show()\n    return np.array(croppedImage)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.518814Z","iopub.execute_input":"2024-11-09T13:42:14.519117Z","iopub.status.idle":"2024-11-09T13:42:14.541174Z","shell.execute_reply.started":"2024-11-09T13:42:14.519084Z","shell.execute_reply":"2024-11-09T13:42:14.540280Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Define DataSet for training","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# # Sample DataFrame\n# df = pd.DataFrame({\n#     'A': [0, 1, 2],\n#     'B': [1, 0, 2],\n#     'C': [2, 2, 1],\n#     'D': [0, 1, 0],\n#     'E': [1, 0, 1]\n#     # add 25 columns in actual case\n# })\n\n# # One-hot encode each column in the DataFrame\n# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# # Apply the one-hot encoding function to each row\n# encoded_data = np.array([one_hot_encode_row(row) for row in df.values])\n\n# # Example: to check one row's shape (25, 3)\n# print(encoded_data[0].shape)  # Should output (5, 3) for this example, adjust to 25 columns\n\n# # Example: print the one-hot encoded array for a row\n# print(encoded_data[0])  # One-hot encoded first row\n\n# # Final shape of the encoded data for all rows\n# print(encoded_data.shape)  # (number_of_rows, 25, 3)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.542404Z","iopub.execute_input":"2024-11-09T13:42:14.542909Z","iopub.status.idle":"2024-11-09T13:42:14.554342Z","shell.execute_reply.started":"2024-11-09T13:42:14.542867Z","shell.execute_reply":"2024-11-09T13:42:14.553437Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# s = one_hot_encode_row([0,1,2,0,0])\n# # Apply the one-hot encoding function to each row\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.555648Z","iopub.execute_input":"2024-11-09T13:42:14.556116Z","iopub.status.idle":"2024-11-09T13:42:14.563124Z","shell.execute_reply.started":"2024-11-09T13:42:14.556072Z","shell.execute_reply":"2024-11-09T13:42:14.562229Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class RSNADataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids, s1_coords, s2_coords,cfg,transform,isTrain = True,transform_axial = None,transform_sag = None):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.transform_sag = transform_sag\n        self.transform_axial = transform_axial\n        self.s1_coords = self.align_cord(study_ids,s1_coords)\n        self.s2_coords = self.align_cord(study_ids,s2_coords)\n        self.labeldf = study_ids[[col for col in study_ids.columns if col not in ['study_id','series_id_sg1','series_id_sg2','series_id_a2']]]\n\n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n    \n    def align_cord(self,study_ids,cords):\n        std_id = study_ids[['study_id']] \n        merged = pd.merge(std_id,cords,on=['study_id'],how=\"inner\")\n        return merged[[col for col in merged.columns if col not in ['study_id']]]\n\n    def load_dicom_stack(self, dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n\n        # Determine the plane for sorting (sagittal, coronal, axial)\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n\n        # Sort DICOM files based on positions (reverse sort for axial plane if needed)\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n\n        # Get the shape of each pixel array (height, width)\n        shapes = [d.pixel_array.shape for d in dicoms]\n        # Check if all DICOM images have the same shape\n        if len(set(shapes)) > 1:\n            # There's a shape mismatch, find the minimum shape (height, width)\n            min_shape = np.min(shapes, axis=0)\n            # Resize images to the minimum shape\n            resized_arrays = []\n            for d in dicoms:\n                img = d.pixel_array.astype(\"float32\")\n                if img.shape != tuple(min_shape):\n                    resized_img = cv2.resize(img, (min_shape[1], min_shape[0]))  # Resize to (width, height)\n                else:\n                    resized_img = img  # No resizing needed\n                resized_arrays.append(resized_img)\n\n            # Stack the resized images along the first axis\n            array = np.stack(resized_arrays)\n        else:\n            # If all shapes are the same, no resizing is needed\n            array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n\n        # Reorder the array according to the sorted positions\n        array = array[idx]\n\n        return {\n            \"array\": self.convert_to_8bit(array),\n            \"positions\": ipp,\n            \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")\n        }\n     \n    def one_hot_encode_row(self, row):\n        return np.array([np.eye(3)[int(val)] for val in row]) \n    def __getitem__(self, idx):\n        row = self.study_ids.iloc[idx]\n        \n        sag_t1 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg1)), plane=\"sagittal\")\n        ax_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_a2)), plane=\"axial\", reverse_sort=True)\n        sag_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg2)), plane=\"sagittal\")\n        \n        top_left_hand_corner_sag_t2 = sag_t2[\"positions\"][len(sag_t2[\"array\"]) // 2]\n        sag_y_axis_to_pixel_space = [top_left_hand_corner_sag_t2[2]]\n        while len(sag_y_axis_to_pixel_space) < sag_t2[\"array\"].shape[1]: \n            sag_y_axis_to_pixel_space.append(sag_y_axis_to_pixel_space[-1] - sag_t2[\"pixel_spacing\"][1])\n        \n        sag_y_coord_to_axial_slice = {}\n        for ax_t2_slice, ax_t2_pos in zip(ax_t2[\"array\"], ax_t2[\"positions\"]):\n            diffs = np.abs(np.asarray(sag_y_axis_to_pixel_space) - ax_t2_pos[2])\n            sag_y_coord = np.argmin(diffs)\n            sag_y_coord_to_axial_slice[sag_y_coord] = ax_t2_slice\n        \n        sag1_cord = self.s1_coords.iloc[idx].tolist()\n        sag2_cord = self.s2_coords.iloc[idx].tolist()\n        \n        #print(\"s1 cord\",sag1_cord)\n        #print(\"s2 cord\",sag2_cord)\n        img= sag_t2[\"array\"][len(sag_t2[\"array\"])//2]\n        corresponding_axial = self.transform_axial(img, sag_y_coord_to_axial_slice, sag2_cord,no_of_axial_slice = self.cfg.sag_axial_slices)\n        #print(corresponding_axial.shape)\n\n        \n        crop_result_t2 = self.transform_sag(sag_t2, sag2_cord,numberOfImageFromCenter = self.cfg.sag_2_slices)\n        #print(crop_result_t2.shape)\n        \n        crop_result_t1 = self.transform_sag(sag_t1, sag1_cord,numberOfImageFromCenter = self.cfg.sag_1_slices)\n        #print(crop_result_t1.shape)\n        \n        \n        \n        if self.isTrain:\n            label  = self.one_hot_encode_row( self.labeldf.iloc[idx])\n            return (crop_result_t2,crop_result_t1,corresponding_axial),label\n        else:\n            return (crop_result_t2,crop_result_t1, corresponding_axial)\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n\nds = RSNADataset(train_df,cordinates_t1,cordinates_t2 ,cfg,resize_transform,isTrain= False,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\n(k1,k2,k3)  =  ds[0]\nprint(\"k1 shape\",k1.shape)\nprint(\"k1 shape\",k2.shape)\nprint(\"k3 shape\",k3.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:14.564687Z","iopub.execute_input":"2024-11-09T13:42:14.565077Z","iopub.status.idle":"2024-11-09T13:42:17.508322Z","shell.execute_reply.started":"2024-11-09T13:42:14.564998Z","shell.execute_reply":"2024-11-09T13:42:17.507366Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nk1 shape (5, 3, 256, 256)\nk1 shape (5, 9, 256, 256)\nk3 shape (5, 3, 256, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.509548Z","iopub.execute_input":"2024-11-09T13:42:17.509947Z","iopub.status.idle":"2024-11-09T13:42:17.514664Z","shell.execute_reply.started":"2024-11-09T13:42:17.509910Z","shell.execute_reply":"2024-11-09T13:42:17.513738Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#  model1 = models.video.r3d_18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.515965Z","iopub.execute_input":"2024-11-09T13:42:17.516617Z","iopub.status.idle":"2024-11-09T13:42:17.528219Z","shell.execute_reply.started":"2024-11-09T13:42:17.516574Z","shell.execute_reply":"2024-11-09T13:42:17.527324Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# model1.stem[0].weight","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.529272Z","iopub.execute_input":"2024-11-09T13:42:17.529618Z","iopub.status.idle":"2024-11-09T13:42:17.538684Z","shell.execute_reply.started":"2024-11-09T13:42:17.529576Z","shell.execute_reply":"2024-11-09T13:42:17.537573Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# print(model1)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.539801Z","iopub.execute_input":"2024-11-09T13:42:17.540415Z","iopub.status.idle":"2024-11-09T13:42:17.547260Z","shell.execute_reply.started":"2024-11-09T13:42:17.540372Z","shell.execute_reply":"2024-11-09T13:42:17.546402Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n\n        # Define three 3D ResNet-18 models\n        self.model1 = models.video.r3d_18(pretrained=True)  # First model (input shape: 5,3,256,256)\n        self.model2 = models.video.r3d_18(pretrained=True)  # Second model (input shape: 5,3,256,256)\n        self.model3 = models.video.r3d_18(pretrained=True)  # Third model (input shape: 5,9,256,256)\n        \n        original_conv1 = self.model1.stem[0]\n        self.model1.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model2.stem[0] = nn.Conv3d(5, 64, kernel_size=(9, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model3.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n\n        \n        with torch.no_grad():\n            self.model1.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model1.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            self.model2.stem[0].weight[:, :3, :3, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, 3:, :3, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            #self.model2.stem[0].weight[:, :, :3, :, :] = self.model2.stem[0].weight[:, :, :, :, :]  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, :, 3:6, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            self.model2.stem[0].weight[:, :, 6:9, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            \n            \n            self.model3.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model3.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n        \n        self.model1.fc = nn.Identity()\n        self.model2.fc = nn.Identity()\n        self.model3.fc = nn.Identity()\n        \n        # Extract the feature size from the output of the models\n        self.hidden_size = 32  # Adjust this if necessary, based on the model's output size\n        self.flatten_size = 512\n        # Concatenation layer\n        self.fc = nn.Linear(self.flatten_size * 3, self.hidden_size)   # Final linear layer after concatenation\n\n        # Subclass outputs (25 classes, each has 3 subclasses)\n        self.subclass_layers = nn.ModuleList([nn.Linear(self.hidden_size, 3) for _ in range(25)])\n\n    def forward(self, x1, x2, x3):\n        # Forward pass through each model\n        output1 = self.model1(x1)  # Shape: [batch_size, hidden_size]\n        output2 = self.model2(x2)  # Shape: [batch_size, hidden_size]\n        output3 = self.model3(x3)  # Shape: [batch_size, hidden_size]\n        \n        flatten1 = torch.flatten(output1, 1)\n        flatten2 = torch.flatten(output2, 1)\n        flatten3 = torch.flatten(output3, 1)\n        #print(flatten1.shape)\n        #print(flatten2.shape)\n       # print(flatten3.shape)\n        # Concatenate outputs from the three models\n        concatenated_output = torch.cat((flatten1, flatten2,flatten2), dim=1)  # Shape: [batch_size, hidden_size * 3]\n       #print(concatenated_output.shape)\n        # Pass concatenated output through the final linear layer\n        combined_output = self.fc(concatenated_output)  # Shape: [batch_size, hidden_size]\n\n        # Subclass prediction for each class\n        subclass_outputs = [torch.softmax(layer(combined_output), dim=1) for layer in self.subclass_layers]\n\n        return subclass_outputs\n\n# Example usage\n#if __name__ == \"__main__\":\n  ","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.548694Z","iopub.execute_input":"2024-11-09T13:42:17.549029Z","iopub.status.idle":"2024-11-09T13:42:17.568912Z","shell.execute_reply.started":"2024-11-09T13:42:17.548997Z","shell.execute_reply":"2024-11-09T13:42:17.567973Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# model = CustomModel()\n# model = model.to(cfg.device)\n\n# # Example input tensors\n# input1 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the first model\n# input2 = torch.randn(8, 5, 9, 256, 256)  # Batch size 8 for the second model\n# input3 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the third model\n\n# outputs = model(input1.to(cfg.device), input2.to(cfg.device), input3.to(cfg.device))\n# for i, output in enumerate(outputs):\n#     print(f\"Subclass Output for Class {i + 1}: Shape {output.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.569941Z","iopub.execute_input":"2024-11-09T13:42:17.570270Z","iopub.status.idle":"2024-11-09T13:42:17.580684Z","shell.execute_reply.started":"2024-11-09T13:42:17.570239Z","shell.execute_reply":"2024-11-09T13:42:17.579748Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n\n# def prepare_target(raw_labels):\n#     \"\"\"\n#     Convert raw labels into one-hot encoded labels for 25 classes.\n    \n#     Args:\n#         raw_labels (torch.Tensor): Tensor of shape [batch_size, 25] where each value is the subclass label (0, 1, or 2).\n        \n#     Returns:\n#         torch.Tensor: One-hot encoded labels of shape [batch_size, 25, 3].\n#     \"\"\"\n#     batch_size = raw_labels.shape[0]\n#     num_classes = raw_labels.shape[1]\n#     num_subclasses = 3  # There are 3 subclasses\n\n#     # One-hot encode the subclass labels\n#     one_hot_labels = F.one_hot(raw_labels, num_classes=num_subclasses)  # Shape: [batch_size, 25, 3]\n    \n#     return one_hot_labels.float()  # Return as float for compatibility with loss functions\n\n# # Example usage:\n# batch_size = 8\n# num_classes = 25\n\n# # Randomly generated raw labels where each value is 0, 1, or 2\n# raw_labels = torch.randint(0, 3, (batch_size, num_classes))  # Shape: [batch_size, 25]\n\n# # Prepare one-hot encoded targets\n# targets = prepare_target(raw_labels)\n\n# print(targets.shape)  # Should print: torch.Size([8, 25, 3])","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.582078Z","iopub.execute_input":"2024-11-09T13:42:17.582349Z","iopub.status.idle":"2024-11-09T13:42:17.593339Z","shell.execute_reply.started":"2024-11-09T13:42:17.582320Z","shell.execute_reply":"2024-11-09T13:42:17.592505Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Training And Validation Functions","metadata":{}},{"cell_type":"code","source":"def val_epoch(model, loader):\n\n    model.eval()\n    predictions = []\n    bar = tqdm(loader)\n    with torch.no_grad():\n        for data in bar:\n            \n            sagittal_t2,sagittal_t1,axial_t2, = data\n            axial_t2, sagittal_t2, sagittal_t1 = axial_t2.to(device), sagittal_t2.to(device), sagittal_t1.to(device)\n           \n            logits = model(sagittal_t2,sagittal_t1,axial_t2)\n             \n            loss = 0\n            batch_prediction = []\n            for i in range(25):\n                # Select the i-th subclass prediction and corresponding target\n                subclass_pred = logits[i]  # Output from model, shape: [batch_size, 3]\n                #print(subclass_pred.detach().cpu().numpy().shape)\n                batch_prediction.append(subclass_pred.detach().cpu().numpy())\n                #print(subclass_pred)\n                # Compute the loss for the i-th subclass\n\n            predictions.append(np.array(batch_prediction))\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.594371Z","iopub.execute_input":"2024-11-09T13:42:17.594768Z","iopub.status.idle":"2024-11-09T13:42:17.603997Z","shell.execute_reply.started":"2024-11-09T13:42:17.594714Z","shell.execute_reply":"2024-11-09T13:42:17.603167Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"dataset_train = RSNADataset(train_df,cordinates_t1,cordinates_t2,cfg,resize_transform, isTrain= False,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)\nprediction_dl = torch.utils.data.DataLoader(dataset_train, batch_size=cfg.batch_size, sampler=RandomSampler(dataset_train), num_workers=cfg.num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.609970Z","iopub.execute_input":"2024-11-09T13:42:17.610252Z","iopub.status.idle":"2024-11-09T13:42:17.626229Z","shell.execute_reply.started":"2024-11-09T13:42:17.610223Z","shell.execute_reply":"2024-11-09T13:42:17.625140Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = cfg.CUDA_VISIBLE_DEVICES\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:17.627599Z","iopub.execute_input":"2024-11-09T13:42:17.628055Z","iopub.status.idle":"2024-11-09T13:42:17.633625Z","shell.execute_reply.started":"2024-11-09T13:42:17.628014Z","shell.execute_reply":"2024-11-09T13:42:17.632679Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":" cfg.model_dir_lsdd","metadata":{"execution":{"iopub.status.busy":"2024-11-09T14:05:57.419516Z","iopub.execute_input":"2024-11-09T14:05:57.420281Z","iopub.status.idle":"2024-11-09T14:05:57.426708Z","shell.execute_reply.started":"2024-11-09T14:05:57.420239Z","shell.execute_reply":"2024-11-09T14:05:57.425771Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/training-lsdd/resnet18_final_fold0.pth'"},"metadata":{}}]},{"cell_type":"code","source":"model = CustomModel()\nmodel = model.to(device)\n\n# Load the model weights (assuming 'model_weights.pth' is the saved file)\nmodel_weights_path = \"/kaggle/input/lumbarspinediseasedetection/pytorch/v1/1/resnet18_final_fold0.pth\"\nmodel.load_state_dict(torch.load(model_weights_path,map_location=device))\n\n# Set the model to evaluation mode (if you are using it for inference)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T14:06:54.742750Z","iopub.execute_input":"2024-11-09T14:06:54.743238Z","iopub.status.idle":"2024-11-09T14:07:15.638275Z","shell.execute_reply.started":"2024-11-09T14:06:54.743198Z","shell.execute_reply":"2024-11-09T14:07:15.636790Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1448\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    823\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n","\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model weights (assuming 'model_weights.pth' is the saved file)\u001b[39;00m\n","Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mCustomModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28msuper\u001b[39m(CustomModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define three 3D ResNet-18 models\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr3d_18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# First model (input shape: 5,3,256,256)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel2 \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mr3d_18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Second model (input shape: 5,3,256,256)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel3 \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mr3d_18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Third model (input shape: 5,9,256,256)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/resnet.py:408\u001b[0m, in \u001b[0;36mr3d_18\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct 18 layer Resnet3D model.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m.. betastatus:: video module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m weights \u001b[38;5;241m=\u001b[39m R3D_18_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_video_resnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBasicBlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mConv3DSimple\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBasicStem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/resnet.py:306\u001b[0m, in \u001b[0;36m_video_resnet\u001b[0;34m(block, conv_makers, layers, stem, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m model \u001b[38;5;241m=\u001b[39m VideoResNet(block, conv_makers, layers, stem, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:765\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    763\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    764\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:624\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    622\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    623\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 624\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"],"ename":"URLError","evalue":"<urlopen error [Errno -3] Temporary failure in name resolution>","output_type":"error"}]},{"cell_type":"code","source":"predictions = val_epoch(model,prediction_dl)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.830800Z","iopub.status.idle":"2024-11-09T13:42:39.831157Z","shell.execute_reply.started":"2024-11-09T13:42:39.830980Z","shell.execute_reply":"2024-11-09T13:42:39.830998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.833232Z","iopub.status.idle":"2024-11-09T13:42:39.833606Z","shell.execute_reply.started":"2024-11-09T13:42:39.833424Z","shell.execute_reply":"2024-11-09T13:42:39.833443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_arr = np.concatenate(predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.834937Z","iopub.status.idle":"2024-11-09T13:42:39.835298Z","shell.execute_reply.started":"2024-11-09T13:42:39.835116Z","shell.execute_reply":"2024-11-09T13:42:39.835133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_arr.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.836459Z","iopub.status.idle":"2024-11-09T13:42:39.836834Z","shell.execute_reply.started":"2024-11-09T13:42:39.836632Z","shell.execute_reply":"2024-11-09T13:42:39.836650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class names (for example)\nclass_names = [\n 'spinal_canal_stenosis_l1_l2',\n 'spinal_canal_stenosis_l2_l3',\n 'spinal_canal_stenosis_l3_l4',\n 'spinal_canal_stenosis_l4_l5',\n 'spinal_canal_stenosis_l5_s1',\n 'left_neural_foraminal_narrowing_l1_l2',\n 'left_neural_foraminal_narrowing_l2_l3',\n 'left_neural_foraminal_narrowing_l3_l4',\n 'left_neural_foraminal_narrowing_l4_l5',\n 'left_neural_foraminal_narrowing_l5_s1',\n 'right_neural_foraminal_narrowing_l1_l2',\n 'right_neural_foraminal_narrowing_l2_l3',\n 'right_neural_foraminal_narrowing_l3_l4',\n 'right_neural_foraminal_narrowing_l4_l5',\n 'right_neural_foraminal_narrowing_l5_s1',\n 'left_subarticular_stenosis_l1_l2',\n 'left_subarticular_stenosis_l2_l3',\n 'left_subarticular_stenosis_l3_l4',\n 'left_subarticular_stenosis_l4_l5',\n 'left_subarticular_stenosis_l5_s1',\n 'right_subarticular_stenosis_l1_l2',\n 'right_subarticular_stenosis_l2_l3',\n 'right_subarticular_stenosis_l3_l4',\n 'right_subarticular_stenosis_l4_l5',\n 'right_subarticular_stenosis_l5_s1']\n\n# Prepare a list to hold the data for the DataFrame\ndata = []\n\n# Populate the data list with class names and subclass predictions\nfor class_idx, class_name in enumerate(class_names):\n    for example_idx in range(predictions_arr.shape[1]):  # Loop through examples\n        # Get subclass predictions for this class and example\n        subclass_predictions = predictions_arr[class_idx, example_idx, :].tolist()\n        # Append to the data list\n        data.append([f\"{train_df.iloc[example_idx].study_id}_{class_name}\"] + subclass_predictions)\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=['row_id', 'normal_mild', 'moderate', 'severe'])\n\n# Display the DataFrame\ndf_sorted = df.sort_values(by='row_id')","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.838452Z","iopub.status.idle":"2024-11-09T13:42:39.838807Z","shell.execute_reply.started":"2024-11-09T13:42:39.838616Z","shell.execute_reply":"2024-11-09T13:42:39.838632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sorted['normal_mild'] = df_sorted['normal_mild'].round(7)\ndf_sorted['moderate'] = df_sorted['moderate'].round(7)\ndf_sorted['severe'] = df_sorted['severe'].round(7)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.839722Z","iopub.status.idle":"2024-11-09T13:42:39.840079Z","shell.execute_reply.started":"2024-11-09T13:42:39.839909Z","shell.execute_reply":"2024-11-09T13:42:39.839927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sorted.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.841070Z","iopub.status.idle":"2024-11-09T13:42:39.841416Z","shell.execute_reply.started":"2024-11-09T13:42:39.841242Z","shell.execute_reply":"2024-11-09T13:42:39.841259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sorted.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.843315Z","iopub.status.idle":"2024-11-09T13:42:39.843802Z","shell.execute_reply.started":"2024-11-09T13:42:39.843539Z","shell.execute_reply":"2024-11-09T13:42:39.843564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sorted","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:42:39.845159Z","iopub.status.idle":"2024-11-09T13:42:39.845641Z","shell.execute_reply.started":"2024-11-09T13:42:39.845391Z","shell.execute_reply":"2024-11-09T13:42:39.845415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}