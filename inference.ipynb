{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":9578054,"sourceType":"datasetVersion","datasetId":5824402},{"sourceId":206138086,"sourceType":"kernelVersion"},{"sourceId":127817,"sourceType":"modelInstanceVersion","modelInstanceId":107642,"modelId":131980},{"sourceId":129460,"sourceType":"modelInstanceVersion","modelInstanceId":109072,"modelId":133404}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\nimport albumentations as A\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nimport pydicom\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom sklearn.model_selection import train_test_split\nimport bisect\nimport time\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-16T14:56:15.375685Z","iopub.execute_input":"2024-11-16T14:56:15.376089Z","iopub.status.idle":"2024-11-16T14:56:43.365700Z","shell.execute_reply.started":"2024-11-16T14:56:15.376049Z","shell.execute_reply":"2024-11-16T14:56:43.364905Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# Config\ncfg= SimpleNamespace(\n    img_dir= \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images\",\n    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    n_frames=3,\n    epochs=10,\n    lr=0.0005,\n    batch_size=16,\n    backbone=\"resnet18\",\n    seed= 0,\n    model_dir = \"/kaggle/working/\",\n    kernel_type = \"resnet18\",\n    num_workers = 8,\n    n_epochs = 5,\n    init_lr =0.0005,\n    CUDA_VISIBLE_DEVICES = \"0\",\n    sag_axial_slices = 3,\n    sag_2_slices =  1,\n    sag_1_slices = 4,\n    model_dir_point = \"/kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\",\n    model_dir_lsdd = \"/kaggle/input/training-lsdd/resnet18_final_fold0.pth\",\n    \n)\nset_seed(seed=cfg.seed) # Makes results reproducable","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.367222Z","iopub.execute_input":"2024-11-16T14:56:43.367678Z","iopub.status.idle":"2024-11-16T14:56:43.411632Z","shell.execute_reply.started":"2024-11-16T14:56:43.367645Z","shell.execute_reply":"2024-11-16T14:56:43.410771Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_training_dataframe(cfg,isTrain=True):\n    \n    test_series_description = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')\n\n    sagtialla_t1_df = test_series_description[test_series_description['series_description'] == \"Sagittal T1\"]\n    sagtialla_t1_df_u = sagtialla_t1_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n    \n    axial_t2_df = test_series_description[test_series_description['series_description'] == \"Axial T2\"]\n    axial_t2_df_u = axial_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    sagtialla_t2_df = test_series_description[test_series_description['series_description'] == \"Sagittal T2/STIR\"]\n    sagtialla_t2_df_u = sagtialla_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    merge_sagital_t1 = sagtialla_t1_df_u\n    merge_sagital_t1 = merge_sagital_t1.drop(columns=['series_description'])\n    merge_sagital_t1.rename(columns={'series_id': 'series_id_sg1'}, inplace=True)\n    \n    merge_sagital_t2 = pd.merge(merge_sagital_t1,sagtialla_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_sagital_t2 = merge_sagital_t2.drop(columns=['series_description'])\n    merge_sagital_t2.rename(columns={'series_id': 'series_id_sg2'}, inplace=True)\n\n    merge_axial_t2 = pd.merge(merge_sagital_t2,axial_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_axial_t2 = merge_axial_t2.drop(columns=['series_description'])\n    merge_axial_t2.rename(columns={'series_id': 'series_id_a2'}, inplace=True)\n\n    return merge_axial_t2  \n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.412783Z","iopub.execute_input":"2024-11-16T14:56:43.413119Z","iopub.status.idle":"2024-11-16T14:56:43.422966Z","shell.execute_reply.started":"2024-11-16T14:56:43.413086Z","shell.execute_reply":"2024-11-16T14:56:43.422090Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def batch_to_device(batch, device, skip_keys=[]):\n    batch_dict= {}\n    for key in batch:\n        if key in skip_keys:\n             batch_dict[key]= batch[key]\n        else:    \n            batch_dict[key]= batch[key].to(device)\n    return batch_dict\n\ndef visualize_prediction(batch, pred, epoch):\n    \n    mid= cfg.n_frames//2\n    \n    # Plot\n    for idx in range(1):\n    \n        # Select Data\n        img= batch[\"img\"][idx, mid, :, :].cpu().numpy()*255\n        cs_true= batch[\"label\"][idx, ...].cpu().numpy()*256\n        cs= pred[idx, ...].cpu().numpy()*256\n                \n        coords_list = [(\"TRUE\", \"lightblue\", cs_true), (\"PRED\", \"orange\", cs)]\n        text_labels = [str(x) for x in range(1,21)]\n        \n        # Plot coords\n        fig, axes = plt.subplots(1, len(coords_list), figsize=(10,4))\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        for ax, (title, color, coords) in zip(axes, coords_list):\n            ax.imshow(img, cmap='gray')\n            ax.scatter(coords[0::2], coords[1::2], c=color, s=50)\n            ax.axis('off')\n            ax.set_title(title)\n\n            # Add text labels near the coordinates\n            for i, (x, y) in enumerate(zip(coords[0::2], coords[1::2])):\n                if i < len(text_labels):  # Ensure there are enough labels\n                    ax.text(x + 10, y, text_labels[i], color='white', fontsize=15, bbox=dict(facecolor='black', alpha=0.5))\n\n\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        plt.show()\n#         plt.close(fig)\n    return\n\ndef load_weights_skip_mismatch(model, weights_path, device):\n    # Load Weights\n    state_dict = torch.load(weights_path, map_location=device)\n    model_dict = model.state_dict()\n    \n    # Iter models\n    params = {}\n    for (sdk, sfv), (mdk, mdv) in zip(state_dict.items(), model_dict.items()):\n        if sfv.size() == mdv.size():\n            params[sdk] = sfv\n        else:\n            print(\"Skipping param: {}, {} != {}\".format(sdk, sfv.size(), mdv.size()))\n    \n    # Reload + Skip\n    model.load_state_dict(params, strict=False)\n    print(\"Loaded weights from:\", weights_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.425005Z","iopub.execute_input":"2024-11-16T14:56:43.425312Z","iopub.status.idle":"2024-11-16T14:56:43.441854Z","shell.execute_reply.started":"2024-11-16T14:56:43.425280Z","shell.execute_reply":"2024-11-16T14:56:43.440921Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Create csv Dataset","metadata":{}},{"cell_type":"code","source":"train_df = load_training_dataframe(cfg)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.443047Z","iopub.execute_input":"2024-11-16T14:56:43.443405Z","iopub.status.idle":"2024-11-16T14:56:43.484228Z","shell.execute_reply.started":"2024-11-16T14:56:43.443364Z","shell.execute_reply":"2024-11-16T14:56:43.483336Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.485304Z","iopub.execute_input":"2024-11-16T14:56:43.485579Z","iopub.status.idle":"2024-11-16T14:56:43.498250Z","shell.execute_reply.started":"2024-11-16T14:56:43.485549Z","shell.execute_reply":"2024-11-16T14:56:43.497325Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   study_id  series_id_sg1  series_id_sg2  series_id_a2\n0  44036939     2828203845     3844393089    3481971518","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id_sg1</th>\n      <th>series_id_sg2</th>\n      <th>series_id_a2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>3844393089</td>\n      <td>3481971518</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Co-ordinate Prediction Dataset","metadata":{}},{"cell_type":"code","source":"class PreTrainDataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids,cfg,transform,isTrain = False,is_dataset_for_t1= False):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.is_dataset_for_t1 = is_dataset_for_t1\n\n    \n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n\n\n    def load_dicom_stack(self, dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n\n        # Determine the plane for sorting (sagittal, coronal, axial)\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n\n        # Sort DICOM files based on positions (reverse sort for axial plane if needed)\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n\n        # Get the shape of each pixel array (height, width)\n        shapes = [d.pixel_array.shape for d in dicoms]\n        # Check if all DICOM images have the same shape\n        if len(set(shapes)) > 1:\n            # There's a shape mismatch, find the minimum shape (height, width)\n            min_shape = np.min(shapes, axis=0)\n            # Resize images to the minimum shape\n            resized_arrays = []\n            for d in dicoms:\n                img = d.pixel_array.astype(\"float32\")\n                if img.shape != tuple(min_shape):\n                    resized_img = cv2.resize(img, (min_shape[1], min_shape[0]))  # Resize to (width, height)\n                else:\n                    resized_img = img  # No resizing needed\n                resized_arrays.append(resized_img)\n\n            # Stack the resized images along the first axis\n            array = np.stack(resized_arrays)\n        else:\n            # If all shapes are the same, no resizing is needed\n            array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n\n        # Reorder the array according to the sorted positions\n        array = array[idx]\n\n        return {\n            \"array\": self.convert_to_8bit(array),\n            \"positions\": ipp,\n            \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")\n        }\n    \n    def pad_image(self, img):\n        n= img.shape[0]\n        if n >= self.cfg.n_frames:\n            start_idx = (n - self.cfg.n_frames) // 2\n            return img[start_idx:start_idx + self.cfg.n_frames,:, :]\n        else:\n            pad_left = (self.cfg.n_frames - n) // 2\n            pad_right = self.cfg.n_frames - n - pad_left\n            return np.pad(img, ((pad_left, pad_right),(0,0), (0,0)), 'constant', constant_values=0)\n    \n    def load_img(self, series_id):\n        fname = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(series_id)), plane=\"sagittal\")\n        img= fname[\"array\"]\n        img= self.pad_image(img)\n        img= np.transpose(img, (1,2, 0))\n        img= self.transform(image=img)[\"image\"]\n        img= np.transpose(img, (2, 0, 1))\n        img= (img / 255.0)\n        return img\n        \n        \n    def __getitem__(self, idx):\n        d= self.study_ids.iloc[idx]\n        if self.is_dataset_for_t1:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg1)])      \n        else:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg2)])      \n                \n        img= self.load_img(series_id)\n        if self.isTrain:\n            return {\n                'img': img, \n                'label': label,\n            }\n        else:\n            return {\n                'img': img \n            }\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n    \n\nresize_transform_point= A.Compose([\nA.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\nA.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n])\n\n\n\n\nds = PreTrainDataset(train_df, cfg,resize_transform_point)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\nfor k, v in ds[0].items():\n    print(k, v.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:43.499540Z","iopub.execute_input":"2024-11-16T14:56:43.499917Z","iopub.status.idle":"2024-11-16T14:56:44.852267Z","shell.execute_reply.started":"2024-11-16T14:56:43.499855Z","shell.execute_reply":"2024-11-16T14:56:44.851281Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nimg (3, 256, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load backbone for RSNA 2024 task\nmodel_path = \"/kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\"\nmodel = timm.create_model('resnet18', pretrained=False, num_classes=20)\nmodel = model.to(cfg.device)\nload_weights_skip_mismatch(model, model_path, cfg.device)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:44.853637Z","iopub.execute_input":"2024-11-16T14:56:44.854143Z","iopub.status.idle":"2024-11-16T14:56:45.733033Z","shell.execute_reply.started":"2024-11-16T14:56:44.854096Z","shell.execute_reply":"2024-11-16T14:56:45.732035Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2699087807.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(weights_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded weights from: /kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Co-ordinate Inferencing","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:35:01.530636Z","iopub.execute_input":"2024-10-07T04:35:01.531106Z","iopub.status.idle":"2024-10-07T04:35:01.554979Z","shell.execute_reply.started":"2024-10-07T04:35:01.531037Z","shell.execute_reply":"2024-10-07T04:35:01.55382Z"}}},{"cell_type":"code","source":"def coordinate_prediction(model,pred_dataloader,isSagitalT1 = False):\n    predictions = []\n    with torch.no_grad():\n        model = model.eval()\n        for batch in tqdm(pred_dataloader):\n            batch = batch_to_device(batch, cfg.device)\n\n            pred = model(batch[\"img\"].float())\n            pred = torch.sigmoid(pred)\n            predictions.append(pred)\n            \n    data_list_cpu = [tensor.cpu().numpy() for tensor in predictions]\n    combined_data = np.vstack(data_list_cpu)\n    df_cords = pd.DataFrame(combined_data)\n    if isSagitalT1:\n        df_cords.to_csv(\"predicted_cordinates_sag_t1.csv\",index=False)\n    else:\n        df_cords.to_csv(\"predicted_cordinates_sag_t2.csv\",index=False)\n    return df_cords","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:45.734653Z","iopub.execute_input":"2024-11-16T14:56:45.736977Z","iopub.status.idle":"2024-11-16T14:56:45.745696Z","shell.execute_reply.started":"2024-11-16T14:56:45.736922Z","shell.execute_reply":"2024-11-16T14:56:45.744333Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Coordinate Prediction","metadata":{}},{"cell_type":"code","source":"train_ds = PreTrainDataset(train_df, cfg,resize_transform_point)\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:45.751283Z","iopub.execute_input":"2024-11-16T14:56:45.752182Z","iopub.status.idle":"2024-11-16T14:56:45.761038Z","shell.execute_reply.started":"2024-11-16T14:56:45.752127Z","shell.execute_reply":"2024-11-16T14:56:45.759744Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cordinates_t1 = coordinate_prediction(model,train_dl,isSagitalT1 = True)\ncordinates_t2 = coordinate_prediction(model,train_dl,isSagitalT1 = False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:45.762361Z","iopub.execute_input":"2024-11-16T14:56:45.762777Z","iopub.status.idle":"2024-11-16T14:56:48.682709Z","shell.execute_reply.started":"2024-11-16T14:56:45.762736Z","shell.execute_reply":"2024-11-16T14:56:48.681775Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"cordinates_t1['study_id'] = train_df['study_id'].values\ncordinates_t2['study_id'] = train_df['study_id'].values","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.683935Z","iopub.execute_input":"2024-11-16T14:56:48.684251Z","iopub.status.idle":"2024-11-16T14:56:48.690780Z","shell.execute_reply.started":"2024-11-16T14:56:48.684218Z","shell.execute_reply":"2024-11-16T14:56:48.689571Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"cordinates_t1","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.691774Z","iopub.execute_input":"2024-11-16T14:56:48.692093Z","iopub.status.idle":"2024-11-16T14:56:48.718439Z","shell.execute_reply.started":"2024-11-16T14:56:48.692059Z","shell.execute_reply":"2024-11-16T14:56:48.717431Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         0        1         2        3         4         5         6  \\\n0  0.40336  0.31897  0.624557  0.36428  0.359472  0.430439  0.565006   \n\n          7         8         9  ...        11        12        13        14  \\\n0  0.454914  0.314881  0.540189  ...  0.563507  0.322633  0.682666  0.545385   \n\n         15        16        17        18        19  study_id  \n0  0.653523  0.385318  0.841513  0.562395  0.713457  44036939  \n\n[1 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>study_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.40336</td>\n      <td>0.31897</td>\n      <td>0.624557</td>\n      <td>0.36428</td>\n      <td>0.359472</td>\n      <td>0.430439</td>\n      <td>0.565006</td>\n      <td>0.454914</td>\n      <td>0.314881</td>\n      <td>0.540189</td>\n      <td>...</td>\n      <td>0.563507</td>\n      <td>0.322633</td>\n      <td>0.682666</td>\n      <td>0.545385</td>\n      <td>0.653523</td>\n      <td>0.385318</td>\n      <td>0.841513</td>\n      <td>0.562395</td>\n      <td>0.713457</td>\n      <td>44036939</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Utils for LSDD ","metadata":{}},{"cell_type":"code","source":"def get_padded_roi(orientation, numberOfImageFromCenter):\n    # Calculate middle index\n    middleImage = len(orientation[\"array\"]) // 2\n\n    # Calculate start and end indices\n    start_idx = middleImage - numberOfImageFromCenter\n    end_idx = middleImage + numberOfImageFromCenter + 1\n\n    # Handle bounds\n    array_length = orientation[\"array\"].shape[0]  # Slicing along the first axis (number of images)\n\n    # Ensure we don't go beyond the array's bounds\n    start_pad = max(0, -start_idx)\n    end_pad = max(0, end_idx - array_length)\n\n    # Slice the valid part of the array along the first axis\n    roi = orientation[\"array\"][max(0, start_idx):min(array_length, end_idx)]\n\n    # Pad with zeros if needed\n    if start_pad > 0 or end_pad > 0:\n        roi = np.pad(roi, ((start_pad, end_pad), (0, 0), (0, 0)), mode='constant', constant_values=0)\n\n    return roi","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.720759Z","iopub.execute_input":"2024-11-16T14:56:48.721414Z","iopub.status.idle":"2024-11-16T14:56:48.731827Z","shell.execute_reply.started":"2024-11-16T14:56:48.721364Z","shell.execute_reply":"2024-11-16T14:56:48.730915Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def prepare_level_wise_axial(sagittal_img, imgsag_y_coord_to_axial_slice, coordinates,no_of_axial_slice = 3):\n    h, w = sagittal_img.shape\n    axial_list = []\n    \n    first_key = next(iter(imgsag_y_coord_to_axial_slice))\n    # Get the first value\n    first_value = imgsag_y_coord_to_axial_slice[first_key]\n\n    keys = list(imgsag_y_coord_to_axial_slice.keys())\n    keys.sort()\n\n    for i in range(0, len(coordinates), 4):\n        #print(i)\n        category = coordinates[i:i+4]  # Extracting 4 elements at a time\n        y= [category[1]*h,category[3]*h]\n        minimum = math.floor(min(y[0],y[1]))\n        maximum  = math.ceil(max(y[0],y[1]))\n        filtered_keys = [k for k in imgsag_y_coord_to_axial_slice.keys() if minimum <= k <= maximum]\n        \n        if len(filtered_keys) >= no_of_axial_slice:\n            # Use numpy to select 3 keys at uniform intervals\n            selected_keys = np.linspace(0, len(filtered_keys)-1,no_of_axial_slice , dtype=int)\n            selected_keys = [filtered_keys[i] for i in selected_keys]\n        else:\n            if len(filtered_keys) == 0:\n                index = bisect.bisect_left(keys, minimum)\n                # Check if we can find a nearest value less than current_value\n                if index > 0:\n                    filtered_keys.append(keys[index - 1])\n\n                index = bisect.bisect_right(keys, maximum)\n                if index < len(keys):\n                    filtered_keys.append(keys[index])  # Return the nearest value greater than current_value\n                    \n\n            filtered_keys.extend([filtered_keys[-1]] * (no_of_axial_slice - len(filtered_keys)))\n            selected_keys = filtered_keys\n        \n        if len(selected_keys) ==  0:\n            result  = np.zeros((no_of_axial_slice, first_value.shape[0],first_value.shape[1]))\n        else:\n            selected_axial =[imgsag_y_coord_to_axial_slice.get(k) for k in selected_keys]\n            result  = np.array(selected_axial)\n\n        roi_copy_list  = []\n        for j in range(result.shape[0]):\n            roi_copy_list.append(resize_transform(image=result[j])[\"image\"])\n            \n        axial_list.append(np.array(roi_copy_list))\n        \n    return  np.array(axial_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.733369Z","iopub.execute_input":"2024-11-16T14:56:48.733645Z","iopub.status.idle":"2024-11-16T14:56:48.748784Z","shell.execute_reply.started":"2024-11-16T14:56:48.733613Z","shell.execute_reply":"2024-11-16T14:56:48.747675Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"resize_transform= A.Compose([\n    A.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\n    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n    A.Normalize()\n])\n\ndef angle_of_line(x1, y1, x2, y2):\n    return math.degrees(math.atan2(-(y2-y1), x2-x1))\n\ndef crop_between_keypoints(roi,img, keypoint1, keypoint2):\n    h, w = img.shape\n    x1, y1 = int(keypoint1[0]), int(keypoint1[1])\n    x2, y2 = int(keypoint2[0]), int(keypoint2[1])\n\n    # Calculate bounding box around the keypoints\n    left = int(min(x1, x2) - (w * 0.1))\n    right = int(max(x1, x2) + (w * 0.1))\n    top = int(min(y1, y2) - (h * 0.07))\n    bottom = int(max(y1, y2) + (h * 0.1))\n    \n    left = max(0, left)\n    right = min(w, right)\n    top = max(0, top)\n    bottom = min(h, bottom)\n    # Crop the image\n    return img[top:bottom, left:right],roi[:,top:bottom, left:right]\n\ndef plot_5_crops(orientataion,coords_temp,numberOfImageFromCenter = 3):\n    # Create a figure and axis for the grid\n    #fig = plt.figure(figsize=(10, 10))\n    #gs = gridspec.GridSpec(1, 5, width_ratios=[1]*5)\n    \n    #print(coords_temp)\n    # Plot the crops\n    #print(\"plot-5-crop-img\")\n    orientataion['array'] = get_padded_roi(orientataion,numberOfImageFromCenter)\n    middleImage = len(orientataion[\"array\"])//2\n    img = orientataion[\"array\"][middleImage]\n    \n    roi = orientataion[\"array\"][middleImage-numberOfImageFromCenter:middleImage+numberOfImageFromCenter+1]\n    croppedImage = []\n    #print(p)\n    for i in range(0, len(coords_temp), 4):\n        # Copy of img\n        img_copy= img.copy()\n        h, w = img.shape\n        roi_copy = roi.copy()\n        # Extract Keypoints\n        category = coords_temp[i:i+4]  # Extracting 4 elements at a time\n        a= (category[0]*w, category[1]*h)\n        b= (category[2]*w, category[3]*h)\n        \n        # Rotate\n        rotate_angle= angle_of_line(a[0], a[1], b[0], b[1])\n        transform = A.Compose([\n            A.Rotate(limit=(-rotate_angle, -rotate_angle), p=1.0),\n        ], keypoint_params= A.KeypointParams(format='xy', remove_invisible=False),\n        )\n\n        t= transform(image=img_copy, keypoints=[a,b])\n        img_copy= t[\"image\"]\n        a,b= t[\"keypoints\"]\n        #print(img_copy.shape)\n        # Crop + Resize\n        img_copy,roi_copy = crop_between_keypoints(roi_copy,img_copy, a, b)\n        #print(roi_copy.shape)\n        #print(img_copy.shape)\n        roi_copy_list  = []\n        for j in range (0,numberOfImageFromCenter*2+1):\n            roi_copy_list.append(resize_transform(image=roi_copy[j])[\"image\"])\n        roi_copy_list = np.array(roi_copy_list)\n        \n        img_copy = roi_copy_list[numberOfImageFromCenter]\n        croppedImage.append(roi_copy_list)\n        # Plot\n        #ax = plt.subplot(gs[i//4])\n        #ax.imshow(img_copy, cmap='gray')\n        #ax.set_title(f\"L{i//4+1}\")\n        #ax.axis('off')\n    #plt.show()\n    return np.array(croppedImage)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.750365Z","iopub.execute_input":"2024-11-16T14:56:48.750947Z","iopub.status.idle":"2024-11-16T14:56:48.770274Z","shell.execute_reply.started":"2024-11-16T14:56:48.750836Z","shell.execute_reply":"2024-11-16T14:56:48.769294Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Define DataSet for training","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# # Sample DataFrame\n# df = pd.DataFrame({\n#     'A': [0, 1, 2],\n#     'B': [1, 0, 2],\n#     'C': [2, 2, 1],\n#     'D': [0, 1, 0],\n#     'E': [1, 0, 1]\n#     # add 25 columns in actual case\n# })\n\n# # One-hot encode each column in the DataFrame\n# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# # Apply the one-hot encoding function to each row\n# encoded_data = np.array([one_hot_encode_row(row) for row in df.values])\n\n# # Example: to check one row's shape (25, 3)\n# print(encoded_data[0].shape)  # Should output (5, 3) for this example, adjust to 25 columns\n\n# # Example: print the one-hot encoded array for a row\n# print(encoded_data[0])  # One-hot encoded first row\n\n# # Final shape of the encoded data for all rows\n# print(encoded_data.shape)  # (number_of_rows, 25, 3)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.771570Z","iopub.execute_input":"2024-11-16T14:56:48.772099Z","iopub.status.idle":"2024-11-16T14:56:48.782415Z","shell.execute_reply.started":"2024-11-16T14:56:48.772055Z","shell.execute_reply":"2024-11-16T14:56:48.781493Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# s = one_hot_encode_row([0,1,2,0,0])\n# # Apply the one-hot encoding function to each row\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.783645Z","iopub.execute_input":"2024-11-16T14:56:48.784062Z","iopub.status.idle":"2024-11-16T14:56:48.791654Z","shell.execute_reply.started":"2024-11-16T14:56:48.784020Z","shell.execute_reply":"2024-11-16T14:56:48.790725Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class RSNADataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids, s1_coords, s2_coords,cfg,transform,isTrain = True,transform_axial = None,transform_sag = None):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.transform_sag = transform_sag\n        self.transform_axial = transform_axial\n        self.s1_coords = self.align_cord(study_ids,s1_coords)\n        self.s2_coords = self.align_cord(study_ids,s2_coords)\n        self.labeldf = study_ids[[col for col in study_ids.columns if col not in ['study_id','series_id_sg1','series_id_sg2','series_id_a2']]]\n\n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n    \n    def align_cord(self,study_ids,cords):\n        std_id = study_ids[['study_id']] \n        merged = pd.merge(std_id,cords,on=['study_id'],how=\"inner\")\n        return merged[[col for col in merged.columns if col not in ['study_id']]]\n\n    def load_dicom_stack(self, dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n\n        # Determine the plane for sorting (sagittal, coronal, axial)\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n\n        # Sort DICOM files based on positions (reverse sort for axial plane if needed)\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n\n        # Get the shape of each pixel array (height, width)\n        shapes = [d.pixel_array.shape for d in dicoms]\n        # Check if all DICOM images have the same shape\n        if len(set(shapes)) > 1:\n            # There's a shape mismatch, find the minimum shape (height, width)\n            min_shape = np.min(shapes, axis=0)\n            # Resize images to the minimum shape\n            resized_arrays = []\n            for d in dicoms:\n                img = d.pixel_array.astype(\"float32\")\n                if img.shape != tuple(min_shape):\n                    resized_img = cv2.resize(img, (min_shape[1], min_shape[0]))  # Resize to (width, height)\n                else:\n                    resized_img = img  # No resizing needed\n                resized_arrays.append(resized_img)\n\n            # Stack the resized images along the first axis\n            array = np.stack(resized_arrays)\n        else:\n            # If all shapes are the same, no resizing is needed\n            array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n\n        # Reorder the array according to the sorted positions\n        array = array[idx]\n\n        return {\n            \"array\": self.convert_to_8bit(array),\n            \"positions\": ipp,\n            \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")\n        }\n     \n    def one_hot_encode_row(self, row):\n        return np.array([np.eye(3)[int(val)] for val in row]) \n    def __getitem__(self, idx):\n        row = self.study_ids.iloc[idx]\n        \n        sag_t1 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg1)), plane=\"sagittal\")\n        ax_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_a2)), plane=\"axial\", reverse_sort=True)\n        sag_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg2)), plane=\"sagittal\")\n        \n        top_left_hand_corner_sag_t2 = sag_t2[\"positions\"][len(sag_t2[\"array\"]) // 2]\n        sag_y_axis_to_pixel_space = [top_left_hand_corner_sag_t2[2]]\n        while len(sag_y_axis_to_pixel_space) < sag_t2[\"array\"].shape[1]: \n            sag_y_axis_to_pixel_space.append(sag_y_axis_to_pixel_space[-1] - sag_t2[\"pixel_spacing\"][1])\n        \n        sag_y_coord_to_axial_slice = {}\n        for ax_t2_slice, ax_t2_pos in zip(ax_t2[\"array\"], ax_t2[\"positions\"]):\n            diffs = np.abs(np.asarray(sag_y_axis_to_pixel_space) - ax_t2_pos[2])\n            sag_y_coord = np.argmin(diffs)\n            sag_y_coord_to_axial_slice[sag_y_coord] = ax_t2_slice\n        \n        sag1_cord = self.s1_coords.iloc[idx].tolist()\n        sag2_cord = self.s2_coords.iloc[idx].tolist()\n        \n        #print(\"s1 cord\",sag1_cord)\n        #print(\"s2 cord\",sag2_cord)\n        img= sag_t2[\"array\"][len(sag_t2[\"array\"])//2]\n        corresponding_axial = self.transform_axial(img, sag_y_coord_to_axial_slice, sag2_cord,no_of_axial_slice = self.cfg.sag_axial_slices)\n        #print(corresponding_axial.shape)\n\n        \n        crop_result_t2 = self.transform_sag(sag_t2, sag2_cord,numberOfImageFromCenter = self.cfg.sag_2_slices)\n        #print(crop_result_t2.shape)\n        \n        crop_result_t1 = self.transform_sag(sag_t1, sag1_cord,numberOfImageFromCenter = self.cfg.sag_1_slices)\n        #print(crop_result_t1.shape)\n        \n        \n        \n        if self.isTrain:\n            label  = self.one_hot_encode_row( self.labeldf.iloc[idx])\n            return (crop_result_t2,crop_result_t1,corresponding_axial),label\n        else:\n            return (crop_result_t2,crop_result_t1, corresponding_axial)\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n\nds = RSNADataset(train_df,cordinates_t1,cordinates_t2 ,cfg,resize_transform,isTrain= False,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\n(k1,k2,k3)  =  ds[0]\nprint(\"k1 shape\",k1.shape)\nprint(\"k1 shape\",k2.shape)\nprint(\"k3 shape\",k3.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:48.794848Z","iopub.execute_input":"2024-11-16T14:56:48.795206Z","iopub.status.idle":"2024-11-16T14:56:51.429475Z","shell.execute_reply.started":"2024-11-16T14:56:48.795174Z","shell.execute_reply":"2024-11-16T14:56:51.428514Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nk1 shape (5, 3, 256, 256)\nk1 shape (5, 9, 256, 256)\nk3 shape (5, 3, 256, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.430519Z","iopub.execute_input":"2024-11-16T14:56:51.430808Z","iopub.status.idle":"2024-11-16T14:56:51.435033Z","shell.execute_reply.started":"2024-11-16T14:56:51.430777Z","shell.execute_reply":"2024-11-16T14:56:51.434170Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#  model1 = models.video.r3d_18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.436245Z","iopub.execute_input":"2024-11-16T14:56:51.436519Z","iopub.status.idle":"2024-11-16T14:56:51.452535Z","shell.execute_reply.started":"2024-11-16T14:56:51.436488Z","shell.execute_reply":"2024-11-16T14:56:51.451775Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# model1.stem[0].weight","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.453629Z","iopub.execute_input":"2024-11-16T14:56:51.453923Z","iopub.status.idle":"2024-11-16T14:56:51.462563Z","shell.execute_reply.started":"2024-11-16T14:56:51.453892Z","shell.execute_reply":"2024-11-16T14:56:51.461670Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# print(model1)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.463750Z","iopub.execute_input":"2024-11-16T14:56:51.464203Z","iopub.status.idle":"2024-11-16T14:56:51.473263Z","shell.execute_reply.started":"2024-11-16T14:56:51.464162Z","shell.execute_reply":"2024-11-16T14:56:51.472362Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n\n        # Define three 3D ResNet-18 models\n        self.model1 = models.video.r3d_18(pretrained=False)  # First model (input shape: 5,3,256,256)\n        self.model2 = models.video.r3d_18(pretrained=False)  # Second model (input shape: 5,3,256,256)\n        self.model3 = models.video.r3d_18(pretrained=False)  # Third model (input shape: 5,9,256,256)\n        \n        original_conv1 = self.model1.stem[0]\n        self.model1.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model2.stem[0] = nn.Conv3d(5, 64, kernel_size=(9, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model3.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n\n        \n        with torch.no_grad():\n            self.model1.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model1.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            self.model2.stem[0].weight[:, :3, :3, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, 3:, :3, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            #self.model2.stem[0].weight[:, :, :3, :, :] = self.model2.stem[0].weight[:, :, :, :, :]  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, :, 3:6, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            self.model2.stem[0].weight[:, :, 6:9, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            \n            \n            self.model3.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model3.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n        \n        self.model1.fc = nn.Identity()\n        self.model2.fc = nn.Identity()\n        self.model3.fc = nn.Identity()\n        \n        # Extract the feature size from the output of the models\n        self.hidden_size = 32  # Adjust this if necessary, based on the model's output size\n        self.flatten_size = 512\n        # Concatenation layer\n        self.fc = nn.Linear(self.flatten_size * 3, self.hidden_size)   # Final linear layer after concatenation\n\n        # Subclass outputs (25 classes, each has 3 subclasses)\n        self.subclass_layers = nn.ModuleList([nn.Linear(self.hidden_size, 3) for _ in range(25)])\n\n    def forward(self, x1, x2, x3):\n        # Forward pass through each model\n        output1 = self.model1(x1)  # Shape: [batch_size, hidden_size]\n        output2 = self.model2(x2)  # Shape: [batch_size, hidden_size]\n        output3 = self.model3(x3)  # Shape: [batch_size, hidden_size]\n        \n        flatten1 = torch.flatten(output1, 1)\n        flatten2 = torch.flatten(output2, 1)\n        flatten3 = torch.flatten(output3, 1)\n        #print(flatten1.shape)\n        #print(flatten2.shape)\n       # print(flatten3.shape)\n        # Concatenate outputs from the three models\n        concatenated_output = torch.cat((flatten1, flatten2,flatten2), dim=1)  # Shape: [batch_size, hidden_size * 3]\n       #print(concatenated_output.shape)\n        # Pass concatenated output through the final linear layer\n        combined_output = self.fc(concatenated_output)  # Shape: [batch_size, hidden_size]\n\n        # Subclass prediction for each class\n        subclass_outputs = [torch.softmax(layer(combined_output), dim=1) for layer in self.subclass_layers]\n\n        return subclass_outputs\n\n# Example usage\n#if __name__ == \"__main__\":\n  ","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.474533Z","iopub.execute_input":"2024-11-16T14:56:51.474838Z","iopub.status.idle":"2024-11-16T14:56:51.494298Z","shell.execute_reply.started":"2024-11-16T14:56:51.474801Z","shell.execute_reply":"2024-11-16T14:56:51.493369Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# model = CustomModel()\n# model = model.to(cfg.device)\n\n# # Example input tensors\n# input1 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the first model\n# input2 = torch.randn(8, 5, 9, 256, 256)  # Batch size 8 for the second model\n# input3 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the third model\n\n# outputs = model(input1.to(cfg.device), input2.to(cfg.device), input3.to(cfg.device))\n# for i, output in enumerate(outputs):\n#     print(f\"Subclass Output for Class {i + 1}: Shape {output.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.495351Z","iopub.execute_input":"2024-11-16T14:56:51.495620Z","iopub.status.idle":"2024-11-16T14:56:51.508685Z","shell.execute_reply.started":"2024-11-16T14:56:51.495589Z","shell.execute_reply":"2024-11-16T14:56:51.507763Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n\n# def prepare_target(raw_labels):\n#     \"\"\"\n#     Convert raw labels into one-hot encoded labels for 25 classes.\n    \n#     Args:\n#         raw_labels (torch.Tensor): Tensor of shape [batch_size, 25] where each value is the subclass label (0, 1, or 2).\n        \n#     Returns:\n#         torch.Tensor: One-hot encoded labels of shape [batch_size, 25, 3].\n#     \"\"\"\n#     batch_size = raw_labels.shape[0]\n#     num_classes = raw_labels.shape[1]\n#     num_subclasses = 3  # There are 3 subclasses\n\n#     # One-hot encode the subclass labels\n#     one_hot_labels = F.one_hot(raw_labels, num_classes=num_subclasses)  # Shape: [batch_size, 25, 3]\n    \n#     return one_hot_labels.float()  # Return as float for compatibility with loss functions\n\n# # Example usage:\n# batch_size = 8\n# num_classes = 25\n\n# # Randomly generated raw labels where each value is 0, 1, or 2\n# raw_labels = torch.randint(0, 3, (batch_size, num_classes))  # Shape: [batch_size, 25]\n\n# # Prepare one-hot encoded targets\n# targets = prepare_target(raw_labels)\n\n# print(targets.shape)  # Should print: torch.Size([8, 25, 3])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.509696Z","iopub.execute_input":"2024-11-16T14:56:51.510001Z","iopub.status.idle":"2024-11-16T14:56:51.522852Z","shell.execute_reply.started":"2024-11-16T14:56:51.509971Z","shell.execute_reply":"2024-11-16T14:56:51.522113Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Training And Validation Functions","metadata":{}},{"cell_type":"code","source":"def val_epoch(model, loader):\n\n    model.eval()\n    predictions = []\n    bar = tqdm(loader)\n    with torch.no_grad():\n        for data in bar:\n            \n            sagittal_t2,sagittal_t1,axial_t2, = data\n            axial_t2, sagittal_t2, sagittal_t1 = axial_t2.to(device), sagittal_t2.to(device), sagittal_t1.to(device)\n           \n            logits = model(sagittal_t2,sagittal_t1,axial_t2)\n             \n            loss = 0\n            batch_prediction = []\n            for i in range(25):\n                # Select the i-th subclass prediction and corresponding target\n                subclass_pred = logits[i]  # Output from model, shape: [batch_size, 3]\n                #print(subclass_pred.detach().cpu().numpy().shape)\n                batch_prediction.append(subclass_pred.detach().cpu().numpy())\n                #print(subclass_pred)\n                # Compute the loss for the i-th subclass\n\n            predictions.append(np.array(batch_prediction))\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.523876Z","iopub.execute_input":"2024-11-16T14:56:51.524159Z","iopub.status.idle":"2024-11-16T14:56:51.535079Z","shell.execute_reply.started":"2024-11-16T14:56:51.524129Z","shell.execute_reply":"2024-11-16T14:56:51.534238Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"dataset_train = RSNADataset(train_df,cordinates_t1,cordinates_t2,cfg,resize_transform, isTrain= False,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)\nprediction_dl = torch.utils.data.DataLoader(dataset_train, batch_size=cfg.batch_size, sampler=RandomSampler(dataset_train), num_workers=cfg.num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.539939Z","iopub.execute_input":"2024-11-16T14:56:51.540233Z","iopub.status.idle":"2024-11-16T14:56:51.557745Z","shell.execute_reply.started":"2024-11-16T14:56:51.540203Z","shell.execute_reply":"2024-11-16T14:56:51.556713Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = cfg.CUDA_VISIBLE_DEVICES\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.558818Z","iopub.execute_input":"2024-11-16T14:56:51.559164Z","iopub.status.idle":"2024-11-16T14:56:51.564964Z","shell.execute_reply.started":"2024-11-16T14:56:51.559132Z","shell.execute_reply":"2024-11-16T14:56:51.564088Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":" cfg.model_dir_lsdd","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.565997Z","iopub.execute_input":"2024-11-16T14:56:51.566272Z","iopub.status.idle":"2024-11-16T14:56:51.576736Z","shell.execute_reply.started":"2024-11-16T14:56:51.566242Z","shell.execute_reply":"2024-11-16T14:56:51.575754Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/training-lsdd/resnet18_final_fold0.pth'"},"metadata":{}}]},{"cell_type":"code","source":"model = CustomModel()\nmodel = model.to(device)\n\n# Load the model weights (assuming 'model_weights.pth' is the saved file)\nmodel_weights_path = \"/kaggle/input/training-lsdd/resnet18_final_fold0.pth\"\nmodel.load_state_dict(torch.load(model_weights_path,map_location=device))\n\n# Set the model to evaluation mode (if you are using it for inference)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:51.577830Z","iopub.execute_input":"2024-11-16T14:56:51.578152Z","iopub.status.idle":"2024-11-16T14:56:55.630561Z","shell.execute_reply.started":"2024-11-16T14:56:51.578105Z","shell.execute_reply":"2024-11-16T14:56:55.629634Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/tmp/ipykernel_31/1537351406.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_weights_path,map_location=device))\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"CustomModel(\n  (model1): VideoResNet(\n    (stem): BasicStem(\n      (0): Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n    (fc): Identity()\n  )\n  (model2): VideoResNet(\n    (stem): BasicStem(\n      (0): Conv3d(5, 64, kernel_size=(9, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n    (fc): Identity()\n  )\n  (model3): VideoResNet(\n    (stem): BasicStem(\n      (0): Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n    (fc): Identity()\n  )\n  (fc): Linear(in_features=1536, out_features=32, bias=True)\n  (subclass_layers): ModuleList(\n    (0-24): 25 x Linear(in_features=32, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"predictions = val_epoch(model,prediction_dl)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:55.631843Z","iopub.execute_input":"2024-11-16T14:56:55.632247Z","iopub.status.idle":"2024-11-16T14:56:59.738058Z","shell.execute_reply.started":"2024-11-16T14:56:55.632202Z","shell.execute_reply":"2024-11-16T14:56:59.737047Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:04<00:00,  4.09s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.739612Z","iopub.execute_input":"2024-11-16T14:56:59.739970Z","iopub.status.idle":"2024-11-16T14:56:59.746489Z","shell.execute_reply.started":"2024-11-16T14:56:59.739931Z","shell.execute_reply":"2024-11-16T14:56:59.745489Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"predictions_arr = np.concatenate(predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.748011Z","iopub.execute_input":"2024-11-16T14:56:59.748400Z","iopub.status.idle":"2024-11-16T14:56:59.756670Z","shell.execute_reply.started":"2024-11-16T14:56:59.748353Z","shell.execute_reply":"2024-11-16T14:56:59.755765Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"predictions_arr.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.757890Z","iopub.execute_input":"2024-11-16T14:56:59.758195Z","iopub.status.idle":"2024-11-16T14:56:59.770487Z","shell.execute_reply.started":"2024-11-16T14:56:59.758164Z","shell.execute_reply":"2024-11-16T14:56:59.769615Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(25, 1, 3)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class names (for example)\nclass_names = [\n 'spinal_canal_stenosis_l1_l2',\n 'spinal_canal_stenosis_l2_l3',\n 'spinal_canal_stenosis_l3_l4',\n 'spinal_canal_stenosis_l4_l5',\n 'spinal_canal_stenosis_l5_s1',\n 'left_neural_foraminal_narrowing_l1_l2',\n 'left_neural_foraminal_narrowing_l2_l3',\n 'left_neural_foraminal_narrowing_l3_l4',\n 'left_neural_foraminal_narrowing_l4_l5',\n 'left_neural_foraminal_narrowing_l5_s1',\n 'right_neural_foraminal_narrowing_l1_l2',\n 'right_neural_foraminal_narrowing_l2_l3',\n 'right_neural_foraminal_narrowing_l3_l4',\n 'right_neural_foraminal_narrowing_l4_l5',\n 'right_neural_foraminal_narrowing_l5_s1',\n 'left_subarticular_stenosis_l1_l2',\n 'left_subarticular_stenosis_l2_l3',\n 'left_subarticular_stenosis_l3_l4',\n 'left_subarticular_stenosis_l4_l5',\n 'left_subarticular_stenosis_l5_s1',\n 'right_subarticular_stenosis_l1_l2',\n 'right_subarticular_stenosis_l2_l3',\n 'right_subarticular_stenosis_l3_l4',\n 'right_subarticular_stenosis_l4_l5',\n 'right_subarticular_stenosis_l5_s1']\n\n# Prepare a list to hold the data for the DataFrame\ndata = []\n\n# Populate the data list with class names and subclass predictions\nfor class_idx, class_name in enumerate(class_names):\n    for example_idx in range(predictions_arr.shape[1]):  # Loop through examples\n        # Get subclass predictions for this class and example\n        subclass_predictions = predictions_arr[class_idx, example_idx, :].tolist()\n        # Append to the data list\n        data.append([f\"{train_df.iloc[example_idx].study_id}_{class_name}\"] + subclass_predictions)\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=['row_id', 'normal_mild', 'moderate', 'severe'])\n\n# Display the DataFrame\ndf_sorted = df.sort_values(by='row_id')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.771634Z","iopub.execute_input":"2024-11-16T14:56:59.771975Z","iopub.status.idle":"2024-11-16T14:56:59.786607Z","shell.execute_reply.started":"2024-11-16T14:56:59.771942Z","shell.execute_reply":"2024-11-16T14:56:59.785809Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_sorted['normal_mild'] = df_sorted['normal_mild'].round(7)\ndf_sorted['moderate'] = df_sorted['moderate'].round(7)\ndf_sorted['severe'] = df_sorted['severe'].round(7)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.787612Z","iopub.execute_input":"2024-11-16T14:56:59.787921Z","iopub.status.idle":"2024-11-16T14:56:59.801421Z","shell.execute_reply.started":"2024-11-16T14:56:59.787849Z","shell.execute_reply":"2024-11-16T14:56:59.800603Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df_sorted.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.802407Z","iopub.execute_input":"2024-11-16T14:56:59.802698Z","iopub.status.idle":"2024-11-16T14:56:59.821022Z","shell.execute_reply.started":"2024-11-16T14:56:59.802667Z","shell.execute_reply":"2024-11-16T14:56:59.820112Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                        row_id  normal_mild      moderate  \\\n20  44036939_right_subarticular_stenosis_l1_l2     1.000000  0.000000e+00   \n21  44036939_right_subarticular_stenosis_l2_l3     1.000000  0.000000e+00   \n22  44036939_right_subarticular_stenosis_l3_l4     1.000000  0.000000e+00   \n23  44036939_right_subarticular_stenosis_l4_l5     1.000000  2.000000e-07   \n24  44036939_right_subarticular_stenosis_l5_s1     0.999996  3.700000e-06   \n0         44036939_spinal_canal_stenosis_l1_l2     1.000000  1.000000e-07   \n1         44036939_spinal_canal_stenosis_l2_l3     1.000000  0.000000e+00   \n2         44036939_spinal_canal_stenosis_l3_l4     1.000000  0.000000e+00   \n3         44036939_spinal_canal_stenosis_l4_l5     1.000000  0.000000e+00   \n4         44036939_spinal_canal_stenosis_l5_s1     1.000000  0.000000e+00   \n\n          severe  \n20  0.000000e+00  \n21  1.000000e-07  \n22  4.000000e-07  \n23  0.000000e+00  \n24  0.000000e+00  \n0   2.000000e-07  \n1   0.000000e+00  \n2   0.000000e+00  \n3   0.000000e+00  \n4   0.000000e+00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>normal_mild</th>\n      <th>moderate</th>\n      <th>severe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20</th>\n      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e-07</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>4.000000e-07</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n      <td>1.000000</td>\n      <td>2.000000e-07</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n      <td>0.999996</td>\n      <td>3.700000e-06</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>44036939_spinal_canal_stenosis_l1_l2</td>\n      <td>1.000000</td>\n      <td>1.000000e-07</td>\n      <td>2.000000e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44036939_spinal_canal_stenosis_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44036939_spinal_canal_stenosis_l3_l4</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44036939_spinal_canal_stenosis_l4_l5</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>44036939_spinal_canal_stenosis_l5_s1</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_sorted.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.822201Z","iopub.execute_input":"2024-11-16T14:56:59.822489Z","iopub.status.idle":"2024-11-16T14:56:59.830648Z","shell.execute_reply.started":"2024-11-16T14:56:59.822458Z","shell.execute_reply":"2024-11-16T14:56:59.829925Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df_sorted","metadata":{"execution":{"iopub.status.busy":"2024-11-16T14:56:59.831602Z","iopub.execute_input":"2024-11-16T14:56:59.831936Z","iopub.status.idle":"2024-11-16T14:56:59.852768Z","shell.execute_reply.started":"2024-11-16T14:56:59.831898Z","shell.execute_reply":"2024-11-16T14:56:59.851916Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                                             row_id  normal_mild  \\\n5    44036939_left_neural_foraminal_narrowing_l1_l2     1.000000   \n6    44036939_left_neural_foraminal_narrowing_l2_l3     1.000000   \n7    44036939_left_neural_foraminal_narrowing_l3_l4     1.000000   \n8    44036939_left_neural_foraminal_narrowing_l4_l5     1.000000   \n9    44036939_left_neural_foraminal_narrowing_l5_s1     0.999969   \n15        44036939_left_subarticular_stenosis_l1_l2     1.000000   \n16        44036939_left_subarticular_stenosis_l2_l3     1.000000   \n17        44036939_left_subarticular_stenosis_l3_l4     0.999999   \n18        44036939_left_subarticular_stenosis_l4_l5     1.000000   \n19        44036939_left_subarticular_stenosis_l5_s1     1.000000   \n10  44036939_right_neural_foraminal_narrowing_l1_l2     1.000000   \n11  44036939_right_neural_foraminal_narrowing_l2_l3     1.000000   \n12  44036939_right_neural_foraminal_narrowing_l3_l4     1.000000   \n13  44036939_right_neural_foraminal_narrowing_l4_l5     1.000000   \n14  44036939_right_neural_foraminal_narrowing_l5_s1     1.000000   \n20       44036939_right_subarticular_stenosis_l1_l2     1.000000   \n21       44036939_right_subarticular_stenosis_l2_l3     1.000000   \n22       44036939_right_subarticular_stenosis_l3_l4     1.000000   \n23       44036939_right_subarticular_stenosis_l4_l5     1.000000   \n24       44036939_right_subarticular_stenosis_l5_s1     0.999996   \n0              44036939_spinal_canal_stenosis_l1_l2     1.000000   \n1              44036939_spinal_canal_stenosis_l2_l3     1.000000   \n2              44036939_spinal_canal_stenosis_l3_l4     1.000000   \n3              44036939_spinal_canal_stenosis_l4_l5     1.000000   \n4              44036939_spinal_canal_stenosis_l5_s1     1.000000   \n\n        moderate        severe  \n5   2.000000e-07  0.000000e+00  \n6   0.000000e+00  0.000000e+00  \n7   2.000000e-07  0.000000e+00  \n8   0.000000e+00  0.000000e+00  \n9   3.140000e-05  0.000000e+00  \n15  0.000000e+00  0.000000e+00  \n16  0.000000e+00  0.000000e+00  \n17  1.300000e-06  0.000000e+00  \n18  0.000000e+00  0.000000e+00  \n19  0.000000e+00  0.000000e+00  \n10  0.000000e+00  0.000000e+00  \n11  0.000000e+00  0.000000e+00  \n12  0.000000e+00  0.000000e+00  \n13  0.000000e+00  0.000000e+00  \n14  0.000000e+00  0.000000e+00  \n20  0.000000e+00  0.000000e+00  \n21  0.000000e+00  1.000000e-07  \n22  0.000000e+00  4.000000e-07  \n23  2.000000e-07  0.000000e+00  \n24  3.700000e-06  0.000000e+00  \n0   1.000000e-07  2.000000e-07  \n1   0.000000e+00  0.000000e+00  \n2   0.000000e+00  0.000000e+00  \n3   0.000000e+00  0.000000e+00  \n4   0.000000e+00  0.000000e+00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>normal_mild</th>\n      <th>moderate</th>\n      <th>severe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n      <td>1.000000</td>\n      <td>2.000000e-07</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n      <td>1.000000</td>\n      <td>2.000000e-07</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n      <td>0.999969</td>\n      <td>3.140000e-05</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>44036939_left_subarticular_stenosis_l1_l2</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>44036939_left_subarticular_stenosis_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>44036939_left_subarticular_stenosis_l3_l4</td>\n      <td>0.999999</td>\n      <td>1.300000e-06</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>44036939_left_subarticular_stenosis_l4_l5</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>44036939_left_subarticular_stenosis_l5_s1</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>44036939_right_neural_foraminal_narrowing_l1_l2</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>44036939_right_neural_foraminal_narrowing_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>44036939_right_neural_foraminal_narrowing_l3_l4</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>44036939_right_neural_foraminal_narrowing_l4_l5</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>44036939_right_neural_foraminal_narrowing_l5_s1</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e-07</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>4.000000e-07</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n      <td>1.000000</td>\n      <td>2.000000e-07</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n      <td>0.999996</td>\n      <td>3.700000e-06</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>44036939_spinal_canal_stenosis_l1_l2</td>\n      <td>1.000000</td>\n      <td>1.000000e-07</td>\n      <td>2.000000e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44036939_spinal_canal_stenosis_l2_l3</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44036939_spinal_canal_stenosis_l3_l4</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44036939_spinal_canal_stenosis_l4_l5</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>44036939_spinal_canal_stenosis_l5_s1</td>\n      <td>1.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}