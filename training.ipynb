{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":9578054,"sourceType":"datasetVersion","datasetId":5824402},{"sourceId":127817,"sourceType":"modelInstanceVersion","modelInstanceId":107642,"modelId":131980}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\nimport albumentations as A\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nimport pydicom\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom sklearn.model_selection import train_test_split\nimport bisect\nimport time\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T10:16:37.538735Z","iopub.execute_input":"2024-10-11T10:16:37.539321Z","iopub.status.idle":"2024-10-11T10:16:45.650019Z","shell.execute_reply.started":"2024-10-11T10:16:37.539270Z","shell.execute_reply":"2024-10-11T10:16:45.649011Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:16:45.652046Z","iopub.execute_input":"2024-10-11T10:16:45.653119Z","iopub.status.idle":"2024-10-11T10:17:01.765460Z","shell.execute_reply.started":"2024-10-11T10:16:45.653072Z","shell.execute_reply":"2024-10-11T10:17:01.764346Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-6n8qpfeq\n  Running command git clone --filter=blob:none --quiet https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-6n8qpfeq\n  Resolved https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to commit 7021d63a49106e22c79b40564a7d39930e7b0f53\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: warmup_scheduler\n  Building wheel for warmup_scheduler (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for warmup_scheduler: filename=warmup_scheduler-0.3.2-py3-none-any.whl size=3880 sha256=b2464e8e2e584077616f6a1340f7b43cb36bb54de3fd4584a2f0d6a511df8408\n  Stored in directory: /tmp/pip-ephem-wheel-cache-86eggzt_/wheels/49/78/e6/9168d5844935482a171c7880a0626fa1c6c412b55666635f59\nSuccessfully built warmup_scheduler\nInstalling collected packages: warmup_scheduler\nSuccessfully installed warmup_scheduler-0.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# Config\ncfg= SimpleNamespace(\n    img_dir= \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images\",\n    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    n_frames=3,\n    epochs=4,\n    lr=0.0005,\n    batch_size=16,\n    backbone=\"resnet18\",\n    seed= 0,\n    model_dir = \"/kaggle/working/\",\n    kernel_type = \"resnet18\",\n    num_workers = 4,\n    n_epochs = 4,\n    init_lr =0.0005,\n    CUDA_VISIBLE_DEVICES = \"0\",\n    sag_axial_slices = 3,\n    sag_2_slices =  1,\n    sag_1_slices = 4\n)\nset_seed(seed=cfg.seed) # Makes results reproducable","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.766762Z","iopub.execute_input":"2024-10-11T10:17:01.767081Z","iopub.status.idle":"2024-10-11T10:17:01.805192Z","shell.execute_reply.started":"2024-10-11T10:17:01.767047Z","shell.execute_reply":"2024-10-11T10:17:01.804281Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_training_dataframe(cfg,isTrain=True):\n    \n    train_df  = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n    train_series_description = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n    train_df_df_cleaned = train_df.dropna()\n\n    sagtialla_t1_df = train_series_description[train_series_description['series_description'] == \"Sagittal T1\"]\n    sagtialla_t1_df_u = sagtialla_t1_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n    \n    axial_t2_df = train_series_description[train_series_description['series_description'] == \"Axial T2\"]\n    axial_t2_df_u = axial_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    sagtialla_t2_df = train_series_description[train_series_description['series_description'] == \"Sagittal T2/STIR\"]\n    sagtialla_t2_df_u = sagtialla_t2_df.drop_duplicates(subset=['study_id']).reset_index(drop=True)\n\n    merge_sagital_t1 = pd.merge(train_df_df_cleaned,sagtialla_t1_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_sagital_t1 = merge_sagital_t1.drop(columns=['series_description'])\n    merge_sagital_t1.rename(columns={'series_id': 'series_id_sg1'}, inplace=True)\n    \n    merge_sagital_t2 = pd.merge(merge_sagital_t1,sagtialla_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_sagital_t2 = merge_sagital_t2.drop(columns=['series_description'])\n    merge_sagital_t2.rename(columns={'series_id': 'series_id_sg2'}, inplace=True)\n\n    merge_axial_t2 = pd.merge(merge_sagital_t2,axial_t2_df_u, on=[\"study_id\"],how=\"inner\" )\n    merge_axial_t2 = merge_axial_t2.drop(columns=['series_description'])\n    merge_axial_t2.rename(columns={'series_id': 'series_id_a2'}, inplace=True)\n\n    label2id = {'Normal/Mild': 0, 'Moderate':1, 'Severe':2}\n    merge_axial_t2 = merge_axial_t2.replace(label2id)\n    return merge_axial_t2  \n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.807462Z","iopub.execute_input":"2024-10-11T10:17:01.807821Z","iopub.status.idle":"2024-10-11T10:17:01.819194Z","shell.execute_reply.started":"2024-10-11T10:17:01.807789Z","shell.execute_reply":"2024-10-11T10:17:01.818374Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def batch_to_device(batch, device, skip_keys=[]):\n    batch_dict= {}\n    for key in batch:\n        if key in skip_keys:\n             batch_dict[key]= batch[key]\n        else:    \n            batch_dict[key]= batch[key].to(device)\n    return batch_dict\n\ndef visualize_prediction(batch, pred, epoch):\n    \n    mid= cfg.n_frames//2\n    \n    # Plot\n    for idx in range(1):\n    \n        # Select Data\n        img= batch[\"img\"][idx, mid, :, :].cpu().numpy()*255\n        cs_true= batch[\"label\"][idx, ...].cpu().numpy()*256\n        cs= pred[idx, ...].cpu().numpy()*256\n                \n        coords_list = [(\"TRUE\", \"lightblue\", cs_true), (\"PRED\", \"orange\", cs)]\n        text_labels = [str(x) for x in range(1,21)]\n        \n        # Plot coords\n        fig, axes = plt.subplots(1, len(coords_list), figsize=(10,4))\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        for ax, (title, color, coords) in zip(axes, coords_list):\n            ax.imshow(img, cmap='gray')\n            ax.scatter(coords[0::2], coords[1::2], c=color, s=50)\n            ax.axis('off')\n            ax.set_title(title)\n\n            # Add text labels near the coordinates\n            for i, (x, y) in enumerate(zip(coords[0::2], coords[1::2])):\n                if i < len(text_labels):  # Ensure there are enough labels\n                    ax.text(x + 10, y, text_labels[i], color='white', fontsize=15, bbox=dict(facecolor='black', alpha=0.5))\n\n\n        fig.suptitle(\"EPOCH: {}\".format(epoch))\n        plt.show()\n#         plt.close(fig)\n    return\n\ndef load_weights_skip_mismatch(model, weights_path, device):\n    # Load Weights\n    state_dict = torch.load(weights_path, map_location=device)\n    model_dict = model.state_dict()\n    \n    # Iter models\n    params = {}\n    for (sdk, sfv), (mdk, mdv) in zip(state_dict.items(), model_dict.items()):\n        if sfv.size() == mdv.size():\n            params[sdk] = sfv\n        else:\n            print(\"Skipping param: {}, {} != {}\".format(sdk, sfv.size(), mdv.size()))\n    \n    # Reload + Skip\n    model.load_state_dict(params, strict=False)\n    print(\"Loaded weights from:\", weights_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.820234Z","iopub.execute_input":"2024-10-11T10:17:01.820574Z","iopub.status.idle":"2024-10-11T10:17:01.838353Z","shell.execute_reply.started":"2024-10-11T10:17:01.820524Z","shell.execute_reply":"2024-10-11T10:17:01.837382Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Create csv Dataset","metadata":{}},{"cell_type":"code","source":"train_df = load_training_dataframe(cfg)\n#train_df = train_df.head(10)\n# train_df = train_df[train_df['study_id'].isin([\n#     4646740\n# ])] ","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.839434Z","iopub.execute_input":"2024-10-11T10:17:01.840265Z","iopub.status.idle":"2024-10-11T10:17:01.964346Z","shell.execute_reply.started":"2024-10-11T10:17:01.840230Z","shell.execute_reply":"2024-10-11T10:17:01.963427Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2546555283.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  merge_axial_t2 = merge_axial_t2.replace(label2id)\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_df","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.965462Z","iopub.execute_input":"2024-10-11T10:17:01.965793Z","iopub.status.idle":"2024-10-11T10:17:01.970001Z","shell.execute_reply.started":"2024-10-11T10:17:01.965762Z","shell.execute_reply":"2024-10-11T10:17:01.969071Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"folder_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/4646740/3201256954'\n\n# List all files in the directory\nfiles = os.listdir(folder_path)\n\n# Filter only files (not directories)\nfiles = [f for f in files if os.path.isfile(os.path.join(folder_path, f))]\n\nprint(files)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:01.971433Z","iopub.execute_input":"2024-10-11T10:17:01.971828Z","iopub.status.idle":"2024-10-11T10:17:02.010716Z","shell.execute_reply.started":"2024-10-11T10:17:01.971787Z","shell.execute_reply":"2024-10-11T10:17:02.009906Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['12.dcm', '18.dcm', '9.dcm', '22.dcm', '25.dcm', '39.dcm', '45.dcm', '14.dcm', '11.dcm', '44.dcm', '24.dcm', '34.dcm', '29.dcm', '23.dcm', '41.dcm', '35.dcm', '10.dcm', '46.dcm', '28.dcm', '43.dcm', '50.dcm', '37.dcm', '17.dcm', '30.dcm', '1.dcm', '15.dcm', '2.dcm', '52.dcm', '48.dcm', '36.dcm', '8.dcm', '7.dcm', '53.dcm', '21.dcm', '49.dcm', '51.dcm', '33.dcm', '5.dcm', '4.dcm', '42.dcm', '54.dcm', '47.dcm', '31.dcm', '38.dcm', '19.dcm', '27.dcm', '6.dcm', '16.dcm', '20.dcm', '40.dcm', '3.dcm', '32.dcm', '26.dcm', '13.dcm']\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_df","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:02.011773Z","iopub.execute_input":"2024-10-11T10:17:02.012045Z","iopub.status.idle":"2024-10-11T10:17:02.015879Z","shell.execute_reply.started":"2024-10-11T10:17:02.012015Z","shell.execute_reply":"2024-10-11T10:17:02.015022Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Co-ordinate Prediction Dataset","metadata":{}},{"cell_type":"code","source":"class PreTrainDataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids,cfg,transform,isTrain = False,is_dataset_for_t1= False):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.is_dataset_for_t1 = is_dataset_for_t1\n\n    \n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n\n\n    def load_dicom_stack(self,dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n        # if reverse_sort=False, then increasing array index will be from RIGHT->LEFT and CAUDAL->CRANIAL\n        # thus we do reverse_sort=True for axial so increasing array index is craniocaudal\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n        array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n        array = array[idx]\n        return {\"array\": self.convert_to_8bit(array), \"positions\": ipp, \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")}\n    \n    def pad_image(self, img):\n        n= img.shape[0]\n        if n >= self.cfg.n_frames:\n            start_idx = (n - self.cfg.n_frames) // 2\n            return img[start_idx:start_idx + self.cfg.n_frames,:, :]\n        else:\n            pad_left = (self.cfg.n_frames - n) // 2\n            pad_right = self.cfg.n_frames - n - pad_left\n            return np.pad(img, ((pad_left, pad_right),(0,0), (0,0)), 'constant', constant_values=0)\n    \n    def load_img(self, series_id):\n        fname = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(series_id)), plane=\"sagittal\")\n        img= fname[\"array\"]\n        img= self.pad_image(img)\n        img= np.transpose(img, (1,2, 0))\n        img= self.transform(image=img)[\"image\"]\n        img= np.transpose(img, (2, 0, 1))\n        img= (img / 255.0)\n        return img\n        \n        \n    def __getitem__(self, idx):\n        d= self.study_ids.iloc[idx]\n        \n        if self.is_dataset_for_t1:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg1)])      \n        else:\n            series_id= \"/\".join([str(d.study_id),str(d.series_id_sg2)])      \n                \n        img= self.load_img(series_id)\n        if self.isTrain:\n            return {\n                'img': img, \n                'label': label,\n            }\n        else:\n            return {\n                'img': img \n            }\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n    \n\nresize_transform_point= A.Compose([\nA.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\nA.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n])\n\n\n\n\nds = PreTrainDataset(train_df, cfg,resize_transform_point)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\nfor k, v in ds[0].items():\n    print(k, v.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:02.020179Z","iopub.execute_input":"2024-10-11T10:17:02.020542Z","iopub.status.idle":"2024-10-11T10:17:02.727284Z","shell.execute_reply.started":"2024-10-11T10:17:02.020499Z","shell.execute_reply":"2024-10-11T10:17:02.726345Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nimg (3, 256, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load backbone for RSNA 2024 task\nmodel_path = \"/kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\"\nmodel = timm.create_model('resnet18', pretrained=True, num_classes=20)\nmodel = model.to(cfg.device)\nload_weights_skip_mismatch(model, model_path, cfg.device)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:02.728433Z","iopub.execute_input":"2024-10-11T10:17:02.728738Z","iopub.status.idle":"2024-10-11T10:17:04.220719Z","shell.execute_reply.started":"2024-10-11T10:17:02.728707Z","shell.execute_reply":"2024-10-11T10:17:04.219796Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3c6231989f45eeb82233676510e737"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/2699087807.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(weights_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded weights from: /kaggle/input/lumbar_spine_verterbrae_disc-detection/pytorch/1/1/resnet18_0.pt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Co-ordinate Inferencing","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:35:01.530636Z","iopub.execute_input":"2024-10-07T04:35:01.531106Z","iopub.status.idle":"2024-10-07T04:35:01.554979Z","shell.execute_reply.started":"2024-10-07T04:35:01.531037Z","shell.execute_reply":"2024-10-07T04:35:01.553820Z"}}},{"cell_type":"code","source":"def coordinate_prediction(model,pred_dataloader,isSagitalT1 = False):\n    predictions = []\n    with torch.no_grad():\n        model = model.eval()\n        for batch in tqdm(pred_dataloader):\n            batch = batch_to_device(batch, cfg.device)\n\n            pred = model(batch[\"img\"].float())\n            pred = torch.sigmoid(pred)\n            predictions.append(pred)\n            \n    data_list_cpu = [tensor.cpu().numpy() for tensor in predictions]\n    combined_data = np.vstack(data_list_cpu)\n    df_cords = pd.DataFrame(combined_data)\n    if isSagitalT1:\n        df_cords.to_csv(\"predicted_cordinates_sag_t1.csv\",index=False)\n    else:\n        df_cords.to_csv(\"predicted_cordinates_sag_t2.csv\",index=False)\n    return df_cords","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:04.221805Z","iopub.execute_input":"2024-10-11T10:17:04.222099Z","iopub.status.idle":"2024-10-11T10:17:04.229460Z","shell.execute_reply.started":"2024-10-11T10:17:04.222067Z","shell.execute_reply":"2024-10-11T10:17:04.228542Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Coordinate Prediction","metadata":{}},{"cell_type":"code","source":"train_ds = PreTrainDataset(train_df, cfg,resize_transform_point)\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:04.230842Z","iopub.execute_input":"2024-10-11T10:17:04.231173Z","iopub.status.idle":"2024-10-11T10:17:04.242817Z","shell.execute_reply.started":"2024-10-11T10:17:04.231130Z","shell.execute_reply":"2024-10-11T10:17:04.241961Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cordinates_t1 = coordinate_prediction(model,train_dl,isSagitalT1 = True)\ncordinates_t2 = coordinate_prediction(model,train_dl,isSagitalT1 = False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:17:04.243908Z","iopub.execute_input":"2024-10-11T10:17:04.244223Z","iopub.status.idle":"2024-10-11T10:38:52.196795Z","shell.execute_reply.started":"2024-10-11T10:17:04.244191Z","shell.execute_reply":"2024-10-11T10:38:52.195856Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 112/112 [12:44<00:00,  6.82s/it]\n100%|██████████| 112/112 [09:03<00:00,  4.86s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"#train_df['study_id']","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.198074Z","iopub.execute_input":"2024-10-11T10:38:52.198384Z","iopub.status.idle":"2024-10-11T10:38:52.202711Z","shell.execute_reply.started":"2024-10-11T10:38:52.198351Z","shell.execute_reply":"2024-10-11T10:38:52.201793Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#cordinates_t1","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.203783Z","iopub.execute_input":"2024-10-11T10:38:52.204075Z","iopub.status.idle":"2024-10-11T10:38:52.215364Z","shell.execute_reply.started":"2024-10-11T10:38:52.204041Z","shell.execute_reply":"2024-10-11T10:38:52.214488Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"cordinates_t1['study_id'] = train_df['study_id'].values\ncordinates_t2['study_id'] = train_df['study_id'].values","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.216409Z","iopub.execute_input":"2024-10-11T10:38:52.216727Z","iopub.status.idle":"2024-10-11T10:38:52.226993Z","shell.execute_reply.started":"2024-10-11T10:38:52.216692Z","shell.execute_reply":"2024-10-11T10:38:52.226022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"cordinates_t1","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.228105Z","iopub.execute_input":"2024-10-11T10:38:52.228380Z","iopub.status.idle":"2024-10-11T10:38:52.264600Z","shell.execute_reply.started":"2024-10-11T10:38:52.228350Z","shell.execute_reply":"2024-10-11T10:38:52.263607Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"             0         1         2         3         4         5         6  \\\n0     0.358656  0.374705  0.509141  0.371950  0.344499  0.489798  0.511614   \n1     0.390123  0.272419  0.629843  0.302406  0.361005  0.392987  0.603127   \n2     0.377804  0.277879  0.551068  0.310396  0.354630  0.411120  0.536126   \n3     0.422230  0.272064  0.554788  0.318366  0.375878  0.379581  0.531984   \n4     0.402759  0.257216  0.596857  0.293941  0.369302  0.386106  0.574754   \n...        ...       ...       ...       ...       ...       ...       ...   \n1785  0.348200  0.222990  0.571816  0.267857  0.310215  0.352571  0.529989   \n1786  0.377478  0.307640  0.564883  0.339093  0.337882  0.457434  0.533598   \n1787  0.332602  0.327601  0.519570  0.348472  0.317127  0.434848  0.492108   \n1788  0.349592  0.288166  0.521606  0.327668  0.318610  0.419837  0.505833   \n1789  0.410975  0.253050  0.576159  0.289596  0.386226  0.387870  0.544321   \n\n             7         8         9  ...        11        12        13  \\\n0     0.473629  0.353111  0.605587  ...  0.587102  0.373437  0.722634   \n1     0.413803  0.353677  0.510201  ...  0.507031  0.364985  0.626339   \n2     0.413698  0.358668  0.525839  ...  0.527877  0.356278  0.658888   \n3     0.418145  0.357825  0.490115  ...  0.513823  0.356010  0.614302   \n4     0.422091  0.338726  0.532496  ...  0.546451  0.322665  0.671288   \n...        ...       ...       ...  ...       ...       ...       ...   \n1785  0.385268  0.293090  0.515912  ...  0.505981  0.307020  0.670571   \n1786  0.461048  0.322099  0.600892  ...  0.591484  0.334299  0.757367   \n1787  0.478025  0.310772  0.575786  ...  0.579262  0.321266  0.717456   \n1788  0.439730  0.319585  0.558770  ...  0.535285  0.354532  0.707523   \n1789  0.400536  0.391321  0.514609  ...  0.509812  0.395124  0.658983   \n\n            14        15        16        17        18        19    study_id  \n0     0.526738  0.671897  0.424140  0.849819  0.551493  0.743065     4003253  \n1     0.617148  0.590170  0.398347  0.745176  0.657364  0.680044     4646740  \n2     0.532196  0.629970  0.390902  0.790636  0.553370  0.707839     7143189  \n3     0.512234  0.588183  0.397038  0.752634  0.528639  0.675257     8785691  \n4     0.537063  0.671580  0.354253  0.850769  0.566507  0.759883    10728036  \n...        ...       ...       ...       ...       ...       ...         ...  \n1785  0.526042  0.634243  0.350070  0.830489  0.557492  0.720233  4282019580  \n1786  0.519319  0.693534  0.415685  0.898814  0.571472  0.791665  4283570761  \n1787  0.487239  0.674535  0.357391  0.863263  0.506651  0.764439  4284048608  \n1788  0.532307  0.632179  0.429828  0.826329  0.586490  0.707909  4287160193  \n1789  0.565052  0.605555  0.461614  0.784938  0.604415  0.681290  4290709089  \n\n[1790 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>study_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.358656</td>\n      <td>0.374705</td>\n      <td>0.509141</td>\n      <td>0.371950</td>\n      <td>0.344499</td>\n      <td>0.489798</td>\n      <td>0.511614</td>\n      <td>0.473629</td>\n      <td>0.353111</td>\n      <td>0.605587</td>\n      <td>...</td>\n      <td>0.587102</td>\n      <td>0.373437</td>\n      <td>0.722634</td>\n      <td>0.526738</td>\n      <td>0.671897</td>\n      <td>0.424140</td>\n      <td>0.849819</td>\n      <td>0.551493</td>\n      <td>0.743065</td>\n      <td>4003253</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.390123</td>\n      <td>0.272419</td>\n      <td>0.629843</td>\n      <td>0.302406</td>\n      <td>0.361005</td>\n      <td>0.392987</td>\n      <td>0.603127</td>\n      <td>0.413803</td>\n      <td>0.353677</td>\n      <td>0.510201</td>\n      <td>...</td>\n      <td>0.507031</td>\n      <td>0.364985</td>\n      <td>0.626339</td>\n      <td>0.617148</td>\n      <td>0.590170</td>\n      <td>0.398347</td>\n      <td>0.745176</td>\n      <td>0.657364</td>\n      <td>0.680044</td>\n      <td>4646740</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.377804</td>\n      <td>0.277879</td>\n      <td>0.551068</td>\n      <td>0.310396</td>\n      <td>0.354630</td>\n      <td>0.411120</td>\n      <td>0.536126</td>\n      <td>0.413698</td>\n      <td>0.358668</td>\n      <td>0.525839</td>\n      <td>...</td>\n      <td>0.527877</td>\n      <td>0.356278</td>\n      <td>0.658888</td>\n      <td>0.532196</td>\n      <td>0.629970</td>\n      <td>0.390902</td>\n      <td>0.790636</td>\n      <td>0.553370</td>\n      <td>0.707839</td>\n      <td>7143189</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.422230</td>\n      <td>0.272064</td>\n      <td>0.554788</td>\n      <td>0.318366</td>\n      <td>0.375878</td>\n      <td>0.379581</td>\n      <td>0.531984</td>\n      <td>0.418145</td>\n      <td>0.357825</td>\n      <td>0.490115</td>\n      <td>...</td>\n      <td>0.513823</td>\n      <td>0.356010</td>\n      <td>0.614302</td>\n      <td>0.512234</td>\n      <td>0.588183</td>\n      <td>0.397038</td>\n      <td>0.752634</td>\n      <td>0.528639</td>\n      <td>0.675257</td>\n      <td>8785691</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.402759</td>\n      <td>0.257216</td>\n      <td>0.596857</td>\n      <td>0.293941</td>\n      <td>0.369302</td>\n      <td>0.386106</td>\n      <td>0.574754</td>\n      <td>0.422091</td>\n      <td>0.338726</td>\n      <td>0.532496</td>\n      <td>...</td>\n      <td>0.546451</td>\n      <td>0.322665</td>\n      <td>0.671288</td>\n      <td>0.537063</td>\n      <td>0.671580</td>\n      <td>0.354253</td>\n      <td>0.850769</td>\n      <td>0.566507</td>\n      <td>0.759883</td>\n      <td>10728036</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1785</th>\n      <td>0.348200</td>\n      <td>0.222990</td>\n      <td>0.571816</td>\n      <td>0.267857</td>\n      <td>0.310215</td>\n      <td>0.352571</td>\n      <td>0.529989</td>\n      <td>0.385268</td>\n      <td>0.293090</td>\n      <td>0.515912</td>\n      <td>...</td>\n      <td>0.505981</td>\n      <td>0.307020</td>\n      <td>0.670571</td>\n      <td>0.526042</td>\n      <td>0.634243</td>\n      <td>0.350070</td>\n      <td>0.830489</td>\n      <td>0.557492</td>\n      <td>0.720233</td>\n      <td>4282019580</td>\n    </tr>\n    <tr>\n      <th>1786</th>\n      <td>0.377478</td>\n      <td>0.307640</td>\n      <td>0.564883</td>\n      <td>0.339093</td>\n      <td>0.337882</td>\n      <td>0.457434</td>\n      <td>0.533598</td>\n      <td>0.461048</td>\n      <td>0.322099</td>\n      <td>0.600892</td>\n      <td>...</td>\n      <td>0.591484</td>\n      <td>0.334299</td>\n      <td>0.757367</td>\n      <td>0.519319</td>\n      <td>0.693534</td>\n      <td>0.415685</td>\n      <td>0.898814</td>\n      <td>0.571472</td>\n      <td>0.791665</td>\n      <td>4283570761</td>\n    </tr>\n    <tr>\n      <th>1787</th>\n      <td>0.332602</td>\n      <td>0.327601</td>\n      <td>0.519570</td>\n      <td>0.348472</td>\n      <td>0.317127</td>\n      <td>0.434848</td>\n      <td>0.492108</td>\n      <td>0.478025</td>\n      <td>0.310772</td>\n      <td>0.575786</td>\n      <td>...</td>\n      <td>0.579262</td>\n      <td>0.321266</td>\n      <td>0.717456</td>\n      <td>0.487239</td>\n      <td>0.674535</td>\n      <td>0.357391</td>\n      <td>0.863263</td>\n      <td>0.506651</td>\n      <td>0.764439</td>\n      <td>4284048608</td>\n    </tr>\n    <tr>\n      <th>1788</th>\n      <td>0.349592</td>\n      <td>0.288166</td>\n      <td>0.521606</td>\n      <td>0.327668</td>\n      <td>0.318610</td>\n      <td>0.419837</td>\n      <td>0.505833</td>\n      <td>0.439730</td>\n      <td>0.319585</td>\n      <td>0.558770</td>\n      <td>...</td>\n      <td>0.535285</td>\n      <td>0.354532</td>\n      <td>0.707523</td>\n      <td>0.532307</td>\n      <td>0.632179</td>\n      <td>0.429828</td>\n      <td>0.826329</td>\n      <td>0.586490</td>\n      <td>0.707909</td>\n      <td>4287160193</td>\n    </tr>\n    <tr>\n      <th>1789</th>\n      <td>0.410975</td>\n      <td>0.253050</td>\n      <td>0.576159</td>\n      <td>0.289596</td>\n      <td>0.386226</td>\n      <td>0.387870</td>\n      <td>0.544321</td>\n      <td>0.400536</td>\n      <td>0.391321</td>\n      <td>0.514609</td>\n      <td>...</td>\n      <td>0.509812</td>\n      <td>0.395124</td>\n      <td>0.658983</td>\n      <td>0.565052</td>\n      <td>0.605555</td>\n      <td>0.461614</td>\n      <td>0.784938</td>\n      <td>0.604415</td>\n      <td>0.681290</td>\n      <td>4290709089</td>\n    </tr>\n  </tbody>\n</table>\n<p>1790 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def get_padded_roi(orientation, numberOfImageFromCenter):\n    # Calculate middle index\n    middleImage = len(orientation[\"array\"]) // 2\n\n    # Calculate start and end indices\n    start_idx = middleImage - numberOfImageFromCenter\n    end_idx = middleImage + numberOfImageFromCenter + 1\n\n    # Handle bounds\n    array_length = orientation[\"array\"].shape[0]  # Slicing along the first axis (number of images)\n\n    # Ensure we don't go beyond the array's bounds\n    start_pad = max(0, -start_idx)\n    end_pad = max(0, end_idx - array_length)\n\n    # Slice the valid part of the array along the first axis\n    roi = orientation[\"array\"][max(0, start_idx):min(array_length, end_idx)]\n\n    # Pad with zeros if needed\n    if start_pad > 0 or end_pad > 0:\n        roi = np.pad(roi, ((start_pad, end_pad), (0, 0), (0, 0)), mode='constant', constant_values=0)\n\n    return roi","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.265874Z","iopub.execute_input":"2024-10-11T10:38:52.266193Z","iopub.status.idle":"2024-10-11T10:38:52.273595Z","shell.execute_reply.started":"2024-10-11T10:38:52.266158Z","shell.execute_reply":"2024-10-11T10:38:52.272714Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"orientation = {\n    \"array\": np.random.randint(0, 10, (1, 5, 5))  # Example 3D array of shape (10, 5, 5)\n}\n\nnumberOfImageFromCenter = 1\norientation['array'] = get_padded_roi(orientation, numberOfImageFromCenter)\nprint(orientation)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.274424Z","iopub.execute_input":"2024-10-11T10:38:52.274746Z","iopub.status.idle":"2024-10-11T10:38:52.289748Z","shell.execute_reply.started":"2024-10-11T10:38:52.274711Z","shell.execute_reply":"2024-10-11T10:38:52.288867Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"{'array': array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[5, 0, 3, 3, 7],\n        [9, 3, 5, 2, 4],\n        [7, 6, 8, 8, 1],\n        [6, 7, 7, 8, 1],\n        [5, 9, 8, 9, 4]],\n\n       [[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Utils for LSDD ","metadata":{}},{"cell_type":"code","source":"def prepare_level_wise_axial(sagittal_img, imgsag_y_coord_to_axial_slice, coordinates,no_of_axial_slice = 3):\n    h, w = sagittal_img.shape\n    axial_list = []\n    #print(\"start axial\")\n    first_key = next(iter(imgsag_y_coord_to_axial_slice))\n    # Get the first value\n    first_value = imgsag_y_coord_to_axial_slice[first_key]\n\n    keys = list(imgsag_y_coord_to_axial_slice.keys())\n    keys.sort()\n\n    for i in range(0, len(coordinates), 4):\n        #print(i)\n        category = coordinates[i:i+4]  # Extracting 4 elements at a time\n        y= [category[1]*h,category[3]*h]\n        minimum = math.floor(min(y[0],y[1]))\n        maximum  = math.ceil(max(y[0],y[1]))\n        filtered_keys = [k for k in imgsag_y_coord_to_axial_slice.keys() if minimum <= k <= maximum]\n        \n        if len(filtered_keys) >= no_of_axial_slice:\n            # Use numpy to select 3 keys at uniform intervals\n            selected_keys = np.linspace(0, len(filtered_keys)-1,no_of_axial_slice , dtype=int)\n            selected_keys = [filtered_keys[i] for i in selected_keys]\n        else:\n            if len(filtered_keys) == 0:\n                index = bisect.bisect_left(keys, minimum)\n                # Check if we can find a nearest value less than current_value\n                if index > 0:\n                    filtered_keys.append(keys[index - 1])\n\n                index = bisect.bisect_right(keys, maximum)\n                if index < len(keys):\n                    filtered_keys.append(keys[index])  # Return the nearest value greater than current_value\n                    \n\n            filtered_keys.extend([filtered_keys[-1]] * (no_of_axial_slice - len(filtered_keys)))\n            selected_keys = filtered_keys\n        \n        if len(selected_keys) ==  0:\n            result  = np.zeros((no_of_axial_slice, first_value.shape[0],first_value.shape[1]))\n        else:\n            selected_axial =[imgsag_y_coord_to_axial_slice.get(k) for k in selected_keys]\n            result  = np.array(selected_axial)\n\n        roi_copy_list  = []\n        for j in range(result.shape[0]):\n            roi_copy_list.append(resize_transform(image=result[j])[\"image\"])\n            \n        axial_list.append(np.array(roi_copy_list))\n        \n    return  np.array(axial_list)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.290996Z","iopub.execute_input":"2024-10-11T10:38:52.291301Z","iopub.status.idle":"2024-10-11T10:38:52.305643Z","shell.execute_reply.started":"2024-10-11T10:38:52.291265Z","shell.execute_reply":"2024-10-11T10:38:52.304894Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"resize_transform= A.Compose([\n    A.LongestMaxSize(max_size=256, interpolation=cv2.INTER_CUBIC, always_apply=True),\n    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n    A.Normalize()\n])\n\ndef angle_of_line(x1, y1, x2, y2):\n    return math.degrees(math.atan2(-(y2-y1), x2-x1))\n\ndef crop_between_keypoints(roi,img, keypoint1, keypoint2):\n    h, w = img.shape\n    x1, y1 = int(keypoint1[0]), int(keypoint1[1])\n    x2, y2 = int(keypoint2[0]), int(keypoint2[1])\n\n    # Calculate bounding box around the keypoints\n    left = int(min(x1, x2) - (w * 0.1))\n    right = int(max(x1, x2) + (w * 0.1))\n    top = int(min(y1, y2) - (h * 0.07))\n    bottom = int(max(y1, y2) + (h * 0.1))\n    \n    left = max(0, left)\n    right = min(w, right)\n    top = max(0, top)\n    bottom = min(h, bottom)\n    # Crop the image\n    return img[top:bottom, left:right],roi[:,top:bottom, left:right]\n\ndef plot_5_crops(orientataion,coords_temp,numberOfImageFromCenter = 3):\n    # Create a figure and axis for the grid\n    #fig = plt.figure(figsize=(10, 10))\n    #gs = gridspec.GridSpec(1, 5, width_ratios=[1]*5)\n    #print(\"start crop\")\n    #print(coords_temp)\n    # Plot the crops\n    #print(\"plot-5-crop-img\")\n    orientataion['array'] = get_padded_roi(orientataion,numberOfImageFromCenter)\n    middleImage = len(orientataion[\"array\"])//2\n    img = orientataion[\"array\"][middleImage]\n    \n    roi = orientataion[\"array\"][middleImage-numberOfImageFromCenter:middleImage+numberOfImageFromCenter+1]\n    croppedImage = []\n    #print(p)\n    for i in range(0, len(coords_temp), 4):\n        # Copy of img\n        img_copy= img.copy()\n        h, w = img.shape\n        #print(\"Level \",i//4)\n        #print(\"img shape\",img.shape)\n        roi_copy = roi.copy()\n        # Extract Keypoints\n        category = coords_temp[i:i+4]  # Extracting 4 elements at a time\n        a= (category[0]*w, category[1]*h)\n        b= (category[2]*w, category[3]*h)\n        \n        # Rotate\n        rotate_angle= angle_of_line(a[0], a[1], b[0], b[1])\n        transform = A.Compose([\n            A.Rotate(limit=(-rotate_angle, -rotate_angle), p=1.0),\n        ], keypoint_params= A.KeypointParams(format='xy', remove_invisible=False),\n        )\n    \n        t= transform(image=img_copy, keypoints=[a,b])\n        img_copy= t[\"image\"]\n        a,b= t[\"keypoints\"]\n        #print(\"before\",img_copy.shape)\n        # Crop + Resize\n        img_copy,roi_copy = crop_between_keypoints(roi_copy,img_copy, a, b)\n        #print(\"Before Transformation\",roi_copy.shape)\n        #print(\"after Tra img Shape\",img_copy.shape)\n        roi_copy_list  = []\n        for j in range (0,numberOfImageFromCenter*2+1):\n            roi_copy_list.append(resize_transform(image=roi_copy[j])[\"image\"])\n        roi_copy_list = np.array(roi_copy_list)\n        #print(\"After transformation\",roi_copy_list.shape)\n        img_copy = roi_copy_list[numberOfImageFromCenter]\n        croppedImage.append(roi_copy_list)\n        # Plot\n        #ax = plt.subplot(gs[i//4])\n        #ax.imshow(img_copy, cmap='gray')\n        #ax.set_title(f\"L{i//4+1}\")\n        #ax.axis('off')\n    #plt.show()\n    #print(np.array(croppedImage).shape)\n    return np.array(croppedImage)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.306664Z","iopub.execute_input":"2024-10-11T10:38:52.307450Z","iopub.status.idle":"2024-10-11T10:38:52.327636Z","shell.execute_reply.started":"2024-10-11T10:38:52.307410Z","shell.execute_reply":"2024-10-11T10:38:52.326823Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Define DataSet for training","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# # Sample DataFrame\n# df = pd.DataFrame({\n#     'A': [0, 1, 2],\n#     'B': [1, 0, 2],\n#     'C': [2, 2, 1],\n#     'D': [0, 1, 0],\n#     'E': [1, 0, 1]\n#     # add 25 columns in actual case\n# })\n\n# # One-hot encode each column in the DataFrame\n# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# # Apply the one-hot encoding function to each row\n# encoded_data = np.array([one_hot_encode_row(row) for row in df.values])\n\n# # Example: to check one row's shape (25, 3)\n# print(encoded_data[0].shape)  # Should output (5, 3) for this example, adjust to 25 columns\n\n# # Example: print the one-hot encoded array for a row\n# print(encoded_data[0])  # One-hot encoded first row\n\n# # Final shape of the encoded data for all rows\n# print(encoded_data.shape)  # (number_of_rows, 25, 3)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.329068Z","iopub.execute_input":"2024-10-11T10:38:52.329581Z","iopub.status.idle":"2024-10-11T10:38:52.339668Z","shell.execute_reply.started":"2024-10-11T10:38:52.329527Z","shell.execute_reply":"2024-10-11T10:38:52.338799Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# def one_hot_encode_row(row):\n#     # One hot encode each value in the row (0,1,2)\n#     return np.array([np.eye(3)[int(val)] for val in row])\n\n# s = one_hot_encode_row([0,1,2,0,0])\n# # Apply the one-hot encoding function to each row\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.341103Z","iopub.execute_input":"2024-10-11T10:38:52.341568Z","iopub.status.idle":"2024-10-11T10:38:52.352005Z","shell.execute_reply.started":"2024-10-11T10:38:52.341499Z","shell.execute_reply":"2024-10-11T10:38:52.351162Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class RSNADataset(torch.utils.data.Dataset):\n    def __init__(self, study_ids, s1_coords, s2_coords,cfg,transform,isTrain = True,transform_axial = None,transform_sag = None):\n        self.cfg= cfg\n        self.study_ids = study_ids\n        self.transform = transform\n        self.isTrain = isTrain\n        self.transform_sag = transform_sag\n        self.transform_axial = transform_axial\n        self.s1_coords = self.align_cord(study_ids,s1_coords)\n        self.s2_coords = self.align_cord(study_ids,s2_coords)\n        self.labeldf = study_ids[[col for col in study_ids.columns if col not in ['study_id','series_id_sg1','series_id_sg2','series_id_a2']]]\n\n    def convert_to_8bit(self,x):\n        lower, upper = np.percentile(x, (1, 99))\n        x = np.clip(x, lower, upper)\n        x = x - np.min(x)\n        x = x / np.max(x) \n        return (x * 255).astype(\"uint8\")\n    \n    def align_cord(self,study_ids,cords):\n        std_id = study_ids[['study_id']] \n        merged = pd.merge(std_id,cords,on=['study_id'],how=\"inner\")\n        return merged[[col for col in merged.columns if col not in ['study_id']]]\n\n    def load_dicom_stack(self, dicom_folder, plane, reverse_sort=False):\n        dicom_files = glob.glob(os.path.join(dicom_folder, \"*.dcm\"))\n        dicoms = [pydicom.dcmread(f) for f in dicom_files]\n\n        # Determine the plane for sorting (sagittal, coronal, axial)\n        plane = {\"sagittal\": 0, \"coronal\": 1, \"axial\": 2}[plane.lower()]\n        positions = np.asarray([float(d.ImagePositionPatient[plane]) for d in dicoms])\n\n        # Sort DICOM files based on positions (reverse sort for axial plane if needed)\n        idx = np.argsort(-positions if reverse_sort else positions)\n        ipp = np.asarray([d.ImagePositionPatient for d in dicoms]).astype(\"float\")[idx]\n\n        # Get the shape of each pixel array (height, width)\n        shapes = [d.pixel_array.shape for d in dicoms]\n        # Check if all DICOM images have the same shape\n        if len(set(shapes)) > 1:\n            # There's a shape mismatch, find the minimum shape (height, width)\n            min_shape = np.min(shapes, axis=0)\n            # Resize images to the minimum shape\n            resized_arrays = []\n            for d in dicoms:\n                img = d.pixel_array.astype(\"float32\")\n                if img.shape != tuple(min_shape):\n                    resized_img = cv2.resize(img, (min_shape[1], min_shape[0]))  # Resize to (width, height)\n                else:\n                    resized_img = img  # No resizing needed\n                resized_arrays.append(resized_img)\n\n            # Stack the resized images along the first axis\n            array = np.stack(resized_arrays)\n        else:\n            # If all shapes are the same, no resizing is needed\n            array = np.stack([d.pixel_array.astype(\"float32\") for d in dicoms])\n\n        # Reorder the array according to the sorted positions\n        array = array[idx]\n\n        return {\n            \"array\": self.convert_to_8bit(array),\n            \"positions\": ipp,\n            \"pixel_spacing\": np.asarray(dicoms[0].PixelSpacing).astype(\"float\")\n        }\n\n     \n    def one_hot_encode_row(self, row):\n        return np.array([np.eye(3)[int(val)] for val in row]) \n    def __getitem__(self, idx):\n        row = self.study_ids.iloc[idx]\n        #print(row.study_id)\n        sag_t1 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg1)), plane=\"sagittal\")\n        ax_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_a2)), plane=\"axial\", reverse_sort=True)\n        sag_t2 = self.load_dicom_stack(os.path.join(self.cfg.img_dir, str(row.study_id), str(row.series_id_sg2)), plane=\"sagittal\")\n        \n        top_left_hand_corner_sag_t2 = sag_t2[\"positions\"][len(sag_t2[\"array\"]) // 2]\n        sag_y_axis_to_pixel_space = [top_left_hand_corner_sag_t2[2]]\n        while len(sag_y_axis_to_pixel_space) < sag_t2[\"array\"].shape[1]: \n            sag_y_axis_to_pixel_space.append(sag_y_axis_to_pixel_space[-1] - sag_t2[\"pixel_spacing\"][1])\n        \n        sag_y_coord_to_axial_slice = {}\n        for ax_t2_slice, ax_t2_pos in zip(ax_t2[\"array\"], ax_t2[\"positions\"]):\n            diffs = np.abs(np.asarray(sag_y_axis_to_pixel_space) - ax_t2_pos[2])\n            sag_y_coord = np.argmin(diffs)\n            sag_y_coord_to_axial_slice[sag_y_coord] = ax_t2_slice\n        \n        sag1_cord = self.s1_coords.iloc[idx].tolist()\n        sag2_cord = self.s2_coords.iloc[idx].tolist()\n        \n        #print(\"s1 cord\",sag1_cord)\n        #print(\"s2 cord\",sag2_cord)\n        img= sag_t2[\"array\"][len(sag_t2[\"array\"])//2]\n        corresponding_axial = self.transform_axial(img, sag_y_coord_to_axial_slice, sag2_cord,no_of_axial_slice = self.cfg.sag_axial_slices)\n        #print(corresponding_axial.shape)\n\n        \n        crop_result_t2 = self.transform_sag(sag_t2, sag2_cord,numberOfImageFromCenter = self.cfg.sag_2_slices)\n        #print(crop_result_t2.shape)\n        \n        crop_result_t1 = self.transform_sag(sag_t1, sag1_cord,numberOfImageFromCenter = self.cfg.sag_1_slices)\n        #print(crop_result_t1.shape)\n        \n        \n        \n        if self.isTrain:\n            label  = self.one_hot_encode_row( self.labeldf.iloc[idx])\n            return (crop_result_t2,crop_result_t1,corresponding_axial),label\n        else:\n            return (crop_result_t2,crop_result_t1, corresponding_axial)\n            \n    \n    def __len__(self,):\n        return len(self.study_ids)\n\nds = RSNADataset(train_df,cordinates_t1,cordinates_t2 ,cfg,resize_transform,isTrain= True,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)    \n\n# Plot a Single Sample\nprint(\"---- Sample Shapes -----\")\n(k1,k2,k3), v  =  ds[1]\nprint(\"k1 shape\",k1.shape)\nprint(\"k1 shape\",k2.shape)\nprint(\"k3 shape\",k3.shape)\nprint(\"label shape\",v.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:52.353446Z","iopub.execute_input":"2024-10-11T10:38:52.353786Z","iopub.status.idle":"2024-10-11T10:38:54.028860Z","shell.execute_reply.started":"2024-10-11T10:38:52.353753Z","shell.execute_reply":"2024-10-11T10:38:54.027893Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"---- Sample Shapes -----\nk1 shape (5, 3, 256, 256)\nk1 shape (5, 9, 256, 256)\nk3 shape (5, 3, 256, 256)\nlabel shape (25, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.030101Z","iopub.execute_input":"2024-10-11T10:38:54.030490Z","iopub.status.idle":"2024-10-11T10:38:54.035048Z","shell.execute_reply.started":"2024-10-11T10:38:54.030446Z","shell.execute_reply":"2024-10-11T10:38:54.034086Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#  model1 = models.video.r3d_18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.036087Z","iopub.execute_input":"2024-10-11T10:38:54.036368Z","iopub.status.idle":"2024-10-11T10:38:54.045445Z","shell.execute_reply.started":"2024-10-11T10:38:54.036337Z","shell.execute_reply":"2024-10-11T10:38:54.044723Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# model1.stem[0].weight","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.050983Z","iopub.execute_input":"2024-10-11T10:38:54.051599Z","iopub.status.idle":"2024-10-11T10:38:54.056020Z","shell.execute_reply.started":"2024-10-11T10:38:54.051540Z","shell.execute_reply":"2024-10-11T10:38:54.055135Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# print(model1)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.057085Z","iopub.execute_input":"2024-10-11T10:38:54.057363Z","iopub.status.idle":"2024-10-11T10:38:54.065151Z","shell.execute_reply.started":"2024-10-11T10:38:54.057333Z","shell.execute_reply":"2024-10-11T10:38:54.064299Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n\n        # Define three 3D ResNet-18 models\n        self.model1 = models.video.r3d_18(pretrained=True)  # First model (input shape: 5,3,256,256)\n        self.model2 = models.video.r3d_18(pretrained=True)  # Second model (input shape: 5,3,256,256)\n        self.model3 = models.video.r3d_18(pretrained=True)  # Third model (input shape: 5,9,256,256)\n        \n        original_conv1 = self.model1.stem[0]\n        self.model1.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model2.stem[0] = nn.Conv3d(5, 64, kernel_size=(9, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n        self.model3.stem[0] = nn.Conv3d(5, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n\n        \n        with torch.no_grad():\n            self.model1.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model1.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            self.model2.stem[0].weight[:, :3, :3, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, 3:, :3, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n            \n            #self.model2.stem[0].weight[:, :, :3, :, :] = self.model2.stem[0].weight[:, :, :, :, :]  # Copy the pre-trained weights for 3 channels\n            self.model2.stem[0].weight[:, :, 3:6, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            self.model2.stem[0].weight[:, :, 6:9, :, :] = self.model2.stem[0].weight[:, :, :3, :, :]  # Repeat/modify to match 5 channels\n            \n            \n            self.model3.stem[0].weight[:, :3, :, :, :] = original_conv1.weight  # Copy the pre-trained weights for 3 channels\n            self.model3.stem[0].weight[:, 3:, :, :, :] = original_conv1.weight[:, :2, :, :, :]  # Repeat/modify to match 5 channels\n        \n        self.model1.fc = nn.Identity()\n        self.model2.fc = nn.Identity()\n        self.model3.fc = nn.Identity()\n        \n        # Extract the feature size from the output of the models\n        self.hidden_size = 32  # Adjust this if necessary, based on the model's output size\n        self.flatten_size = 512\n        # Concatenation layer\n        self.fc = nn.Linear(self.flatten_size * 3, self.hidden_size)   # Final linear layer after concatenation\n\n        # Subclass outputs (25 classes, each has 3 subclasses)\n        self.subclass_layers = nn.ModuleList([nn.Linear(self.hidden_size, 3) for _ in range(25)])\n\n    def forward(self, x1, x2, x3):\n        # Forward pass through each model\n        output1 = self.model1(x1)  # Shape: [batch_size, hidden_size]\n        output2 = self.model2(x2)  # Shape: [batch_size, hidden_size]\n        output3 = self.model3(x3)  # Shape: [batch_size, hidden_size]\n        \n        flatten1 = torch.flatten(output1, 1)\n        flatten2 = torch.flatten(output2, 1)\n        flatten3 = torch.flatten(output3, 1)\n        #print(flatten1.shape)\n        #print(flatten2.shape)\n       # print(flatten3.shape)\n        # Concatenate outputs from the three models\n        concatenated_output = torch.cat((flatten1, flatten2,flatten2), dim=1)  # Shape: [batch_size, hidden_size * 3]\n       #print(concatenated_output.shape)\n        # Pass concatenated output through the final linear layer\n        combined_output = self.fc(concatenated_output)  # Shape: [batch_size, hidden_size]\n\n        # Subclass prediction for each class\n        subclass_outputs = [torch.softmax(layer(combined_output), dim=1) for layer in self.subclass_layers]\n\n        return subclass_outputs\n\n# Example usage\n#if __name__ == \"__main__\":\n  ","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.066367Z","iopub.execute_input":"2024-10-11T10:38:54.066785Z","iopub.status.idle":"2024-10-11T10:38:54.086879Z","shell.execute_reply.started":"2024-10-11T10:38:54.066752Z","shell.execute_reply":"2024-10-11T10:38:54.085880Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# model = CustomModel()\n# model = model.to(cfg.device)\n\n# # Example input tensors\n# input1 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the first model\n# input2 = torch.randn(8, 5, 9, 256, 256)  # Batch size 8 for the second model\n# input3 = torch.randn(8, 5, 3, 256, 256)  # Batch size 8 for the third model\n\n# outputs = model(input1.to(cfg.device), input2.to(cfg.device), input3.to(cfg.device))\n# for i, output in enumerate(outputs):\n#     print(f\"Subclass Output for Class {i + 1}: Shape {output.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.088147Z","iopub.execute_input":"2024-10-11T10:38:54.088542Z","iopub.status.idle":"2024-10-11T10:38:54.100575Z","shell.execute_reply.started":"2024-10-11T10:38:54.088498Z","shell.execute_reply":"2024-10-11T10:38:54.099726Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n\n# def prepare_target(raw_labels):\n#     \"\"\"\n#     Convert raw labels into one-hot encoded labels for 25 classes.\n    \n#     Args:\n#         raw_labels (torch.Tensor): Tensor of shape [batch_size, 25] where each value is the subclass label (0, 1, or 2).\n        \n#     Returns:\n#         torch.Tensor: One-hot encoded labels of shape [batch_size, 25, 3].\n#     \"\"\"\n#     batch_size = raw_labels.shape[0]\n#     num_classes = raw_labels.shape[1]\n#     num_subclasses = 3  # There are 3 subclasses\n\n#     # One-hot encode the subclass labels\n#     one_hot_labels = F.one_hot(raw_labels, num_classes=num_subclasses)  # Shape: [batch_size, 25, 3]\n    \n#     return one_hot_labels.float()  # Return as float for compatibility with loss functions\n\n# # Example usage:\n# batch_size = 8\n# num_classes = 25\n\n# # Randomly generated raw labels where each value is 0, 1, or 2\n# raw_labels = torch.randint(0, 3, (batch_size, num_classes))  # Shape: [batch_size, 25]\n\n# # Prepare one-hot encoded targets\n# targets = prepare_target(raw_labels)\n\n# print(targets.shape)  # Should print: torch.Size([8, 25, 3])","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.101768Z","iopub.execute_input":"2024-10-11T10:38:54.102196Z","iopub.status.idle":"2024-10-11T10:38:54.115906Z","shell.execute_reply.started":"2024-10-11T10:38:54.102152Z","shell.execute_reply":"2024-10-11T10:38:54.115008Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Training And Validation Functions","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer):\n\n    model.train()\n    train_loss = []\n    bar = tqdm(loader)\n    for (data, target) in bar:\n\n        optimizer.zero_grad()\n        \n        axial_t2,sagittal_t2,sagittal_t1 = data\n        axial_t2, sagittal_t2, sagittal_t1, target = axial_t2.to(device), sagittal_t2.to(device), sagittal_t1.to(device),   target.to(device)\n        logits = model(axial_t2, sagittal_t2,sagittal_t1)\n             \n        loss = 0\n        for i in range(25):\n            # Select the i-th subclass prediction and corresponding target\n            subclass_pred = logits[i]  # Output from model, shape: [batch_size, 3]\n            subclass_target = target[:, i]  # Raw target label (not one-hot), shape: [batch_size]\n\n            # Compute the loss for the i-th subclass\n            loss += criterion(subclass_pred, subclass_target)\n        \n        loss.backward()\n        \n        optimizer.step()\n\n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n\n    train_loss = np.mean(train_loss)\n    return train_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.117068Z","iopub.execute_input":"2024-10-11T10:38:54.117673Z","iopub.status.idle":"2024-10-11T10:38:54.129563Z","shell.execute_reply.started":"2024-10-11T10:38:54.117629Z","shell.execute_reply":"2024-10-11T10:38:54.128721Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def val_epoch(model, loader):\n\n    model.eval()\n    val_loss = []\n    bar = tqdm(loader)\n    with torch.no_grad():\n        for (data, target) in bar:\n            \n            axial_t2,sagittal_t2,sagittal_t1 = data\n            axial_t2, sagittal_t2, sagittal_t1, target = axial_t2.to(device), sagittal_t2.to(device), sagittal_t1.to(device),   target.to(device)\n           \n            logits = model(axial_t2, sagittal_t2,sagittal_t1)\n             \n            loss = 0\n            for i in range(25):\n                # Select the i-th subclass prediction and corresponding target\n                subclass_pred = logits[i]  # Output from model, shape: [batch_size, 3]\n                subclass_target = target[:, i]  # Raw target label (not one-hot), shape: [batch_size]\n                #print(subclass_pred)\n                # Compute the loss for the i-th subclass\n                loss += criterion(subclass_pred, subclass_target)\n\n            val_loss.append(loss.detach().cpu().numpy())\n\n    return  np.mean(val_loss)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.130721Z","iopub.execute_input":"2024-10-11T10:38:54.131002Z","iopub.status.idle":"2024-10-11T10:38:54.143368Z","shell.execute_reply.started":"2024-10-11T10:38:54.130972Z","shell.execute_reply":"2024-10-11T10:38:54.142591Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Fix Warmup Bug\nfrom warmup_scheduler import GradualWarmupScheduler  # https://github.com/ildoonet/pytorch-gradual-warmup-lr\n\n\nclass GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.144474Z","iopub.execute_input":"2024-10-11T10:38:54.145023Z","iopub.status.idle":"2024-10-11T10:38:54.158624Z","shell.execute_reply.started":"2024-10-11T10:38:54.144989Z","shell.execute_reply":"2024-10-11T10:38:54.157698Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def run(df_train,df_valid,fold=0):\n\n    \n    dataset_train = RSNADataset(df_train,cordinates_t1,cordinates_t2,cfg,resize_transform, isTrain= True,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)\n    dataset_valid = RSNADataset(df_valid, cordinates_t1,cordinates_t2,cfg,resize_transform,isTrain= True,transform_axial = prepare_level_wise_axial,transform_sag = plot_5_crops)\n    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=cfg.batch_size, sampler=RandomSampler(dataset_train), num_workers=cfg.num_workers)\n    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=cfg.batch_size, num_workers=cfg.num_workers)\n\n    model = CustomModel()\n    \n    model = model.to(device)\n\n    #auc_20_max = 0.\n    model_file3 = os.path.join(cfg.model_dir, f'{cfg.kernel_type}_final_fold{fold}.pth')\n\n    optimizer = optim.Adam(model.parameters(), lr=cfg.init_lr)\n    \n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, cfg.n_epochs - 1)\n    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n    \n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, cfg.n_epochs + 1):\n        print(time.ctime(), f'Fold {fold}, Epoch {epoch}')\n\n        train_loss = train_epoch(model, train_loader, optimizer)\n        val_loss = val_epoch(model, valid_loader)\n\n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}.'\n        print(content)\n\n        scheduler_warmup.step()    \n        if epoch==2: scheduler_warmup.step() # bug workaround   \n        \n        model_file  = os.path.join(cfg.model_dir, f'{cfg.kernel_type}_best_fold{epoch}.pth')\n        print('auc_max ({:.6f} --> {:.6f}). Saving model ...'.format(train_loss, val_loss))\n        torch.save(model.state_dict(), model_file)\n    \n\n    torch.save(model.state_dict(), model_file3)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.161833Z","iopub.execute_input":"2024-10-11T10:38:54.162369Z","iopub.status.idle":"2024-10-11T10:38:54.173567Z","shell.execute_reply.started":"2024-10-11T10:38:54.162334Z","shell.execute_reply":"2024-10-11T10:38:54.172767Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = cfg.CUDA_VISIBLE_DEVICES\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.174689Z","iopub.execute_input":"2024-10-11T10:38:54.174988Z","iopub.status.idle":"2024-10-11T10:38:54.186231Z","shell.execute_reply.started":"2024-10-11T10:38:54.174951Z","shell.execute_reply":"2024-10-11T10:38:54.185426Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df_train, df_valid = train_test_split(train_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.187237Z","iopub.execute_input":"2024-10-11T10:38:54.187547Z","iopub.status.idle":"2024-10-11T10:38:54.201031Z","shell.execute_reply.started":"2024-10-11T10:38:54.187516Z","shell.execute_reply":"2024-10-11T10:38:54.200142Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"run(df_train,df_valid,fold=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:38:54.202128Z","iopub.execute_input":"2024-10-11T10:38:54.202434Z","iopub.status.idle":"2024-10-11T11:44:41.593866Z","shell.execute_reply.started":"2024-10-11T10:38:54.202402Z","shell.execute_reply":"2024-10-11T11:44:41.591881Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n100%|██████████| 127M/127M [00:00<00:00, 200MB/s] \n","output_type":"stream"},{"name":"stdout","text":"1432 358\nFri Oct 11 10:38:56 2024 Fold 0, Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"loss: 19.38962, smth: 19.91698: 100%|██████████| 90/90 [14:01<00:00,  9.35s/it] \n100%|██████████| 23/23 [03:36<00:00,  9.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fri Oct 11 10:56:34 2024 Fold 0, Epoch 1, lr: 0.0005000, train loss: 19.91698, valid loss: 19.05346.\nauc_max (19.916983 --> 19.053460). Saving model ...\nFri Oct 11 10:56:35 2024 Fold 0, Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"loss: 18.78666, smth: 19.30875: 100%|██████████| 90/90 [13:25<00:00,  8.95s/it]\n100%|██████████| 23/23 [03:14<00:00,  8.47s/it]\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1732: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  _warn_get_lr_called_within_step(self)\n","output_type":"stream"},{"name":"stdout","text":"Fri Oct 11 11:13:15 2024 Fold 0, Epoch 2, lr: 0.0050000, train loss: 19.30875, valid loss: 19.16362.\nauc_max (19.308750 --> 19.163620). Saving model ...\nFri Oct 11 11:13:15 2024 Fold 0, Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"loss: 16.03252, smth: 19.26404: 100%|██████████| 90/90 [12:46<00:00,  8.52s/it]\n100%|██████████| 23/23 [03:09<00:00,  8.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fri Oct 11 11:29:12 2024 Fold 0, Epoch 3, lr: 0.0037500, train loss: 19.26404, valid loss: 19.07790.\nauc_max (19.264043 --> 19.077900). Saving model ...\nFri Oct 11 11:29:12 2024 Fold 0, Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"loss: 17.55321, smth: 19.16020: 100%|██████████| 90/90 [12:13<00:00,  8.15s/it]\n100%|██████████| 23/23 [03:14<00:00,  8.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fri Oct 11 11:44:40 2024 Fold 0, Epoch 4, lr: 0.0012500, train loss: 19.16020, valid loss: 18.90640.\nauc_max (19.160198 --> 18.906395). Saving model ...\n","output_type":"stream"}]},{"cell_type":"code","source":"df_valid.to_csv(\"hello.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T11:44:41.596478Z","iopub.execute_input":"2024-10-11T11:44:41.596838Z","iopub.status.idle":"2024-10-11T11:44:41.615458Z","shell.execute_reply.started":"2024-10-11T11:44:41.596801Z","shell.execute_reply":"2024-10-11T11:44:41.614703Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}