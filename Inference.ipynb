{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9295358,"sourceType":"datasetVersion","datasetId":5627706},{"sourceId":105448,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":88365,"modelId":112594}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T16:02:07.749224Z","iopub.execute_input":"2024-09-01T16:02:07.749866Z","iopub.status.idle":"2024-09-01T16:02:07.754931Z","shell.execute_reply.started":"2024-09-01T16:02:07.749827Z","shell.execute_reply":"2024-09-01T16:02:07.753977Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport time\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import VotingClassifier\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport optuna\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport albumentations\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport cv2\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.image as mpimg\n\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport random\nimport h5py\nfrom io import BytesIO\n# import apex\n# from apex import amp","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:07.815325Z","iopub.execute_input":"2024-09-01T16:02:07.815777Z","iopub.status.idle":"2024-09-01T16:02:18.373529Z","shell.execute_reply.started":"2024-09-01T16:02:07.815730Z","shell.execute_reply":"2024-09-01T16:02:18.372672Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_na_columns = [\n    \"copyright_license\",\n    \"attribution\",\n    \"patient_id\",\n    \"image_type\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.375391Z","iopub.execute_input":"2024-09-01T16:02:18.376099Z","iopub.status.idle":"2024-09-01T16:02:18.381308Z","shell.execute_reply.started":"2024-09-01T16:02:18.376061Z","shell.execute_reply":"2024-09-01T16:02:18.379852Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"root = Path('/kaggle/input/isic-2024-challenge')\n\ntrain_path = root / 'train-metadata.csv'\ntest_path = root / 'test-metadata.csv'\nsubm_path = root / 'sample_submission.csv'\n\nid_col = 'isic_id'\ntarget_col = 'iddx_full'\ngroup_col = 'patient_id'\n\nerr = 1e-5\nsampling_ratio = 0.01\nseed = 42\n\nnum_cols = [\n    'age_approx',                        # Approximate age of patient at time of imaging.\n    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n    'tbp_lv_A',                          # A inside  lesion.+\n    'tbp_lv_Aext',                       # A outside lesion.+\n    'tbp_lv_B',                          # B inside  lesion.+\n    'tbp_lv_Bext',                       # B outside lesion.+ \n    'tbp_lv_C',                          # Chroma inside  lesion.+\n    'tbp_lv_Cext',                       # Chroma outside lesion.+\n    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n    'tbp_lv_Hext',                       # Hue outside lesion.+\n    'tbp_lv_L',                          # L inside lesion.+\n    'tbp_lv_Lext',                       # L outside lesion.+\n    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaLB',                    #\n    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n    'tbp_lv_eccentricity',               # Eccentricity.+\n    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n]\n\nnew_num_cols = [\n    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n\n    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n\n    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n\n    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n\n    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n    'border_color_interaction_2',\n    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n    'age_normalized_nevi_confidence_2',\n    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n\n    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n]\n\ncat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple']\nfeature_cols = num_cols + new_num_cols + cat_cols ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.382795Z","iopub.execute_input":"2024-09-01T16:02:18.383089Z","iopub.status.idle":"2024-09-01T16:02:18.400778Z","shell.execute_reply.started":"2024-09-01T16:02:18.383057Z","shell.execute_reply":"2024-09-01T16:02:18.399926Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_data(path):\n    return (\n        pl.DataFrame(path)\n        .with_columns(\n            lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n            lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n        )\n        .with_columns(\n            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n            perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n            area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n            consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n        )\n        .with_columns(\n            color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n            consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n        )\n        .with_columns(\n            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n            normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n        )\n        .with_columns(\n            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n            color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n            border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n            size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n            age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .with_columns(\n            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n            border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .to_pandas()\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.403277Z","iopub.execute_input":"2024-09-01T16:02:18.403604Z","iopub.status.idle":"2024-09-01T16:02:18.609530Z","shell.execute_reply.started":"2024-09-01T16:02:18.403572Z","shell.execute_reply":"2024-09-01T16:02:18.608462Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def preprocess(df_train,is_test = False):\n\n    global cat_cols\n\n    df_train.drop(columns = test_na_columns,inplace=True)\n\n    # Select numerical columns\n    numerical_cols = df_train.select_dtypes(include=['number']).columns.tolist()\n    \n    if is_test == False:\n        numerical_cols.remove('target')\n    # Initialize the scaler\n    with open('/kaggle/input/encoder-scaler/scaler.pkl', 'rb') as file:\n        scaler = pickle.load(file)\n        # Fit the scaler on the numerical data and transform\n        df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n\n    with open('/kaggle/input/encoder-scaler/OneHotEncoder.pkl', 'rb') as file:\n        encoder = pickle.load(file)\n        MAX_CAT_VAR_NAME_SIZE = 30\n        new_cat_cols = [x.replace(\" \", \"_\")[:MAX_CAT_VAR_NAME_SIZE] for x in encoder.get_feature_names_out()]\n\n        df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n        df_train.drop(columns = cat_cols,inplace=True)\n\n    if is_test == False:\n        enc = OrdinalEncoder()\n        enc.fit(df_train[[target_col]])\n        df_train[target_col] = enc.transform(df_train[[target_col]])\n        with open('ordinalEndoder.pkl', 'wb') as file:\n            pickle.dump(enc, file)\n\n    return df_train","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.610773Z","iopub.execute_input":"2024-09-01T16:02:18.611085Z","iopub.status.idle":"2024-09-01T16:02:18.622219Z","shell.execute_reply.started":"2024-09-01T16:02:18.611034Z","shell.execute_reply":"2024-09-01T16:02:18.621423Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_df(data_dir,use_meta,is_train= False):\n    df = pd.read_csv(data_dir)\n\n    mode_values = df.select_dtypes(include=['object']).mode().iloc[0]\n    df.fillna(mode_values, inplace=True)\n\n    median_values = df.select_dtypes(include=['number']).median()\n\n    # Fill null values with the median values\n    df.fillna(median_values, inplace=True)\n    \n    df = read_data(df)\n    df = preprocess(df,is_train)\n    \n    if use_meta:\n        dataset_features = df.columns.to_list()\n        meta_features = list(set(dataset_features)-set(['target','isic_id','iddx_full']))\n        n_meta_features = len(meta_features)\n    else:\n        meta_features = None\n        n_meta_features = 0\n        \n    mel_idx = 1\n    return df, meta_features, n_meta_features, mel_idx","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.623401Z","iopub.execute_input":"2024-09-01T16:02:18.623812Z","iopub.status.idle":"2024-09-01T16:02:18.634075Z","shell.execute_reply.started":"2024-09-01T16:02:18.623779Z","shell.execute_reply":"2024-09-01T16:02:18.633232Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def hair_remove(image):\n    # convert image to grayScale\n    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # kernel for morphologyEx\n    kernel = cv2.getStructuringElement(1,(17,17))\n\n    # apply MORPH_BLACKHAT to grayScale image\n    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n\n    # apply thresholding to blackhat\n    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n\n    # inpaint with original image and threshold image\n    final_image = cv2.inpaint(image,threshold,1,cv2.INPAINT_TELEA)\n\n    return final_image","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.635485Z","iopub.execute_input":"2024-09-01T16:02:18.636207Z","iopub.status.idle":"2024-09-01T16:02:18.644982Z","shell.execute_reply.started":"2024-09-01T16:02:18.636162Z","shell.execute_reply":"2024-09-01T16:02:18.644077Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    def __init__(self, csv, mode,images_path, meta_features, transform=None):\n\n        self.csv = csv.reset_index(drop=True)\n        self.mode = mode\n        self.use_meta = meta_features is not None\n        self.meta_features = meta_features\n        self.transform = transform\n        self.hdf5trainFiles = h5py.File(images_path, 'r')\n        \n    def __len__(self):\n        return self.csv.shape[0]\n    \n    def hdf5_dataset2img(self,isic_id):\n        img_bytes  = self.hdf5trainFiles[isic_id][()]\n        images = np.frombuffer(img_bytes, dtype=np.uint8)\n        images = cv2.imdecode(images, cv2.IMREAD_COLOR)\n        return images\n    \n    def __getitem__(self, index):\n\n        row = self.csv.iloc[index]\n        #print(row)\n        #train_image_base_path  = f'/kaggle/input/isic-2024-challenge/train-image/image/{row.isic_id}.jpg'\n        #print(\"filePath:\"+train_image_base_path)\n        image = self.hdf5_dataset2img(row.isic_id)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        #image = hair_remove(image)\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n        #print(self.csv.iloc[index][self.meta_features])\n        if self.use_meta:\n            data = (torch.tensor(image).float(), torch.tensor(self.csv.iloc[index][self.meta_features]).float())\n        else:\n            data = torch.tensor(image).float()\n\n        if self.mode == 'test':\n            return data\n        else:\n            return data, torch.tensor(self.csv.iloc[index].target).long()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.646016Z","iopub.execute_input":"2024-09-01T16:02:18.646279Z","iopub.status.idle":"2024-09-01T16:02:18.658649Z","shell.execute_reply.started":"2024-09-01T16:02:18.646250Z","shell.execute_reply":"2024-09-01T16:02:18.657604Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_transforms(image_size):\n\n    transforms_train = albumentations.Compose([\n        albumentations.Transpose(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2,p=0.75),\n        albumentations.OneOf([\n            albumentations.MotionBlur(blur_limit=5),\n            albumentations.MedianBlur(blur_limit=5),\n            albumentations.GaussianBlur(blur_limit=5),\n            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n        ], p=0.7),\n\n        albumentations.OneOf([\n            albumentations.OpticalDistortion(distort_limit=1.0),\n            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n            albumentations.ElasticTransform(alpha=3),\n        ], p=0.7),\n\n        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n        #albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        albumentations.Resize(image_size, image_size),\n        albumentations.CoarseDropout(max_holes= 3,max_height = int(image_size * 0.20),min_holes = 1,max_width =int(image_size * 0.20),p=0.7 ),\n        albumentations.Normalize()\n    ])\n\n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.659974Z","iopub.execute_input":"2024-09-01T16:02:18.660327Z","iopub.status.idle":"2024-09-01T16:02:18.671463Z","shell.execute_reply.started":"2024-09-01T16:02:18.660285Z","shell.execute_reply":"2024-09-01T16:02:18.670518Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from timm import create_model\nfrom timm import list_models\nimport timm","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:18.675209Z","iopub.execute_input":"2024-09-01T16:02:18.675568Z","iopub.status.idle":"2024-09-01T16:02:21.017027Z","shell.execute_reply.started":"2024-09-01T16:02:18.675520Z","shell.execute_reply":"2024-09-01T16:02:21.016081Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nsigmoid = nn.Sigmoid()\n\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass Swish_Module(nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.018208Z","iopub.execute_input":"2024-09-01T16:02:21.018603Z","iopub.status.idle":"2024-09-01T16:02:21.025563Z","shell.execute_reply.started":"2024-09-01T16:02:21.018554Z","shell.execute_reply":"2024-09-01T16:02:21.024523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Effnet_Melanoma(nn.Module):\n    def __init__(self, enet_type, out_dim, n_meta_features=0, n_meta_dim=[512, 128], pretrained=False,num_heads=2):\n        super(Effnet_Melanoma, self).__init__()\n        self.n_meta_features = n_meta_features\n        self.effnet = timm.create_model(enet_type, pretrained=True)\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        in_ch = self.effnet.classifier.in_features\n        \n        if n_meta_features > 0:\n            self.meta = nn.Sequential(\n                nn.Linear(n_meta_features, n_meta_dim[0]),\n                nn.BatchNorm1d(n_meta_dim[0]),\n                Swish_Module(),\n                nn.Dropout(p=0.3),\n                nn.Linear(n_meta_dim[0], n_meta_dim[1]),\n                nn.BatchNorm1d(n_meta_dim[1]),\n                Swish_Module(),\n            )\n            \n            self.reduce_dim = nn.Linear(in_ch, n_meta_dim[1])\n            self.cross_attention = nn.MultiheadAttention(embed_dim=n_meta_dim[1], num_heads=num_heads)\n            \n            in_ch = n_meta_dim[1]\n        self.myfc = nn.Linear(in_ch, out_dim)\n        self.effnet.classifier = nn.Identity()\n\n    def extract(self, x):\n        x = self.effnet(x)\n        return x\n\n    def forward(self, x, x_meta=None):\n        x = self.extract(x).squeeze(-1).squeeze(-1)\n        if self.n_meta_features > 0:\n            \n            x_meta = self.meta(x_meta)\n            x_meta = x_meta.unsqueeze(1)\n            \n            x = self.reduce_dim(x)\n            x = x.unsqueeze(1)\n            \n            attn_output, _ = self.cross_attention(x, x_meta, x_meta)\n            x = attn_output.squeeze(1) \n            \n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                out = self.myfc(dropout(x))\n            else:\n                out += self.myfc(dropout(x))\n        out /= len(self.dropouts)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.027180Z","iopub.execute_input":"2024-09-01T16:02:21.027625Z","iopub.status.idle":"2024-09-01T16:02:21.042283Z","shell.execute_reply.started":"2024-09-01T16:02:21.027582Z","shell.execute_reply":"2024-09-01T16:02:21.041439Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.043765Z","iopub.execute_input":"2024-09-01T16:02:21.044122Z","iopub.status.idle":"2024-09-01T16:02:21.053873Z","shell.execute_reply.started":"2024-09-01T16:02:21.044081Z","shell.execute_reply":"2024-09-01T16:02:21.052948Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class args:\n    DEBUG = False\n    model_dir = '/kaggle/input/skin-cancer-detection/pytorch/v1/1'\n    log_dir = '/kaggle/working/logs/'\n    kernel_type = 'kag'\n    batch_size = 32\n    out_dim = 2\n    n_epochs = 2\n    num_workers = 2\n    use_amp = False\n    enet_type = 'efficientnet_b2'\n    dp = False\n    image_size = 224\n    data_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n    folded_data_path = '/kaggle/working/folded_metadata.csv'\n    n_splits = 2\n    n_meta_dim = [512,128]\n    init_lr = 0.002\n    fold = [0,1]\n    CUDA_VISIBLE_DEVICES = '0'\n    use_meta = True\n    n_test = 1\n    image_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:04:46.709455Z","iopub.execute_input":"2024-09-01T16:04:46.709863Z","iopub.status.idle":"2024-09-01T16:04:46.716390Z","shell.execute_reply.started":"2024-09-01T16:04:46.709822Z","shell.execute_reply":"2024-09-01T16:04:46.715416Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def partial_auc(validation_gt, v_predict,min_tpr: float=0.80):\n    \n    v_gt = abs(np.asarray(validation_gt)-1)\n\n    # flip the submissions to their compliments\n    v_pred = -1.0*np.asarray(v_predict)\n\n    max_fpr = abs(1-min_tpr)\n\n    # using sklearn.metric functions: (1) roc_curve and (2) auc\n    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n    if max_fpr is None or max_fpr == 1:\n        return auc(fpr, tpr)\n    if max_fpr <= 0 or max_fpr > 1:\n        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n\n    # Add a single point at max_fpr by linear interpolation\n    stop = np.searchsorted(fpr, max_fpr, \"right\")\n    x_interp = [fpr[stop - 1], fpr[stop]]\n    y_interp = [tpr[stop - 1], tpr[stop]]\n    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n    fpr = np.append(fpr[:stop], max_fpr)\n    partial_auc = auc(fpr, tpr)\n    return(partial_auc)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.066221Z","iopub.execute_input":"2024-09-01T16:02:21.066579Z","iopub.status.idle":"2024-09-01T16:02:21.076571Z","shell.execute_reply.started":"2024-09-01T16:02:21.066538Z","shell.execute_reply":"2024-09-01T16:02:21.075693Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df, meta_features, n_meta_features, mel_idx = get_df(\n    args.data_path,\n    args.use_meta,\n    is_train= True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.078185Z","iopub.execute_input":"2024-09-01T16:02:21.078526Z","iopub.status.idle":"2024-09-01T16:02:21.324365Z","shell.execute_reply.started":"2024-09-01T16:02:21.078487Z","shell.execute_reply":"2024-09-01T16:02:21.323242Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['isic_id', 'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'lesion_size_ratio', 'lesion_shape_index', 'hue_contrast', 'luminance_contrast', 'lesion_color_difference', 'border_complexity', 'color_uniformity', 'position_distance_3d', 'perimeter_to_area_ratio', 'area_to_perimeter_ratio', 'lesion_visibility_score', 'symmetry_border_consistency', 'consistency_symmetry_border', 'color_consistency', 'consistency_color', 'size_age_interaction', 'hue_color_std_interaction', 'lesion_severity_index', 'shape_complexity_index', 'color_contrast_index', 'log_lesion_area', 'normalized_lesion_size', 'mean_hue_difference', 'std_dev_contrast', 'color_shape_composite_index', 'lesion_orientation_3d', 'overall_color_difference', 'symmetry_perimeter_interaction', 'comprehensive_lesion_index', 'color_variance_ratio', 'border_color_interaction', 'border_color_interaction_2', 'size_color_contrast_ratio', 'age_normalized_nevi_confidence', 'age_normalized_nevi_confidence_2', 'color_asymmetry_index', 'volume_approximation_3d', 'color_range', 'shape_color_consistency', 'border_length_ratio', 'age_size_symmetry_index', 'index_age_size_symmetry', 'sex_female', 'sex_male', 'anatom_site_general_anterior_t', 'anatom_site_general_head/neck', 'anatom_site_general_lower_extr', 'anatom_site_general_posterior_', 'anatom_site_general_upper_extr', 'tbp_tile_type_3D:_XP', 'tbp_tile_type_3D:_white', 'tbp_lv_location_Head_&_Neck', 'tbp_lv_location_Left_Arm', 'tbp_lv_location_Left_Arm_-_Low', 'tbp_lv_location_Left_Arm_-_Upp', 'tbp_lv_location_Left_Leg', 'tbp_lv_location_Left_Leg_-_Low', 'tbp_lv_location_Left_Leg_-_Upp', 'tbp_lv_location_Right_Arm', 'tbp_lv_location_Right_Arm_-_Lo', 'tbp_lv_location_Right_Arm_-_Up', 'tbp_lv_location_Right_Leg', 'tbp_lv_location_Right_Leg_-_Lo', 'tbp_lv_location_Right_Leg_-_Up', 'tbp_lv_location_Torso_Back', 'tbp_lv_location_Torso_Back_Bot', 'tbp_lv_location_Torso_Back_Mid', 'tbp_lv_location_Torso_Back_Top', 'tbp_lv_location_Torso_Front', 'tbp_lv_location_Torso_Front_Bo', 'tbp_lv_location_Torso_Front_To', 'tbp_lv_location_Unknown', 'tbp_lv_location_simple_Head_&_', 'tbp_lv_location_simple_Left_Ar', 'tbp_lv_location_simple_Left_Le', 'tbp_lv_location_simple_Right_A', 'tbp_lv_location_simple_Right_L', 'tbp_lv_location_simple_Torso_B', 'tbp_lv_location_simple_Torso_F', 'tbp_lv_location_simple_Unknown']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n/tmp/ipykernel_36/2051110496.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n","output_type":"stream"}]},{"cell_type":"code","source":"transforms_train, transforms_val = get_transforms(args.image_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.337829Z","iopub.execute_input":"2024-09-01T16:02:21.338842Z","iopub.status.idle":"2024-09-01T16:02:21.352704Z","shell.execute_reply.started":"2024-09-01T16:02:21.338798Z","shell.execute_reply":"2024-09-01T16:02:21.351593Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pydantic/main.py:193: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n  self.__pydantic_validator__.validate_python(data, self_instance=self)\n","output_type":"stream"}]},{"cell_type":"code","source":"if args.DEBUG:\n    df = df.sample(args.batch_size * 3)\ndataset_test = MelanomaDataset(df, 'test','/kaggle/input/isic-2024-challenge/test-image.hdf5', meta_features, transform=transforms_val)\ntest_loader = torch.utils.data.DataLoader(dataset_test, batch_size=args.batch_size, num_workers=args.num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.354066Z","iopub.execute_input":"2024-09-01T16:02:21.354604Z","iopub.status.idle":"2024-09-01T16:02:21.370266Z","shell.execute_reply.started":"2024-09-01T16:02:21.354571Z","shell.execute_reply":"2024-09-01T16:02:21.369275Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_file = os.path.join(args.model_dir, f'{args.kernel_type}_final_fold{args.fold[0]}.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.371444Z","iopub.execute_input":"2024-09-01T16:02:21.371820Z","iopub.status.idle":"2024-09-01T16:02:21.376821Z","shell.execute_reply.started":"2024-09-01T16:02:21.371776Z","shell.execute_reply":"2024-09-01T16:02:21.375860Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_file","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.377993Z","iopub.execute_input":"2024-09-01T16:02:21.378290Z","iopub.status.idle":"2024-09-01T16:02:21.387923Z","shell.execute_reply.started":"2024-09-01T16:02:21.378258Z","shell.execute_reply":"2024-09-01T16:02:21.387096Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/skin-cancer-detection/pytorch/v1/1/kag_final_fold0.pth'"},"metadata":{}}]},{"cell_type":"code","source":"os.makedirs(args.model_dir, exist_ok=True)\nos.makedirs(args.log_dir, exist_ok=True)\nos.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n\nif args.enet_type == 'resnest101':\n    ModelClass = Resnest_Melanoma\nelif args.enet_type == 'seresnext101':\n    ModelClass = Seresnext_Melanoma\nelif 'efficientnet' in args.enet_type:\n    ModelClass = Effnet_Melanoma\nelse:\n    raise NotImplementedError()\n\nDP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\nprint(os.environ['CUDA_VISIBLE_DEVICES'])\nset_seed()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.389339Z","iopub.execute_input":"2024-09-01T16:02:21.389732Z","iopub.status.idle":"2024-09-01T16:02:21.443853Z","shell.execute_reply.started":"2024-09-01T16:02:21.389690Z","shell.execute_reply":"2024-09-01T16:02:21.442617Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"n_meta_features","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.445236Z","iopub.execute_input":"2024-09-01T16:02:21.445653Z","iopub.status.idle":"2024-09-01T16:02:21.452692Z","shell.execute_reply.started":"2024-09-01T16:02:21.445613Z","shell.execute_reply":"2024-09-01T16:02:21.451523Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"114"},"metadata":{}}]},{"cell_type":"code","source":"\nmodel = ModelClass(\n    args.enet_type,\n    out_dim=args.out_dim,\n    n_meta_features=n_meta_features,\n    n_meta_dim=args.n_meta_dim,\n    num_heads=2\n)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:21.454056Z","iopub.execute_input":"2024-09-01T16:02:21.454803Z","iopub.status.idle":"2024-09-01T16:02:26.962764Z","shell.execute_reply.started":"2024-09-01T16:02:21.454755Z","shell.execute_reply":"2024-09-01T16:02:26.961827Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/36.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3718b101b445328e446960adebe433"}},"metadata":{}}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(model_file), strict=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:26.964266Z","iopub.execute_input":"2024-09-01T16:02:26.964648Z","iopub.status.idle":"2024-09-01T16:02:27.555139Z","shell.execute_reply.started":"2024-09-01T16:02:26.964597Z","shell.execute_reply":"2024-09-01T16:02:27.554089Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3185480519.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_file), strict=True)\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:02:27.556546Z","iopub.execute_input":"2024-09-01T16:02:27.556944Z","iopub.status.idle":"2024-09-01T16:02:27.575799Z","shell.execute_reply.started":"2024-09-01T16:02:27.556899Z","shell.execute_reply":"2024-09-01T16:02:27.574711Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Effnet_Melanoma(\n  (effnet): EfficientNet(\n    (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNormAct2d(\n      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n      (drop): Identity()\n      (act): SiLU(inplace=True)\n    )\n    (blocks): Sequential(\n      (0): Sequential(\n        (0): DepthwiseSeparableConv(\n          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (bn1): BatchNormAct2d(\n            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): DepthwiseSeparableConv(\n          (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n          (bn1): BatchNormAct2d(\n            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (1): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (bn2): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (bn2): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (bn2): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (2): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n          (bn2): BatchNormAct2d(\n            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n          (bn2): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n          (bn2): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (3): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n          (bn2): BatchNormAct2d(\n            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n          (bn2): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n          (bn2): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (3): InvertedResidual(\n          (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n          (bn2): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (4): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n          (bn2): BatchNormAct2d(\n            528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n          (bn2): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n          (bn2): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (3): InvertedResidual(\n          (conv_pw): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n          (bn2): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (5): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n          (bn2): BatchNormAct2d(\n            720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n          (bn2): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n          (bn2): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (3): InvertedResidual(\n          (conv_pw): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n          (bn2): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (4): InvertedResidual(\n          (conv_pw): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n          (bn2): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (6): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n          (bn2): BatchNormAct2d(\n            1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n          (bn2): BatchNormAct2d(\n            2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (aa): Identity()\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n    )\n    (conv_head): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn2): BatchNormAct2d(\n      1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n      (drop): Identity()\n      (act): SiLU(inplace=True)\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (classifier): Identity()\n  )\n  (dropouts): ModuleList(\n    (0-4): 5 x Dropout(p=0.5, inplace=False)\n  )\n  (meta): Sequential(\n    (0): Linear(in_features=114, out_features=512, bias=True)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Swish_Module()\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=512, out_features=128, bias=True)\n    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Swish_Module()\n  )\n  (reduce_dim): Linear(in_features=1408, out_features=128, bias=True)\n  (cross_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n  )\n  (myfc): Linear(in_features=128, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_trans(img, I):\n\n    if I >= 4:\n        img = img.transpose(2, 3)\n    if I % 4 == 0:\n        return img\n    elif I % 4 == 1:\n        return img.flip(2)\n    elif I % 4 == 2:\n        return img.flip(3)\n    elif I % 4 == 3:\n        return img.flip(2).flip(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:03:53.952895Z","iopub.execute_input":"2024-09-01T16:03:53.953323Z","iopub.status.idle":"2024-09-01T16:03:53.959900Z","shell.execute_reply.started":"2024-09-01T16:03:53.953283Z","shell.execute_reply":"2024-09-01T16:03:53.958894Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# predict\nPROBS = []\nwith torch.no_grad():\n    for (data) in tqdm(test_loader):\n        if args.use_meta:\n            data, meta = data\n            data, meta = data.to(device), meta.to(device)\n            probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n            for I in range(args.n_test):\n                l = model(get_trans(data, I), meta)\n                probs += l.softmax(1)\n        else:   \n            data = data.to(device)\n            probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n            for I in range(args.n_test):\n                l = model(get_trans(data, I))\n                probs += l.softmax(1)\n\n        probs /= args.n_test\n        PROBS.append(probs.detach().cpu())\n\nPROBS = torch.cat(PROBS).numpy()\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:04:52.986702Z","iopub.execute_input":"2024-09-01T16:04:52.987640Z","iopub.status.idle":"2024-09-01T16:04:53.912318Z","shell.execute_reply.started":"2024-09-01T16:04:52.987576Z","shell.execute_reply":"2024-09-01T16:04:53.911049Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_36/3004130941.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  data = (torch.tensor(image).float(), torch.tensor(self.csv.iloc[index][self.meta_features]).float())\n/tmp/ipykernel_36/3004130941.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  data = (torch.tensor(image).float(), torch.tensor(self.csv.iloc[index][self.meta_features]).float())\n/tmp/ipykernel_36/3004130941.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  data = (torch.tensor(image).float(), torch.tensor(self.csv.iloc[index][self.meta_features]).float())\n100%|| 1/1 [00:00<00:00,  1.10it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# save cvs\ndf['target'] = PROBS[:, mel_idx]\ndf[['isic_id', 'target']].to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:05:51.869036Z","iopub.execute_input":"2024-09-01T16:05:51.869479Z","iopub.status.idle":"2024-09-01T16:05:51.879770Z","shell.execute_reply.started":"2024-09-01T16:05:51.869437Z","shell.execute_reply":"2024-09-01T16:05:51.878873Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}